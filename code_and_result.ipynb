{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZCeLvoG42mR-",
    "outputId": "0d88e0be-07d4-4e92-e43b-4a87a5d1109e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.0.1+cu118\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.2/10.2 MB\u001B[0m \u001B[31m98.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.8/4.8 MB\u001B[0m \u001B[31m42.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Building wheel for torch_geometric (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UfDUxD4K3ffO",
    "outputId": "94f4e029-7034-4519-b7a6-9bf20e8d5c7a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ShrjQlIi2jyd",
    "ExecuteTime": {
     "end_time": "2023-09-02T08:20:25.276245800Z",
     "start_time": "2023-09-02T08:20:18.407580600Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import Planetoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "M0sP-lhX2jyf"
   },
   "source": [
    "# GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "WxtE7U-Q2jyg",
    "ExecuteTime": {
     "end_time": "2023-09-02T08:20:25.286246900Z",
     "start_time": "2023-09-02T08:20:25.276245800Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, output_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, mode=1):\n",
    "        if mode == 1:\n",
    "            x = F.dropout(x,p = 0.5 ,training=self.training)\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x,p = 0.5 ,training=self.training)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            x = F.dropout(x,p = 0.05 ,training=self.training)\n",
    "            x = F.relu(x)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "        elif mode == 2:\n",
    "            x = F.dropout(x,p = 0.5 ,training=self.training)\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x,p = 0.5 ,training=self.training)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            x = F.dropout(x,p = 0.05 ,training=self.training)\n",
    "            x = F.relu(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "YWcL8Czb2jyg"
   },
   "source": [
    "# GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Vemn24NQ2jyh",
    "ExecuteTime": {
     "end_time": "2023-09-02T08:20:25.296246300Z",
     "start_time": "2023-09-02T08:20:25.286246900Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, Linear\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "\n",
    "# Define the GAT model\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, num_features, output_dim, embedding_dimension=32, num_heads=8):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(num_features, embedding_dimension, heads=num_heads, dropout=0.6)\n",
    "        self.conv2 = GATConv(num_heads * embedding_dimension, output_dim, heads=num_heads, dropout=0.6)\n",
    "        self.linear1 = Linear(embedding_dimension * 2 * num_heads, 32)\n",
    "        # self.linear2 = Linear(32, 16)\n",
    "        # self.linear3 = Linear(16, 8)\n",
    "\n",
    "    def forward(self, x, edge_index, mode=1):\n",
    "        if mode == 1:\n",
    "            # Pass the input through the first GAT layer\n",
    "            x = F.dropout(x, p = 0.6 , training=self.training)\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = F.elu(x)\n",
    "            x = F.dropout(x, p = 0.6 , training=self.training)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "        elif mode == 2:\n",
    "            # Pass the input through the first GAT layer\n",
    "            x = F.dropout(x, p = 0.6 , training=self.training)\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = F.elu(x)\n",
    "            x = F.dropout(x, p = 0.6 , training=self.training)\n",
    "            x = self.conv2(x, edge_index)\n",
    "\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "M12hQ2Fu2jyh"
   },
   "source": [
    "# GATv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "W78B0Qg_2jyh",
    "ExecuteTime": {
     "end_time": "2023-09-02T08:20:25.316246800Z",
     "start_time": "2023-09-02T08:20:25.296246300Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv, Linear\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "\n",
    "# Define the GAT model\n",
    "class GATv2(torch.nn.Module):\n",
    "    def __init__(self, num_features, output_dim, embedding_dimension=16, num_heads=8):\n",
    "        super(GATv2, self).__init__()\n",
    "        self.conv1 = GATv2Conv(num_features, embedding_dimension, heads=num_heads, dropout=0.6)\n",
    "        self.conv2 = GATv2Conv(num_heads * embedding_dimension, embedding_dimension, heads=num_heads, dropout=0.6)\n",
    "        self.linear1 = Linear(embedding_dimension * num_heads, output_dim)\n",
    "        self.linear2 = Linear(embedding_dimension, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, mode=1):\n",
    "        if mode == 1:\n",
    "            # Pass the input through the first GAT layer\n",
    "            x = F.dropout(x, p = 0.6 , training=self.training)\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = F.elu(x)\n",
    "            x = F.dropout(x, p = 0.6 , training=self.training)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "        elif mode == 2:\n",
    "            # Pass the input through the first GAT layer\n",
    "            x = F.dropout(x, p = 0.6 , training=self.training)\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = F.elu(x)\n",
    "            x = F.dropout(x, p = 0.6 , training=self.training)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hi9k4bl42jyi",
    "outputId": "f65f2e43-561b-44be-e115-3e4a1bc70bc1",
    "ExecuteTime": {
     "end_time": "2023-09-02T08:20:35.502667700Z",
     "start_time": "2023-09-02T08:20:35.492667600Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x39PC4j_2jyj",
    "outputId": "06592958-78e2-43bb-8c39-80af5c9aad12",
    "ExecuteTime": {
     "end_time": "2023-09-02T08:20:45.682621200Z",
     "start_time": "2023-09-02T08:20:45.672675500Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "4fTf7EIi2jyj",
    "ExecuteTime": {
     "end_time": "2023-09-02T08:20:56.958977500Z",
     "start_time": "2023-09-02T08:20:47.142100500Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ebf1fa9a-b5d2-4f24-9a18-d15a5ed6732e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "BuctMzAw2jyj"
   },
   "source": [
    "# Create new masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "cOMYGWQs2jyj",
    "ExecuteTime": {
     "end_time": "2023-09-02T08:21:01.569148900Z",
     "start_time": "2023-09-02T08:21:01.559147800Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def make_random_mask(size, train_fraction, validation_fraction=0):\n",
    "    new_order = random.sample(range(size), size)\n",
    "\n",
    "    ls_train = np.zeros(size, dtype=bool)\n",
    "    ls_validation = np.zeros(size, dtype=bool)\n",
    "    ls_test = np.zeros(size, dtype=bool)\n",
    "\n",
    "    for i, value in enumerate(new_order):\n",
    "        if i < int(size * train_fraction):\n",
    "            ls_train[value] = True\n",
    "            ls_validation[value] = False\n",
    "            ls_test[value] = False\n",
    "        elif i < int(size * (train_fraction + validation_fraction)):\n",
    "            ls_train[value] = False\n",
    "            ls_validation[value] = True\n",
    "            ls_test[value] = False\n",
    "        else:\n",
    "            ls_train[value] = False\n",
    "            ls_validation[value] = False\n",
    "            ls_test[value] = True\n",
    "\n",
    "    ls_train = torch.from_numpy(ls_train)\n",
    "    ls_validation = torch.from_numpy(ls_validation)\n",
    "    ls_test = torch.from_numpy(ls_test)\n",
    "\n",
    "    return ls_train, ls_validation, ls_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "M5AnH_522jyj",
    "ExecuteTime": {
     "end_time": "2023-09-02T08:21:02.062903100Z",
     "start_time": "2023-09-02T08:21:02.052850100Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_GNN_mode_and_save(model, data, epochs, train_mask, test_mask, gnn_name=\"GNN_embeds.pt\"):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = F.nll_loss(out[train_mask], data.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            model.eval()\n",
    "            pred = model(data.x, data.edge_index).argmax(dim=1)\n",
    "            correct = (pred[test_mask] == data.y[test_mask]).sum()\n",
    "            acc = int(correct) / int(test_mask.sum())\n",
    "            print(f'epoch : {epoch + 1} = >  Accuracy: {acc:.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    pred = model(data.x, data.edge_index).argmax(dim=1)\n",
    "    correct = (pred[test_mask] == data.y[test_mask]).sum()\n",
    "    acc = int(correct) / int(test_mask.sum())\n",
    "    print(f'Final accuracy: {acc:.4f}')\n",
    "\n",
    "    # now save\n",
    "    model.eval()\n",
    "    pred_emb = model(data.x, data.edge_index, mode=2)\n",
    "    torch.save(pred_emb, gnn_name)\n",
    "\n",
    "    return acc, pred[test_mask], data.y[test_mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "hHYonHDt2jyk"
   },
   "source": [
    "# Ecoder decodre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Fc1qByrB2jyk",
    "ExecuteTime": {
     "end_time": "2023-09-02T08:21:05.945743400Z",
     "start_time": "2023-09-02T08:21:05.915743900Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "import math\n",
    "\n",
    "# Prepare data\n",
    "features = data.x  # Feature matrix\n",
    "num_features = features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "f7Ka2rKU2jyk",
    "ExecuteTime": {
     "end_time": "2023-09-02T08:21:07.180976100Z",
     "start_time": "2023-09-02T08:21:07.170710700Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Define autoencoder\n",
    "class SymmetricAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size):\n",
    "        super(SymmetricAutoencoder, self).__init__()\n",
    "\n",
    "        self.encoder_layers = []\n",
    "        self.decoder_layers = []\n",
    "\n",
    "        # Encoder\n",
    "        size = input_size\n",
    "        while size > embedding_size:\n",
    "            next_size = 2 ** int(math.log2(size) - 1)\n",
    "            self.encoder_layers.append(nn.Linear(size, next_size))\n",
    "            self.encoder_layers.append(nn.ReLU())\n",
    "            size = next_size\n",
    "\n",
    "        self.embedding = nn.Linear(size, embedding_size)\n",
    "\n",
    "        # Decoder\n",
    "\n",
    "        size = embedding_size\n",
    "\n",
    "        while size < input_size:\n",
    "            self.decoder_layers.append(nn.Linear(size, size * 2))\n",
    "            self.encoder_layers.append(nn.ReLU())\n",
    "            size = size * 2\n",
    "\n",
    "        self.decoder_layers.append(nn.Linear(size, input_size))\n",
    "\n",
    "        self.encoder = nn.Sequential(*self.encoder_layers)\n",
    "        self.decoder = nn.Sequential(*self.decoder_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        embedded = self.embedding(encoded)\n",
    "        decoded = self.decoder(embedded)\n",
    "        return decoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "oUdkMtVy2jyk",
    "ExecuteTime": {
     "end_time": "2023-09-02T08:21:08.249173700Z",
     "start_time": "2023-09-02T08:21:08.229144800Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_enc_embeds(data_features, num_features, num_epochs=5000, embedding_size=32, file_name=\"encoder_32.pt\"):\n",
    "    # Initialize model and optimizer\n",
    "    model = SymmetricAutoencoder(num_features, embedding_size).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00003)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed = model(data_features)\n",
    "        loss = criterion(reconstructed, data_features)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "    # Now you can use the encoder part of the trained autoencoder to get embeddings\n",
    "    encoded_features = model.encoder(data_features)\n",
    "    torch.save(encoded_features, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "def split_data(dataset, num_each_class_train, num_val=500, num_test=1000):\n",
    "    data = dataset[0]\n",
    "    y = torch.detach(data.y).numpy()\n",
    "    keys = [x for x in range(len(y))]\n",
    "    y_dictionary = dict(zip(keys, y))\n",
    "    score_list = set(y)\n",
    "    dic_list = [[] for i in range(len(score_list))]\n",
    "\n",
    "    for item in y_dictionary:\n",
    "        dic_list[y_dictionary[item]].append(item)\n",
    "\n",
    "    l = []\n",
    "    for sample_list in (dic_list):\n",
    "        s_list = random.sample(sample_list, num_each_class_train)\n",
    "        l.extend(s_list)\n",
    "\n",
    "    train_mask = [False for i in range(len(data.x))]\n",
    "    for num in l:\n",
    "        train_mask[num] = True\n",
    "    mylist = [x for x in range(len(data.x))]\n",
    "    mylist = [elt for elt in mylist if elt not in l]\n",
    "    l1 = random.sample(mylist, num_val)\n",
    "    mylist = [elt for elt in mylist if elt not in l1]\n",
    "    val_mask = [False for i in range(len(data.x))]\n",
    "    for num in l1:\n",
    "        val_mask[num] = True\n",
    "\n",
    "    l2 = random.sample(mylist, num_test)\n",
    "    test_mask = [False for i in range(len(data.x))]\n",
    "    for num in l2:\n",
    "        test_mask[num] = True\n",
    "\n",
    "    return train_mask, val_mask, test_mask"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-09-02T08:21:09.125707900Z",
     "start_time": "2023-09-02T08:21:09.115705300Z"
    },
    "id": "k1gzNnVkUMvM"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "xkFNBjZL2jyk"
   },
   "source": [
    "# load ,concat  and finalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "CBI8vrT42jyk",
    "ExecuteTime": {
     "end_time": "2023-09-02T08:21:21.260404500Z",
     "start_time": "2023-09-02T08:21:21.240407100Z"
    }
   },
   "outputs": [],
   "source": [
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes, hidden_layer=64):\n",
    "        super().__init__()\n",
    "        self.linear1 = Linear(input_size, hidden_layer)\n",
    "        self.linear2 = Linear(hidden_layer, hidden_layer)\n",
    "        self.linear3 = Linear(hidden_layer, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        x = F.relu(self.linear3(x))\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "N9KxqYlK2jyk",
    "ExecuteTime": {
     "end_time": "2023-09-02T08:40:23.093680300Z",
     "start_time": "2023-09-02T08:40:23.070727600Z"
    }
   },
   "outputs": [],
   "source": [
    "def finalizer_DNN(train_mask, test_mask, hidden_layer_DNN_size=256, repeat_threshold=0.85,lr=0.001, epochs=3000,\n",
    "                  file_name_gnn_emb=\"GNN_embeds.pt\",\n",
    "                  file_name_enc_emb=\"encoder_emb32_cora.pt\", validation_mask = None , early_stop=0.9):\n",
    "    gnn_emb = torch.load(file_name_gnn_emb).to(device)\n",
    "    enc_emb = torch.load(file_name_enc_emb).to(device)\n",
    "\n",
    "    concated_data = torch.cat((gnn_emb, enc_emb), dim=1)\n",
    "\n",
    "    input_size = concated_data.shape[1]\n",
    "    model = DNN(input_size, dataset.num_classes, hidden_layer=hidden_layer_DNN_size).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(concated_data)\n",
    "        loss = F.nll_loss(out[train_mask], data.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            model.eval()\n",
    "\n",
    "            pred = model(concated_data).argmax(dim=1)\n",
    "            correct = (pred[train_mask] == data.y[train_mask]).sum()\n",
    "            acc_train = int(correct) / int(train_mask.sum())\n",
    "\n",
    "            correct = (pred[test_mask] == data.y[test_mask]).sum()\n",
    "            acc = int(correct) / int(test_mask.sum())\n",
    "            print(f'epoch {epoch} ,Accuracy: {acc:.4f} , train acc : {acc_train}')\n",
    "\n",
    "            if validation_mask is not None:\n",
    "              correct = (pred[validation_mask] == data.y[validation_mask]).sum()\n",
    "              acc_validation = int(correct) / int(validation_mask.sum())\n",
    "              if  acc_validation > early_stop:\n",
    "                break\n",
    "\n",
    "\n",
    "    # calculate train accuracy\n",
    "    pred = model(concated_data).argmax(dim=1)\n",
    "    correct = (pred[new_train_mask] == data.y[new_train_mask]).sum()\n",
    "    acc_train = int(correct) / int(new_train_mask.sum())\n",
    "\n",
    "    if (acc_train < repeat_threshold):\n",
    "        return finalizer_DNN(train_mask, test_mask, repeat_threshold=repeat_threshold, epochs=epochs,\n",
    "                             file_name_gnn_emb=file_name_gnn_emb,\n",
    "                             file_name_enc_emb=file_name_enc_emb)\n",
    "\n",
    "    model.eval()\n",
    "    pred = model(concated_data).argmax(dim=1)\n",
    "    correct = (pred[test_mask] == data.y[test_mask]).sum()\n",
    "    acc = int(correct) / int(test_mask.sum())\n",
    "    print(f'Accuracy: {acc:.4f}')\n",
    "    return acc, pred[test_mask], data.y[test_mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "LdB5TcNl2jyk"
   },
   "source": [
    "# ALL scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "lBHpACd52jyk",
    "ExecuteTime": {
     "end_time": "2023-09-02T08:21:54.338327100Z",
     "start_time": "2023-09-02T08:21:54.328325800Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_details(acc_list, f1_score_list):\n",
    "    print(f'Accuracy avg = {statistics.mean(acc_list)}')\n",
    "    print(f'Accuracy deviation = {statistics.stdev(acc_list)}')\n",
    "    print(f'F1 score(macro) avg = {statistics.mean(f1_score_list)}')\n",
    "    print(f'F1 score(macro) deviation = {statistics.stdev(f1_score_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import statistics\n",
    "from sklearn.metrics import f1_score\n",
    "import copy\n",
    "\n",
    "def run_model(GNN_model , Gnn_epochs ,enc_address ,reply_threshold , head_epochs, validation_mask =None , early_stop = 0.9 ,hidden_layer_DNN_size= 256):\n",
    "  f1_score_list_base = []\n",
    "  acc_list_base = []\n",
    "\n",
    "  f1_score_list = []\n",
    "  acc_list = []\n",
    "  for i in range(10):\n",
    "\n",
    "      model = copy.deepcopy(GNN_model)\n",
    "      print(\"*****\")\n",
    "      print(f'epoch : {i + 1} / 10')\n",
    "      # model = GATv2(dataset.num_features, dataset.num_classes, embedding_dimension=32).to(device)\n",
    "      # GNN_model = GAT(dataset.num_features, dataset.num_classes, embedding_dimension=128).to(device)\n",
    "      # model = GCN(128, dataset.num_classes).to(device)\n",
    "      acc_base, preds_base, labels_base = train_GNN_mode_and_save(model, data, Gnn_epochs, train_mask=new_train_mask,\n",
    "                                                                  test_mask=new_test_mask)\n",
    "      preds_base = preds_base.cpu().numpy()\n",
    "      labels_base = labels_base.cpu().numpy()\n",
    "      f1_score_base = f1_score(labels_base, preds_base, average='macro')\n",
    "\n",
    "      f1_score_list_base.append(f1_score_base)\n",
    "      acc_list_base.append(acc_base)\n",
    "\n",
    "      acc, preds, labels = finalizer_DNN(train_mask=new_train_mask, epochs=head_epochs, test_mask=new_test_mask,\n",
    "                                        repeat_threshold=reply_threshold,\n",
    "                                        file_name_enc_emb=enc_address,\n",
    "                                        validation_mask = validation_mask,\n",
    "                                        early_stop = early_stop\n",
    "                                        hidden_layer_DNN_size = hidden_layer_DNN_size)\n",
    "\n",
    "      preds = preds.cpu().numpy()\n",
    "      labels = labels.cpu().numpy()\n",
    "\n",
    "      print(\"base model\")\n",
    "      print(acc_base)\n",
    "      print(f1_score_base)\n",
    "\n",
    "      print(\"OurModel\")\n",
    "      print(acc)\n",
    "      print(f1_score(labels, preds, average='macro'))\n",
    "      f1_score_list.append(f1_score(labels, preds, average='macro'))\n",
    "      acc_list.append(acc)\n",
    "\n",
    "\n",
    "  print(\"+++***Fianal Result***+++\")\n",
    "  print(\"base\")\n",
    "  print_details(acc_list_base, f1_score_list_base)\n",
    "  print(\"new model\")\n",
    "  print_details(acc_list, f1_score_list)"
   ],
   "metadata": {
    "id": "uCpjqLer3fC3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "MCuQNriK2jyk"
   },
   "source": [
    "# Cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "cO-C4si52jyk",
    "ExecuteTime": {
     "end_time": "2023-09-02T08:21:56.398503600Z",
     "start_time": "2023-09-02T08:21:56.368503Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "# Prepare data\n",
    "features = data.x  # Feature matrix\n",
    "num_features = features.shape[1]\n",
    "\n",
    "new_train_mask, new_valdation_mask, new_test_mask = make_random_mask(data.x.shape[0], 0.5, 0.2)\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "make_enc_embeds(data.x, num_features, num_epochs=3000, embedding_size=32, file_name=\"encoder_emb32_cora.pt\")"
   ],
   "metadata": {
    "id": "EEFPRhsRP2J9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "make_enc_embeds(data.x, num_features, num_epochs=3000, embedding_size=64, file_name=\"encoder_emb64_cora.pt\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CvA6Ta4XP5HH",
    "outputId": "65991bbb-68e6-4cc5-8ba7-577d38fe2c8e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch [1/3000], Loss: 0.013141512870788574\n",
      "Epoch [2/3000], Loss: 0.013021192513406277\n",
      "Epoch [3/3000], Loss: 0.012914200313389301\n",
      "Epoch [4/3000], Loss: 0.012818754650652409\n",
      "Epoch [5/3000], Loss: 0.012733210809528828\n",
      "Epoch [6/3000], Loss: 0.012656166218221188\n",
      "Epoch [7/3000], Loss: 0.012586437165737152\n",
      "Epoch [8/3000], Loss: 0.012523035518825054\n",
      "Epoch [9/3000], Loss: 0.012465140782296658\n",
      "Epoch [10/3000], Loss: 0.012412087060511112\n",
      "Epoch [11/3000], Loss: 0.01236333604902029\n",
      "Epoch [12/3000], Loss: 0.012318463064730167\n",
      "Epoch [13/3000], Loss: 0.012277131900191307\n",
      "Epoch [14/3000], Loss: 0.012239079922437668\n",
      "Epoch [15/3000], Loss: 0.012204117141664028\n",
      "Epoch [16/3000], Loss: 0.012172098271548748\n",
      "Epoch [17/3000], Loss: 0.012142907828092575\n",
      "Epoch [18/3000], Loss: 0.012116462923586369\n",
      "Epoch [19/3000], Loss: 0.012092693708837032\n",
      "Epoch [20/3000], Loss: 0.012071527540683746\n",
      "Epoch [21/3000], Loss: 0.012052884325385094\n",
      "Epoch [22/3000], Loss: 0.012036674655973911\n",
      "Epoch [23/3000], Loss: 0.012022779323160648\n",
      "Epoch [24/3000], Loss: 0.012011047452688217\n",
      "Epoch [25/3000], Loss: 0.012001298367977142\n",
      "Epoch [26/3000], Loss: 0.011993316002190113\n",
      "Epoch [27/3000], Loss: 0.011986869387328625\n",
      "Epoch [28/3000], Loss: 0.011981729418039322\n",
      "Epoch [29/3000], Loss: 0.011977669782936573\n",
      "Epoch [30/3000], Loss: 0.01197449304163456\n",
      "Epoch [31/3000], Loss: 0.011972029693424702\n",
      "Epoch [32/3000], Loss: 0.01197013258934021\n",
      "Epoch [33/3000], Loss: 0.011968689039349556\n",
      "Epoch [34/3000], Loss: 0.01196759007871151\n",
      "Epoch [35/3000], Loss: 0.01196674257516861\n",
      "Epoch [36/3000], Loss: 0.011966054327785969\n",
      "Epoch [37/3000], Loss: 0.011965452693402767\n",
      "Epoch [38/3000], Loss: 0.011964878998696804\n",
      "Epoch [39/3000], Loss: 0.011964292265474796\n",
      "Epoch [40/3000], Loss: 0.011963678523898125\n",
      "Epoch [41/3000], Loss: 0.011963029392063618\n",
      "Epoch [42/3000], Loss: 0.01196235790848732\n",
      "Epoch [43/3000], Loss: 0.011961673386394978\n",
      "Epoch [44/3000], Loss: 0.011960992589592934\n",
      "Epoch [45/3000], Loss: 0.011960332281887531\n",
      "Epoch [46/3000], Loss: 0.011959701776504517\n",
      "Epoch [47/3000], Loss: 0.011959115043282509\n",
      "Epoch [48/3000], Loss: 0.011958576738834381\n",
      "Epoch [49/3000], Loss: 0.011958088725805283\n",
      "Epoch [50/3000], Loss: 0.011957652866840363\n",
      "Epoch [51/3000], Loss: 0.011957266367971897\n",
      "Epoch [52/3000], Loss: 0.011956923641264439\n",
      "Epoch [53/3000], Loss: 0.011956620961427689\n",
      "Epoch [54/3000], Loss: 0.01195635087788105\n",
      "Epoch [55/3000], Loss: 0.011956109665334225\n",
      "Epoch [56/3000], Loss: 0.011955891735851765\n",
      "Epoch [57/3000], Loss: 0.01195569522678852\n",
      "Epoch [58/3000], Loss: 0.011955516412854195\n",
      "Epoch [59/3000], Loss: 0.011955353431403637\n",
      "Epoch [60/3000], Loss: 0.011955206282436848\n",
      "Epoch [61/3000], Loss: 0.011955076828598976\n",
      "Epoch [62/3000], Loss: 0.011954961344599724\n",
      "Epoch [63/3000], Loss: 0.01195486169308424\n",
      "Epoch [64/3000], Loss: 0.0119547750800848\n",
      "Epoch [65/3000], Loss: 0.011954703368246555\n",
      "Epoch [66/3000], Loss: 0.011954640038311481\n",
      "Epoch [67/3000], Loss: 0.011954589746892452\n",
      "Epoch [68/3000], Loss: 0.01195454504340887\n",
      "Epoch [69/3000], Loss: 0.011954505927860737\n",
      "Epoch [70/3000], Loss: 0.011954471468925476\n",
      "Epoch [71/3000], Loss: 0.011954435147345066\n",
      "Epoch [72/3000], Loss: 0.01195439975708723\n",
      "Epoch [73/3000], Loss: 0.011954360641539097\n",
      "Epoch [74/3000], Loss: 0.011954322457313538\n",
      "Epoch [75/3000], Loss: 0.01195428054779768\n",
      "Epoch [76/3000], Loss: 0.011954238638281822\n",
      "Epoch [77/3000], Loss: 0.011954196728765965\n",
      "Epoch [78/3000], Loss: 0.011954155750572681\n",
      "Epoch [79/3000], Loss: 0.011954117566347122\n",
      "Epoch [80/3000], Loss: 0.011954083107411861\n",
      "Epoch [81/3000], Loss: 0.011954052373766899\n",
      "Epoch [82/3000], Loss: 0.01195402443408966\n",
      "Epoch [83/3000], Loss: 0.011954003013670444\n",
      "Epoch [84/3000], Loss: 0.011953982524573803\n",
      "Epoch [85/3000], Loss: 0.01195396576076746\n",
      "Epoch [86/3000], Loss: 0.011953949928283691\n",
      "Epoch [87/3000], Loss: 0.011953935958445072\n",
      "Epoch [88/3000], Loss: 0.011953921057283878\n",
      "Epoch [89/3000], Loss: 0.011953907087445259\n",
      "Epoch [90/3000], Loss: 0.011953892186284065\n",
      "Epoch [91/3000], Loss: 0.011953878216445446\n",
      "Epoch [92/3000], Loss: 0.011953861452639103\n",
      "Epoch [93/3000], Loss: 0.011953846551477909\n",
      "Epoch [94/3000], Loss: 0.011953831650316715\n",
      "Epoch [95/3000], Loss: 0.011953815817832947\n",
      "Epoch [96/3000], Loss: 0.011953800916671753\n",
      "Epoch [97/3000], Loss: 0.011953786015510559\n",
      "Epoch [98/3000], Loss: 0.011953771114349365\n",
      "Epoch [99/3000], Loss: 0.011953756213188171\n",
      "Epoch [100/3000], Loss: 0.011953741312026978\n",
      "Epoch [101/3000], Loss: 0.011953727342188358\n",
      "Epoch [102/3000], Loss: 0.011953710578382015\n",
      "Epoch [103/3000], Loss: 0.011953696608543396\n",
      "Epoch [104/3000], Loss: 0.011953682638704777\n",
      "Epoch [105/3000], Loss: 0.011953667737543583\n",
      "Epoch [106/3000], Loss: 0.011953652836382389\n",
      "Epoch [107/3000], Loss: 0.01195363700389862\n",
      "Epoch [108/3000], Loss: 0.011953621171414852\n",
      "Epoch [109/3000], Loss: 0.011953606270253658\n",
      "Epoch [110/3000], Loss: 0.01195359043776989\n",
      "Epoch [111/3000], Loss: 0.011953573673963547\n",
      "Epoch [112/3000], Loss: 0.011953557841479778\n",
      "Epoch [113/3000], Loss: 0.011953541077673435\n",
      "Epoch [114/3000], Loss: 0.011953523382544518\n",
      "Epoch [115/3000], Loss: 0.011953506618738174\n",
      "Epoch [116/3000], Loss: 0.011953488923609257\n",
      "Epoch [117/3000], Loss: 0.011953471228480339\n",
      "Epoch [118/3000], Loss: 0.011953453533351421\n",
      "Epoch [119/3000], Loss: 0.011953433975577354\n",
      "Epoch [120/3000], Loss: 0.011953414417803288\n",
      "Epoch [121/3000], Loss: 0.01195339486002922\n",
      "Epoch [122/3000], Loss: 0.011953375302255154\n",
      "Epoch [123/3000], Loss: 0.011953354813158512\n",
      "Epoch [124/3000], Loss: 0.011953332461416721\n",
      "Epoch [125/3000], Loss: 0.011953311040997505\n",
      "Epoch [126/3000], Loss: 0.011953288689255714\n",
      "Epoch [127/3000], Loss: 0.011953265406191349\n",
      "Epoch [128/3000], Loss: 0.011953243985772133\n",
      "Epoch [129/3000], Loss: 0.011953219771385193\n",
      "Epoch [130/3000], Loss: 0.011953194625675678\n",
      "Epoch [131/3000], Loss: 0.011953169479966164\n",
      "Epoch [132/3000], Loss: 0.011953143402934074\n",
      "Epoch [133/3000], Loss: 0.011953117325901985\n",
      "Epoch [134/3000], Loss: 0.011953088454902172\n",
      "Epoch [135/3000], Loss: 0.011953060515224934\n",
      "Epoch [136/3000], Loss: 0.01195303164422512\n",
      "Epoch [137/3000], Loss: 0.011953000910580158\n",
      "Epoch [138/3000], Loss: 0.01195297110825777\n",
      "Epoch [139/3000], Loss: 0.011952938511967659\n",
      "Epoch [140/3000], Loss: 0.011952905915677547\n",
      "Epoch [141/3000], Loss: 0.011952871456742287\n",
      "Epoch [142/3000], Loss: 0.011952836997807026\n",
      "Epoch [143/3000], Loss: 0.011952800676226616\n",
      "Epoch [144/3000], Loss: 0.011952762492001057\n",
      "Epoch [145/3000], Loss: 0.011952724307775497\n",
      "Epoch [146/3000], Loss: 0.011952684260904789\n",
      "Epoch [147/3000], Loss: 0.01195264421403408\n",
      "Epoch [148/3000], Loss: 0.011952599510550499\n",
      "Epoch [149/3000], Loss: 0.011952554807066917\n",
      "Epoch [150/3000], Loss: 0.011952510103583336\n",
      "Epoch [151/3000], Loss: 0.011952461674809456\n",
      "Epoch [152/3000], Loss: 0.011952412314713001\n",
      "Epoch [153/3000], Loss: 0.011952362023293972\n",
      "Epoch [154/3000], Loss: 0.011952308006584644\n",
      "Epoch [155/3000], Loss: 0.011952253058552742\n",
      "Epoch [156/3000], Loss: 0.011952197179198265\n",
      "Epoch [157/3000], Loss: 0.011952136643230915\n",
      "Epoch [158/3000], Loss: 0.011952074244618416\n",
      "Epoch [159/3000], Loss: 0.011952010914683342\n",
      "Epoch [160/3000], Loss: 0.011951942928135395\n",
      "Epoch [161/3000], Loss: 0.011951874010264874\n",
      "Epoch [162/3000], Loss: 0.011951802298426628\n",
      "Epoch [163/3000], Loss: 0.01195172592997551\n",
      "Epoch [164/3000], Loss: 0.011951646767556667\n",
      "Epoch [165/3000], Loss: 0.011951565742492676\n",
      "Epoch [166/3000], Loss: 0.011951480060815811\n",
      "Epoch [167/3000], Loss: 0.011951391585171223\n",
      "Epoch [168/3000], Loss: 0.011951299384236336\n",
      "Epoch [169/3000], Loss: 0.011951202526688576\n",
      "Epoch [170/3000], Loss: 0.011951101943850517\n",
      "Epoch [171/3000], Loss: 0.011950996704399586\n",
      "Epoch [172/3000], Loss: 0.011950886808335781\n",
      "Epoch [173/3000], Loss: 0.011950771324336529\n",
      "Epoch [174/3000], Loss: 0.011950653046369553\n",
      "Epoch [175/3000], Loss: 0.01195052545517683\n",
      "Epoch [176/3000], Loss: 0.011950395070016384\n",
      "Epoch [177/3000], Loss: 0.011950257234275341\n",
      "Epoch [178/3000], Loss: 0.011950112879276276\n",
      "Epoch [179/3000], Loss: 0.011949962005019188\n",
      "Epoch [180/3000], Loss: 0.011949802748858929\n",
      "Epoch [181/3000], Loss: 0.011949636042118073\n",
      "Epoch [182/3000], Loss: 0.011949460953474045\n",
      "Epoch [183/3000], Loss: 0.011949277482926846\n",
      "Epoch [184/3000], Loss: 0.011949083767831326\n",
      "Epoch [185/3000], Loss: 0.01194888073951006\n",
      "Epoch [186/3000], Loss: 0.011948665603995323\n",
      "Epoch [187/3000], Loss: 0.011948440223932266\n",
      "Epoch [188/3000], Loss: 0.01194820273667574\n",
      "Epoch [189/3000], Loss: 0.011947951279580593\n",
      "Epoch [190/3000], Loss: 0.011947684921324253\n",
      "Epoch [191/3000], Loss: 0.011947407387197018\n",
      "Epoch [192/3000], Loss: 0.01194711122661829\n",
      "Epoch [193/3000], Loss: 0.011946799233555794\n",
      "Epoch [194/3000], Loss: 0.01194646954536438\n",
      "Epoch [195/3000], Loss: 0.01194611843675375\n",
      "Epoch [196/3000], Loss: 0.011945748701691628\n",
      "Epoch [197/3000], Loss: 0.011945356614887714\n",
      "Epoch [198/3000], Loss: 0.011944941245019436\n",
      "Epoch [199/3000], Loss: 0.011944497935473919\n",
      "Epoch [200/3000], Loss: 0.011944030411541462\n",
      "Epoch [201/3000], Loss: 0.011943532153964043\n",
      "Epoch [202/3000], Loss: 0.011943002231419086\n",
      "Epoch [203/3000], Loss: 0.011942438781261444\n",
      "Epoch [204/3000], Loss: 0.011941840872168541\n",
      "Epoch [205/3000], Loss: 0.01194120291620493\n",
      "Epoch [206/3000], Loss: 0.011940523982048035\n",
      "Epoch [207/3000], Loss: 0.011939803138375282\n",
      "Epoch [208/3000], Loss: 0.0119390357285738\n",
      "Epoch [209/3000], Loss: 0.011938217096030712\n",
      "Epoch [210/3000], Loss: 0.011937346309423447\n",
      "Epoch [211/3000], Loss: 0.01193641684949398\n",
      "Epoch [212/3000], Loss: 0.011935429647564888\n",
      "Epoch [213/3000], Loss: 0.011934376321732998\n",
      "Epoch [214/3000], Loss: 0.011933255940675735\n",
      "Epoch [215/3000], Loss: 0.01193206012248993\n",
      "Epoch [216/3000], Loss: 0.01193078700453043\n",
      "Epoch [217/3000], Loss: 0.011929430067539215\n",
      "Epoch [218/3000], Loss: 0.011927983723580837\n",
      "Epoch [219/3000], Loss: 0.011926441453397274\n",
      "Epoch [220/3000], Loss: 0.011924796737730503\n",
      "Epoch [221/3000], Loss: 0.011923043057322502\n",
      "Epoch [222/3000], Loss: 0.011921173892915249\n",
      "Epoch [223/3000], Loss: 0.011919179931282997\n",
      "Epoch [224/3000], Loss: 0.0119170555844903\n",
      "Epoch [225/3000], Loss: 0.011914791539311409\n",
      "Epoch [226/3000], Loss: 0.011912381276488304\n",
      "Epoch [227/3000], Loss: 0.011909814551472664\n",
      "Epoch [228/3000], Loss: 0.011907083913683891\n",
      "Epoch [229/3000], Loss: 0.011904182843863964\n",
      "Epoch [230/3000], Loss: 0.01190110482275486\n",
      "Epoch [231/3000], Loss: 0.011897843331098557\n",
      "Epoch [232/3000], Loss: 0.011894398368895054\n",
      "Epoch [233/3000], Loss: 0.011890762485563755\n",
      "Epoch [234/3000], Loss: 0.011886940337717533\n",
      "Epoch [235/3000], Loss: 0.011882931925356388\n",
      "Epoch [236/3000], Loss: 0.011878743767738342\n",
      "Epoch [237/3000], Loss: 0.011874385178089142\n",
      "Epoch [238/3000], Loss: 0.011869869194924831\n",
      "Epoch [239/3000], Loss: 0.011865217238664627\n",
      "Epoch [240/3000], Loss: 0.011860447935760021\n",
      "Epoch [241/3000], Loss: 0.011855593882501125\n",
      "Epoch [242/3000], Loss: 0.011850684881210327\n",
      "Epoch [243/3000], Loss: 0.011845762841403484\n",
      "Epoch [244/3000], Loss: 0.01184087060391903\n",
      "Epoch [245/3000], Loss: 0.011836054734885693\n",
      "Epoch [246/3000], Loss: 0.01183136273175478\n",
      "Epoch [247/3000], Loss: 0.011826838366687298\n",
      "Epoch [248/3000], Loss: 0.011822517961263657\n",
      "Epoch [249/3000], Loss: 0.011818436905741692\n",
      "Epoch [250/3000], Loss: 0.011814607307314873\n",
      "Epoch [251/3000], Loss: 0.011811034753918648\n",
      "Epoch [252/3000], Loss: 0.011807705275714397\n",
      "Epoch [253/3000], Loss: 0.011804594658315182\n",
      "Epoch [254/3000], Loss: 0.01180166658014059\n",
      "Epoch [255/3000], Loss: 0.011798885650932789\n",
      "Epoch [256/3000], Loss: 0.01179621834307909\n",
      "Epoch [257/3000], Loss: 0.011793641373515129\n",
      "Epoch [258/3000], Loss: 0.011791141703724861\n",
      "Epoch [259/3000], Loss: 0.01178872212767601\n",
      "Epoch [260/3000], Loss: 0.011786392889916897\n",
      "Epoch [261/3000], Loss: 0.011784171685576439\n",
      "Epoch [262/3000], Loss: 0.011782074347138405\n",
      "Epoch [263/3000], Loss: 0.01178011205047369\n",
      "Epoch [264/3000], Loss: 0.011778293177485466\n",
      "Epoch [265/3000], Loss: 0.011776614002883434\n",
      "Epoch [266/3000], Loss: 0.011775068007409573\n",
      "Epoch [267/3000], Loss: 0.011773638427257538\n",
      "Epoch [268/3000], Loss: 0.011772310361266136\n",
      "Epoch [269/3000], Loss: 0.011771063320338726\n",
      "Epoch [270/3000], Loss: 0.011769887991249561\n",
      "Epoch [271/3000], Loss: 0.011768762953579426\n",
      "Epoch [272/3000], Loss: 0.011767681688070297\n",
      "Epoch [273/3000], Loss: 0.01176663488149643\n",
      "Epoch [274/3000], Loss: 0.011765618808567524\n",
      "Epoch [275/3000], Loss: 0.011764632537961006\n",
      "Epoch [276/3000], Loss: 0.011763676069676876\n",
      "Epoch [277/3000], Loss: 0.011762751266360283\n",
      "Epoch [278/3000], Loss: 0.011761859990656376\n",
      "Epoch [279/3000], Loss: 0.011761005967855453\n",
      "Epoch [280/3000], Loss: 0.011760188266634941\n",
      "Epoch [281/3000], Loss: 0.011759412474930286\n",
      "Epoch [282/3000], Loss: 0.011758670210838318\n",
      "Epoch [283/3000], Loss: 0.011757967993617058\n",
      "Epoch [284/3000], Loss: 0.011757297441363335\n",
      "Epoch [285/3000], Loss: 0.011756659485399723\n",
      "Epoch [286/3000], Loss: 0.0117560513317585\n",
      "Epoch [287/3000], Loss: 0.011755469255149364\n",
      "Epoch [288/3000], Loss: 0.01175491139292717\n",
      "Epoch [289/3000], Loss: 0.011754377745091915\n",
      "Epoch [290/3000], Loss: 0.0117538683116436\n",
      "Epoch [291/3000], Loss: 0.011753381229937077\n",
      "Epoch [292/3000], Loss: 0.011752917431294918\n",
      "Epoch [293/3000], Loss: 0.011752473190426826\n",
      "Epoch [294/3000], Loss: 0.011752048507332802\n",
      "Epoch [295/3000], Loss: 0.011751644313335419\n",
      "Epoch [296/3000], Loss: 0.011751256883144379\n",
      "Epoch [297/3000], Loss: 0.011750883422791958\n",
      "Epoch [298/3000], Loss: 0.011750524863600731\n",
      "Epoch [299/3000], Loss: 0.011750179342925549\n",
      "Epoch [300/3000], Loss: 0.011749844998121262\n",
      "Epoch [301/3000], Loss: 0.01174951996654272\n",
      "Epoch [302/3000], Loss: 0.011749206110835075\n",
      "Epoch [303/3000], Loss: 0.011748900637030602\n",
      "Epoch [304/3000], Loss: 0.011748602613806725\n",
      "Epoch [305/3000], Loss: 0.011748312041163445\n",
      "Epoch [306/3000], Loss: 0.011748028919100761\n",
      "Epoch [307/3000], Loss: 0.0117477523162961\n",
      "Epoch [308/3000], Loss: 0.011747480370104313\n",
      "Epoch [309/3000], Loss: 0.011747213080525398\n",
      "Epoch [310/3000], Loss: 0.011746952310204506\n",
      "Epoch [311/3000], Loss: 0.011746692471206188\n",
      "Epoch [312/3000], Loss: 0.011746435426175594\n",
      "Epoch [313/3000], Loss: 0.011746182106435299\n",
      "Epoch [314/3000], Loss: 0.011745930649340153\n",
      "Epoch [315/3000], Loss: 0.011745679192245007\n",
      "Epoch [316/3000], Loss: 0.01174542959779501\n",
      "Epoch [317/3000], Loss: 0.011745180003345013\n",
      "Epoch [318/3000], Loss: 0.011744929477572441\n",
      "Epoch [319/3000], Loss: 0.01174467895179987\n",
      "Epoch [320/3000], Loss: 0.011744428426027298\n",
      "Epoch [321/3000], Loss: 0.011744176037609577\n",
      "Epoch [322/3000], Loss: 0.011743922717869282\n",
      "Epoch [323/3000], Loss: 0.011743665672838688\n",
      "Epoch [324/3000], Loss: 0.01174340769648552\n",
      "Epoch [325/3000], Loss: 0.011743145063519478\n",
      "Epoch [326/3000], Loss: 0.011742879636585712\n",
      "Epoch [327/3000], Loss: 0.011742610484361649\n",
      "Epoch [328/3000], Loss: 0.011742336675524712\n",
      "Epoch [329/3000], Loss: 0.011742057278752327\n",
      "Epoch [330/3000], Loss: 0.011741772294044495\n",
      "Epoch [331/3000], Loss: 0.01174148079007864\n",
      "Epoch [332/3000], Loss: 0.011741184629499912\n",
      "Epoch [333/3000], Loss: 0.011740878224372864\n",
      "Epoch [334/3000], Loss: 0.011740565299987793\n",
      "Epoch [335/3000], Loss: 0.011740241199731827\n",
      "Epoch [336/3000], Loss: 0.011739908717572689\n",
      "Epoch [337/3000], Loss: 0.01173956599086523\n",
      "Epoch [338/3000], Loss: 0.011739210225641727\n",
      "Epoch [339/3000], Loss: 0.011738842353224754\n",
      "Epoch [340/3000], Loss: 0.011738459579646587\n",
      "Epoch [341/3000], Loss: 0.01173806469887495\n",
      "Epoch [342/3000], Loss: 0.011737652122974396\n",
      "Epoch [343/3000], Loss: 0.011737222783267498\n",
      "Epoch [344/3000], Loss: 0.011736776679754257\n",
      "Epoch [345/3000], Loss: 0.0117363091558218\n",
      "Epoch [346/3000], Loss: 0.011735822074115276\n",
      "Epoch [347/3000], Loss: 0.011735310778021812\n",
      "Epoch [348/3000], Loss: 0.011734775267541409\n",
      "Epoch [349/3000], Loss: 0.011734211817383766\n",
      "Epoch [350/3000], Loss: 0.01173361949622631\n",
      "Epoch [351/3000], Loss: 0.011732997372746468\n",
      "Epoch [352/3000], Loss: 0.011732339859008789\n",
      "Epoch [353/3000], Loss: 0.011731646955013275\n",
      "Epoch [354/3000], Loss: 0.011730913072824478\n",
      "Epoch [355/3000], Loss: 0.011730137281119823\n",
      "Epoch [356/3000], Loss: 0.011729315854609013\n",
      "Epoch [357/3000], Loss: 0.011728445068001747\n",
      "Epoch [358/3000], Loss: 0.011727521196007729\n",
      "Epoch [359/3000], Loss: 0.01172653865069151\n",
      "Epoch [360/3000], Loss: 0.011725496500730515\n",
      "Epoch [361/3000], Loss: 0.011724387295544147\n",
      "Epoch [362/3000], Loss: 0.01172320730984211\n",
      "Epoch [363/3000], Loss: 0.011721950024366379\n",
      "Epoch [364/3000], Loss: 0.011720609851181507\n",
      "Epoch [365/3000], Loss: 0.011719184927642345\n",
      "Epoch [366/3000], Loss: 0.011717665940523148\n",
      "Epoch [367/3000], Loss: 0.01171604823321104\n",
      "Epoch [368/3000], Loss: 0.011714327149093151\n",
      "Epoch [369/3000], Loss: 0.011712498031556606\n",
      "Epoch [370/3000], Loss: 0.011710556223988533\n",
      "Epoch [371/3000], Loss: 0.011708494275808334\n",
      "Epoch [372/3000], Loss: 0.01170631404966116\n",
      "Epoch [373/3000], Loss: 0.011704008094966412\n",
      "Epoch [374/3000], Loss: 0.011701575480401516\n",
      "Epoch [375/3000], Loss: 0.011699015274643898\n",
      "Epoch [376/3000], Loss: 0.011696324683725834\n",
      "Epoch [377/3000], Loss: 0.011693506501615047\n",
      "Epoch [378/3000], Loss: 0.011690563522279263\n",
      "Epoch [379/3000], Loss: 0.011687498539686203\n",
      "Epoch [380/3000], Loss: 0.011684317141771317\n",
      "Epoch [381/3000], Loss: 0.011681030504405499\n",
      "Epoch [382/3000], Loss: 0.011677649803459644\n",
      "Epoch [383/3000], Loss: 0.011674189940094948\n",
      "Epoch [384/3000], Loss: 0.011670663952827454\n",
      "Epoch [385/3000], Loss: 0.011667091399431229\n",
      "Epoch [386/3000], Loss: 0.01166348997503519\n",
      "Epoch [387/3000], Loss: 0.011659882962703705\n",
      "Epoch [388/3000], Loss: 0.011656287126243114\n",
      "Epoch [389/3000], Loss: 0.01165272481739521\n",
      "Epoch [390/3000], Loss: 0.011649212799966335\n",
      "Epoch [391/3000], Loss: 0.011645767837762833\n",
      "Epoch [392/3000], Loss: 0.011642404831945896\n",
      "Epoch [393/3000], Loss: 0.011639128439128399\n",
      "Epoch [394/3000], Loss: 0.011635947972536087\n",
      "Epoch [395/3000], Loss: 0.01163286343216896\n",
      "Epoch [396/3000], Loss: 0.011629872024059296\n",
      "Epoch [397/3000], Loss: 0.011626970022916794\n",
      "Epoch [398/3000], Loss: 0.011624149978160858\n",
      "Epoch [399/3000], Loss: 0.011621401645243168\n",
      "Epoch [400/3000], Loss: 0.011618715710937977\n",
      "Epoch [401/3000], Loss: 0.011616082862019539\n",
      "Epoch [402/3000], Loss: 0.011613497510552406\n",
      "Epoch [403/3000], Loss: 0.011610951274633408\n",
      "Epoch [404/3000], Loss: 0.011608440428972244\n",
      "Epoch [405/3000], Loss: 0.011605963110923767\n",
      "Epoch [406/3000], Loss: 0.011603513732552528\n",
      "Epoch [407/3000], Loss: 0.011601094156503677\n",
      "Epoch [408/3000], Loss: 0.011598700657486916\n",
      "Epoch [409/3000], Loss: 0.011596333235502243\n",
      "Epoch [410/3000], Loss: 0.011593984439969063\n",
      "Epoch [411/3000], Loss: 0.01159165520220995\n",
      "Epoch [412/3000], Loss: 0.011589338071644306\n",
      "Epoch [413/3000], Loss: 0.01158702839165926\n",
      "Epoch [414/3000], Loss: 0.011584720574319363\n",
      "Epoch [415/3000], Loss: 0.011582409031689167\n",
      "Epoch [416/3000], Loss: 0.011580083519220352\n",
      "Epoch [417/3000], Loss: 0.01157774031162262\n",
      "Epoch [418/3000], Loss: 0.011575371026992798\n",
      "Epoch [419/3000], Loss: 0.01157296635210514\n",
      "Epoch [420/3000], Loss: 0.011570519767701626\n",
      "Epoch [421/3000], Loss: 0.011568024754524231\n",
      "Epoch [422/3000], Loss: 0.01156547013670206\n",
      "Epoch [423/3000], Loss: 0.011562850326299667\n",
      "Epoch [424/3000], Loss: 0.01156015694141388\n",
      "Epoch [425/3000], Loss: 0.01155738066881895\n",
      "Epoch [426/3000], Loss: 0.011554514057934284\n",
      "Epoch [427/3000], Loss: 0.01155154686421156\n",
      "Epoch [428/3000], Loss: 0.01154846977442503\n",
      "Epoch [429/3000], Loss: 0.011545274406671524\n",
      "Epoch [430/3000], Loss: 0.011541946791112423\n",
      "Epoch [431/3000], Loss: 0.011538473889231682\n",
      "Epoch [432/3000], Loss: 0.011534844525158405\n",
      "Epoch [433/3000], Loss: 0.011531045660376549\n",
      "Epoch [434/3000], Loss: 0.011527063325047493\n",
      "Epoch [435/3000], Loss: 0.01152288168668747\n",
      "Epoch [436/3000], Loss: 0.011518488638103008\n",
      "Epoch [437/3000], Loss: 0.011513869278132915\n",
      "Epoch [438/3000], Loss: 0.011509012430906296\n",
      "Epoch [439/3000], Loss: 0.01150390226393938\n",
      "Epoch [440/3000], Loss: 0.01149852853268385\n",
      "Epoch [441/3000], Loss: 0.011492888443171978\n",
      "Epoch [442/3000], Loss: 0.011486981064081192\n",
      "Epoch [443/3000], Loss: 0.011480807326734066\n",
      "Epoch [444/3000], Loss: 0.01147438120096922\n",
      "Epoch [445/3000], Loss: 0.011467714793980122\n",
      "Epoch [446/3000], Loss: 0.01146083977073431\n",
      "Epoch [447/3000], Loss: 0.011453787796199322\n",
      "Epoch [448/3000], Loss: 0.011446598917245865\n",
      "Epoch [449/3000], Loss: 0.011439322493970394\n",
      "Epoch [450/3000], Loss: 0.011432008817791939\n",
      "Epoch [451/3000], Loss: 0.011424710974097252\n",
      "Epoch [452/3000], Loss: 0.011417476460337639\n",
      "Epoch [453/3000], Loss: 0.011410337872803211\n",
      "Epoch [454/3000], Loss: 0.011403320357203484\n",
      "Epoch [455/3000], Loss: 0.011396434158086777\n",
      "Epoch [456/3000], Loss: 0.011389680206775665\n",
      "Epoch [457/3000], Loss: 0.011383059434592724\n",
      "Epoch [458/3000], Loss: 0.011376575566828251\n",
      "Epoch [459/3000], Loss: 0.011370242573320866\n",
      "Epoch [460/3000], Loss: 0.011364077217876911\n",
      "Epoch [461/3000], Loss: 0.011358111165463924\n",
      "Epoch [462/3000], Loss: 0.01135236956179142\n",
      "Epoch [463/3000], Loss: 0.011346870101988316\n",
      "Epoch [464/3000], Loss: 0.011341622099280357\n",
      "Epoch [465/3000], Loss: 0.011336622759699821\n",
      "Epoch [466/3000], Loss: 0.011331863701343536\n",
      "Epoch [467/3000], Loss: 0.011327331885695457\n",
      "Epoch [468/3000], Loss: 0.01132301427423954\n",
      "Epoch [469/3000], Loss: 0.011318900622427464\n",
      "Epoch [470/3000], Loss: 0.011314978823065758\n",
      "Epoch [471/3000], Loss: 0.0113112423568964\n",
      "Epoch [472/3000], Loss: 0.01130768097937107\n",
      "Epoch [473/3000], Loss: 0.011304281651973724\n",
      "Epoch [474/3000], Loss: 0.011301033198833466\n",
      "Epoch [475/3000], Loss: 0.011297918856143951\n",
      "Epoch [476/3000], Loss: 0.011294924654066563\n",
      "Epoch [477/3000], Loss: 0.011292037554085255\n",
      "Epoch [478/3000], Loss: 0.011289243586361408\n",
      "Epoch [479/3000], Loss: 0.011286529712378979\n",
      "Epoch [480/3000], Loss: 0.011283887550234795\n",
      "Epoch [481/3000], Loss: 0.011281310580670834\n",
      "Epoch [482/3000], Loss: 0.011278793215751648\n",
      "Epoch [483/3000], Loss: 0.011276336386799812\n",
      "Epoch [484/3000], Loss: 0.011273938231170177\n",
      "Epoch [485/3000], Loss: 0.011271599680185318\n",
      "Epoch [486/3000], Loss: 0.01126931793987751\n",
      "Epoch [487/3000], Loss: 0.011267093941569328\n",
      "Epoch [488/3000], Loss: 0.011264924891293049\n",
      "Epoch [489/3000], Loss: 0.011262811720371246\n",
      "Epoch [490/3000], Loss: 0.011260753497481346\n",
      "Epoch [491/3000], Loss: 0.011258743703365326\n",
      "Epoch [492/3000], Loss: 0.01125678513199091\n",
      "Epoch [493/3000], Loss: 0.01125487219542265\n",
      "Epoch [494/3000], Loss: 0.01125300396233797\n",
      "Epoch [495/3000], Loss: 0.01125117763876915\n",
      "Epoch [496/3000], Loss: 0.011249389499425888\n",
      "Epoch [497/3000], Loss: 0.011247636750340462\n",
      "Epoch [498/3000], Loss: 0.011245918460190296\n",
      "Epoch [499/3000], Loss: 0.011244232766330242\n",
      "Epoch [500/3000], Loss: 0.011242575012147427\n",
      "Epoch [501/3000], Loss: 0.011240941472351551\n",
      "Epoch [502/3000], Loss: 0.011239334009587765\n",
      "Epoch [503/3000], Loss: 0.01123774889856577\n",
      "Epoch [504/3000], Loss: 0.011236180551350117\n",
      "Epoch [505/3000], Loss: 0.011234631761908531\n",
      "Epoch [506/3000], Loss: 0.01123309601098299\n",
      "Epoch [507/3000], Loss: 0.011231575161218643\n",
      "Epoch [508/3000], Loss: 0.011230064556002617\n",
      "Epoch [509/3000], Loss: 0.011228562332689762\n",
      "Epoch [510/3000], Loss: 0.01122706662863493\n",
      "Epoch [511/3000], Loss: 0.011225574649870396\n",
      "Epoch [512/3000], Loss: 0.011224082671105862\n",
      "Epoch [513/3000], Loss: 0.01122258696705103\n",
      "Epoch [514/3000], Loss: 0.011221085675060749\n",
      "Epoch [515/3000], Loss: 0.011219573207199574\n",
      "Epoch [516/3000], Loss: 0.011218048632144928\n",
      "Epoch [517/3000], Loss: 0.01121650543063879\n",
      "Epoch [518/3000], Loss: 0.011214938946068287\n",
      "Epoch [519/3000], Loss: 0.011213346384465694\n",
      "Epoch [520/3000], Loss: 0.011211724020540714\n",
      "Epoch [521/3000], Loss: 0.011210068129003048\n",
      "Epoch [522/3000], Loss: 0.01120836939662695\n",
      "Epoch [523/3000], Loss: 0.01120662596076727\n",
      "Epoch [524/3000], Loss: 0.011204829439520836\n",
      "Epoch [525/3000], Loss: 0.011202974244952202\n",
      "Epoch [526/3000], Loss: 0.011201055720448494\n",
      "Epoch [527/3000], Loss: 0.011199062690138817\n",
      "Epoch [528/3000], Loss: 0.011196993291378021\n",
      "Epoch [529/3000], Loss: 0.011194834485650063\n",
      "Epoch [530/3000], Loss: 0.011192581616342068\n",
      "Epoch [531/3000], Loss: 0.011190228164196014\n",
      "Epoch [532/3000], Loss: 0.011187763884663582\n",
      "Epoch [533/3000], Loss: 0.011185182258486748\n",
      "Epoch [534/3000], Loss: 0.011182477697730064\n",
      "Epoch [535/3000], Loss: 0.011179642751812935\n",
      "Epoch [536/3000], Loss: 0.01117667555809021\n",
      "Epoch [537/3000], Loss: 0.011173575185239315\n",
      "Epoch [538/3000], Loss: 0.011170340701937675\n",
      "Epoch [539/3000], Loss: 0.01116697583347559\n",
      "Epoch [540/3000], Loss: 0.011163490824401379\n",
      "Epoch [541/3000], Loss: 0.011159896850585938\n",
      "Epoch [542/3000], Loss: 0.011156213469803333\n",
      "Epoch [543/3000], Loss: 0.011152461171150208\n",
      "Epoch [544/3000], Loss: 0.011148666962981224\n",
      "Epoch [545/3000], Loss: 0.011144867166876793\n",
      "Epoch [546/3000], Loss: 0.011141091585159302\n",
      "Epoch [547/3000], Loss: 0.011137374676764011\n",
      "Epoch [548/3000], Loss: 0.01113374624401331\n",
      "Epoch [549/3000], Loss: 0.011130223050713539\n",
      "Epoch [550/3000], Loss: 0.011126813478767872\n",
      "Epoch [551/3000], Loss: 0.011123505420982838\n",
      "Epoch [552/3000], Loss: 0.011120273731648922\n",
      "Epoch [553/3000], Loss: 0.01111708302050829\n",
      "Epoch [554/3000], Loss: 0.011113891378045082\n",
      "Epoch [555/3000], Loss: 0.011110661551356316\n",
      "Epoch [556/3000], Loss: 0.011107371188700199\n",
      "Epoch [557/3000], Loss: 0.01110401377081871\n",
      "Epoch [558/3000], Loss: 0.011100598610937595\n",
      "Epoch [559/3000], Loss: 0.011097155511379242\n",
      "Epoch [560/3000], Loss: 0.011093705892562866\n",
      "Epoch [561/3000], Loss: 0.011090278625488281\n",
      "Epoch [562/3000], Loss: 0.011086893267929554\n",
      "Epoch [563/3000], Loss: 0.011083557270467281\n",
      "Epoch [564/3000], Loss: 0.011080273427069187\n",
      "Epoch [565/3000], Loss: 0.0110770333558321\n",
      "Epoch [566/3000], Loss: 0.01107382494956255\n",
      "Epoch [567/3000], Loss: 0.011070636101067066\n",
      "Epoch [568/3000], Loss: 0.011067458428442478\n",
      "Epoch [569/3000], Loss: 0.011064286343753338\n",
      "Epoch [570/3000], Loss: 0.011061117053031921\n",
      "Epoch [571/3000], Loss: 0.011057958006858826\n",
      "Epoch [572/3000], Loss: 0.011054815724492073\n",
      "Epoch [573/3000], Loss: 0.011051706969738007\n",
      "Epoch [574/3000], Loss: 0.011048640124499798\n",
      "Epoch [575/3000], Loss: 0.011045629158616066\n",
      "Epoch [576/3000], Loss: 0.011042686179280281\n",
      "Epoch [577/3000], Loss: 0.011039819568395615\n",
      "Epoch [578/3000], Loss: 0.011037030257284641\n",
      "Epoch [579/3000], Loss: 0.011034321039915085\n",
      "Epoch [580/3000], Loss: 0.011031686328351498\n",
      "Epoch [581/3000], Loss: 0.011029118672013283\n",
      "Epoch [582/3000], Loss: 0.011026610620319843\n",
      "Epoch [583/3000], Loss: 0.011024153791368008\n",
      "Epoch [584/3000], Loss: 0.01102173887193203\n",
      "Epoch [585/3000], Loss: 0.011019363068044186\n",
      "Epoch [586/3000], Loss: 0.011017022654414177\n",
      "Epoch [587/3000], Loss: 0.011014715768396854\n",
      "Epoch [588/3000], Loss: 0.011012447066605091\n",
      "Epoch [589/3000], Loss: 0.011010237038135529\n",
      "Epoch [590/3000], Loss: 0.011008096858859062\n",
      "Epoch [591/3000], Loss: 0.011005952954292297\n",
      "Epoch [592/3000], Loss: 0.011003663763403893\n",
      "Epoch [593/3000], Loss: 0.011001329869031906\n",
      "Epoch [594/3000], Loss: 0.01099918782711029\n",
      "Epoch [595/3000], Loss: 0.010997099801898003\n",
      "Epoch [596/3000], Loss: 0.010994837619364262\n",
      "Epoch [597/3000], Loss: 0.010992558673024178\n",
      "Epoch [598/3000], Loss: 0.010990404523909092\n",
      "Epoch [599/3000], Loss: 0.010988179594278336\n",
      "Epoch [600/3000], Loss: 0.010985832661390305\n",
      "Epoch [601/3000], Loss: 0.01098354160785675\n",
      "Epoch [602/3000], Loss: 0.010981242172420025\n",
      "Epoch [603/3000], Loss: 0.010978815145790577\n",
      "Epoch [604/3000], Loss: 0.010976370424032211\n",
      "Epoch [605/3000], Loss: 0.010973935946822166\n",
      "Epoch [606/3000], Loss: 0.010971400886774063\n",
      "Epoch [607/3000], Loss: 0.010968814603984356\n",
      "Epoch [608/3000], Loss: 0.010966243222355843\n",
      "Epoch [609/3000], Loss: 0.010963612236082554\n",
      "Epoch [610/3000], Loss: 0.010960939340293407\n",
      "Epoch [611/3000], Loss: 0.010958297178149223\n",
      "Epoch [612/3000], Loss: 0.010955652222037315\n",
      "Epoch [613/3000], Loss: 0.010953006334602833\n",
      "Epoch [614/3000], Loss: 0.010950424708425999\n",
      "Epoch [615/3000], Loss: 0.01094790454953909\n",
      "Epoch [616/3000], Loss: 0.010945437476038933\n",
      "Epoch [617/3000], Loss: 0.010943070985376835\n",
      "Epoch [618/3000], Loss: 0.010940808802843094\n",
      "Epoch [619/3000], Loss: 0.010938628576695919\n",
      "Epoch [620/3000], Loss: 0.01093654427677393\n",
      "Epoch [621/3000], Loss: 0.010934550315141678\n",
      "Epoch [622/3000], Loss: 0.01093260943889618\n",
      "Epoch [623/3000], Loss: 0.010930705815553665\n",
      "Epoch [624/3000], Loss: 0.01092883013188839\n",
      "Epoch [625/3000], Loss: 0.010926959104835987\n",
      "Epoch [626/3000], Loss: 0.010925080627202988\n",
      "Epoch [627/3000], Loss: 0.010923206806182861\n",
      "Epoch [628/3000], Loss: 0.010921342298388481\n",
      "Epoch [629/3000], Loss: 0.01091949176043272\n",
      "Epoch [630/3000], Loss: 0.01091767754405737\n",
      "Epoch [631/3000], Loss: 0.010915911756455898\n",
      "Epoch [632/3000], Loss: 0.010914197191596031\n",
      "Epoch [633/3000], Loss: 0.01091254036873579\n",
      "Epoch [634/3000], Loss: 0.0109109440818429\n",
      "Epoch [635/3000], Loss: 0.010909398086369038\n",
      "Epoch [636/3000], Loss: 0.010907894000411034\n",
      "Epoch [637/3000], Loss: 0.01090642623603344\n",
      "Epoch [638/3000], Loss: 0.010904984548687935\n",
      "Epoch [639/3000], Loss: 0.0109035549685359\n",
      "Epoch [640/3000], Loss: 0.010902132838964462\n",
      "Epoch [641/3000], Loss: 0.010900710709393024\n",
      "Epoch [642/3000], Loss: 0.01089928112924099\n",
      "Epoch [643/3000], Loss: 0.01089784037321806\n",
      "Epoch [644/3000], Loss: 0.010896386578679085\n",
      "Epoch [645/3000], Loss: 0.010894916951656342\n",
      "Epoch [646/3000], Loss: 0.010893428698182106\n",
      "Epoch [647/3000], Loss: 0.010891924612224102\n",
      "Epoch [648/3000], Loss: 0.010890400037169456\n",
      "Epoch [649/3000], Loss: 0.010888855904340744\n",
      "Epoch [650/3000], Loss: 0.01088728941977024\n",
      "Epoch [651/3000], Loss: 0.010885699652135372\n",
      "Epoch [652/3000], Loss: 0.010884083807468414\n",
      "Epoch [653/3000], Loss: 0.010882439091801643\n",
      "Epoch [654/3000], Loss: 0.010880764573812485\n",
      "Epoch [655/3000], Loss: 0.010879055596888065\n",
      "Epoch [656/3000], Loss: 0.010877309367060661\n",
      "Epoch [657/3000], Loss: 0.010875524953007698\n",
      "Epoch [658/3000], Loss: 0.010873701423406601\n",
      "Epoch [659/3000], Loss: 0.010871830396354198\n",
      "Epoch [660/3000], Loss: 0.010869917459785938\n",
      "Epoch [661/3000], Loss: 0.010867955163121223\n",
      "Epoch [662/3000], Loss: 0.010865945369005203\n",
      "Epoch [663/3000], Loss: 0.010863886214792728\n",
      "Epoch [664/3000], Loss: 0.010861778631806374\n",
      "Epoch [665/3000], Loss: 0.010859622620046139\n",
      "Epoch [666/3000], Loss: 0.010857420973479748\n",
      "Epoch [667/3000], Loss: 0.010855177417397499\n",
      "Epoch [668/3000], Loss: 0.010852893814444542\n",
      "Epoch [669/3000], Loss: 0.010850580409169197\n",
      "Epoch [670/3000], Loss: 0.010848241858184338\n",
      "Epoch [671/3000], Loss: 0.010845890268683434\n",
      "Epoch [672/3000], Loss: 0.010843534953892231\n",
      "Epoch [673/3000], Loss: 0.010841191746294498\n",
      "Epoch [674/3000], Loss: 0.010838869027793407\n",
      "Epoch [675/3000], Loss: 0.010836586356163025\n",
      "Epoch [676/3000], Loss: 0.010834353975951672\n",
      "Epoch [677/3000], Loss: 0.010832183994352818\n",
      "Epoch [678/3000], Loss: 0.010830080136656761\n",
      "Epoch [679/3000], Loss: 0.010828048922121525\n",
      "Epoch [680/3000], Loss: 0.010826087556779385\n",
      "Epoch [681/3000], Loss: 0.010824187658727169\n",
      "Epoch [682/3000], Loss: 0.010822336189448833\n",
      "Epoch [683/3000], Loss: 0.010820519179105759\n",
      "Epoch [684/3000], Loss: 0.010818719863891602\n",
      "Epoch [685/3000], Loss: 0.010816920548677444\n",
      "Epoch [686/3000], Loss: 0.01081511378288269\n",
      "Epoch [687/3000], Loss: 0.010813294909894466\n",
      "Epoch [688/3000], Loss: 0.010811463929712772\n",
      "Epoch [689/3000], Loss: 0.010809626430273056\n",
      "Epoch [690/3000], Loss: 0.010807792656123638\n",
      "Epoch [691/3000], Loss: 0.010805975645780563\n",
      "Epoch [692/3000], Loss: 0.010804185643792152\n",
      "Epoch [693/3000], Loss: 0.010802430100739002\n",
      "Epoch [694/3000], Loss: 0.01080071646720171\n",
      "Epoch [695/3000], Loss: 0.0107990438118577\n",
      "Epoch [696/3000], Loss: 0.01079740934073925\n",
      "Epoch [697/3000], Loss: 0.010795810259878635\n",
      "Epoch [698/3000], Loss: 0.01079423725605011\n",
      "Epoch [699/3000], Loss: 0.010792684741318226\n",
      "Epoch [700/3000], Loss: 0.010791140608489513\n",
      "Epoch [701/3000], Loss: 0.01078959833830595\n",
      "Epoch [702/3000], Loss: 0.01078805048018694\n",
      "Epoch [703/3000], Loss: 0.010786491446197033\n",
      "Epoch [704/3000], Loss: 0.010784913785755634\n",
      "Epoch [705/3000], Loss: 0.01078331470489502\n",
      "Epoch [706/3000], Loss: 0.010781691409647465\n",
      "Epoch [707/3000], Loss: 0.010780039243400097\n",
      "Epoch [708/3000], Loss: 0.010778358206152916\n",
      "Epoch [709/3000], Loss: 0.010776646435260773\n",
      "Epoch [710/3000], Loss: 0.010774901136755943\n",
      "Epoch [711/3000], Loss: 0.010773121379315853\n",
      "Epoch [712/3000], Loss: 0.010771302506327629\n",
      "Epoch [713/3000], Loss: 0.010769443586468697\n",
      "Epoch [714/3000], Loss: 0.010767543688416481\n",
      "Epoch [715/3000], Loss: 0.01076559815555811\n",
      "Epoch [716/3000], Loss: 0.010763606987893581\n",
      "Epoch [717/3000], Loss: 0.010761570185422897\n",
      "Epoch [718/3000], Loss: 0.01075948216021061\n",
      "Epoch [719/3000], Loss: 0.010757345706224442\n",
      "Epoch [720/3000], Loss: 0.010755161754786968\n",
      "Epoch [721/3000], Loss: 0.010752931237220764\n",
      "Epoch [722/3000], Loss: 0.010750656947493553\n",
      "Epoch [723/3000], Loss: 0.010748343542218208\n",
      "Epoch [724/3000], Loss: 0.010745997540652752\n",
      "Epoch [725/3000], Loss: 0.010743623599410057\n",
      "Epoch [726/3000], Loss: 0.01074123289436102\n",
      "Epoch [727/3000], Loss: 0.010738832876086235\n",
      "Epoch [728/3000], Loss: 0.010736433789134026\n",
      "Epoch [729/3000], Loss: 0.010734044946730137\n",
      "Epoch [730/3000], Loss: 0.010731672868132591\n",
      "Epoch [731/3000], Loss: 0.010729329660534859\n",
      "Epoch [732/3000], Loss: 0.010727016255259514\n",
      "Epoch [733/3000], Loss: 0.010724733583629131\n",
      "Epoch [734/3000], Loss: 0.010722479782998562\n",
      "Epoch [735/3000], Loss: 0.010720246471464634\n",
      "Epoch [736/3000], Loss: 0.010718019679188728\n",
      "Epoch [737/3000], Loss: 0.0107157863676548\n",
      "Epoch [738/3000], Loss: 0.010713528841733932\n",
      "Epoch [739/3000], Loss: 0.010711230337619781\n",
      "Epoch [740/3000], Loss: 0.010708881542086601\n",
      "Epoch [741/3000], Loss: 0.010706469416618347\n",
      "Epoch [742/3000], Loss: 0.010703991167247295\n",
      "Epoch [743/3000], Loss: 0.010701448656618595\n",
      "Epoch [744/3000], Loss: 0.010698850266635418\n",
      "Epoch [745/3000], Loss: 0.01069620717316866\n",
      "Epoch [746/3000], Loss: 0.010693532414734364\n",
      "Epoch [747/3000], Loss: 0.0106908418238163\n",
      "Epoch [748/3000], Loss: 0.010688151232898235\n",
      "Epoch [749/3000], Loss: 0.01068547647446394\n",
      "Epoch [750/3000], Loss: 0.010682832449674606\n",
      "Epoch [751/3000], Loss: 0.010680233128368855\n",
      "Epoch [752/3000], Loss: 0.010677696205675602\n",
      "Epoch [753/3000], Loss: 0.010675235651433468\n",
      "Epoch [754/3000], Loss: 0.010672866366803646\n",
      "Epoch [755/3000], Loss: 0.010670596733689308\n",
      "Epoch [756/3000], Loss: 0.010668436996638775\n",
      "Epoch [757/3000], Loss: 0.01066638994961977\n",
      "Epoch [758/3000], Loss: 0.01066445279866457\n",
      "Epoch [759/3000], Loss: 0.010662619024515152\n",
      "Epoch [760/3000], Loss: 0.010660871863365173\n",
      "Epoch [761/3000], Loss: 0.010659193620085716\n",
      "Epoch [762/3000], Loss: 0.01065756008028984\n",
      "Epoch [763/3000], Loss: 0.01065594982355833\n",
      "Epoch [764/3000], Loss: 0.010654350742697716\n",
      "Epoch [765/3000], Loss: 0.010652749799191952\n",
      "Epoch [766/3000], Loss: 0.010651147924363613\n",
      "Epoch [767/3000], Loss: 0.010649550706148148\n",
      "Epoch [768/3000], Loss: 0.0106479711830616\n",
      "Epoch [769/3000], Loss: 0.010646420530974865\n",
      "Epoch [770/3000], Loss: 0.010644917376339436\n",
      "Epoch [771/3000], Loss: 0.010643471032381058\n",
      "Epoch [772/3000], Loss: 0.010642089881002903\n",
      "Epoch [773/3000], Loss: 0.010640778578817844\n",
      "Epoch [774/3000], Loss: 0.010639537125825882\n",
      "Epoch [775/3000], Loss: 0.010638359002768993\n",
      "Epoch [776/3000], Loss: 0.01063724048435688\n",
      "Epoch [777/3000], Loss: 0.010636171326041222\n",
      "Epoch [778/3000], Loss: 0.010635148733854294\n",
      "Epoch [779/3000], Loss: 0.010634159669280052\n",
      "Epoch [780/3000], Loss: 0.010633205994963646\n",
      "Epoch [781/3000], Loss: 0.010632281191647053\n",
      "Epoch [782/3000], Loss: 0.010631388984620571\n",
      "Epoch [783/3000], Loss: 0.010630527511239052\n",
      "Epoch [784/3000], Loss: 0.010629676282405853\n",
      "Epoch [785/3000], Loss: 0.010628798045217991\n",
      "Epoch [786/3000], Loss: 0.010627874173223972\n",
      "Epoch [787/3000], Loss: 0.01062696147710085\n",
      "Epoch [788/3000], Loss: 0.010626121424138546\n",
      "Epoch [789/3000], Loss: 0.010625332593917847\n",
      "Epoch [790/3000], Loss: 0.010624527931213379\n",
      "Epoch [791/3000], Loss: 0.010623677633702755\n",
      "Epoch [792/3000], Loss: 0.010622835718095303\n",
      "Epoch [793/3000], Loss: 0.01062204409390688\n",
      "Epoch [794/3000], Loss: 0.010621277615427971\n",
      "Epoch [795/3000], Loss: 0.01062048226594925\n",
      "Epoch [796/3000], Loss: 0.010619664564728737\n",
      "Epoch [797/3000], Loss: 0.010618867352604866\n",
      "Epoch [798/3000], Loss: 0.010618097148835659\n",
      "Epoch [799/3000], Loss: 0.010617319494485855\n",
      "Epoch [800/3000], Loss: 0.010616514831781387\n",
      "Epoch [801/3000], Loss: 0.01061571016907692\n",
      "Epoch [802/3000], Loss: 0.01061492133885622\n",
      "Epoch [803/3000], Loss: 0.01061413623392582\n",
      "Epoch [804/3000], Loss: 0.010613330639898777\n",
      "Epoch [805/3000], Loss: 0.010612512938678265\n",
      "Epoch [806/3000], Loss: 0.01061169896274805\n",
      "Epoch [807/3000], Loss: 0.010610886849462986\n",
      "Epoch [808/3000], Loss: 0.010610061697661877\n",
      "Epoch [809/3000], Loss: 0.010609216056764126\n",
      "Epoch [810/3000], Loss: 0.010608363896608353\n",
      "Epoch [811/3000], Loss: 0.010607507079839706\n",
      "Epoch [812/3000], Loss: 0.010606635361909866\n",
      "Epoch [813/3000], Loss: 0.010605743154883385\n",
      "Epoch [814/3000], Loss: 0.01060483232140541\n",
      "Epoch [815/3000], Loss: 0.010603905655443668\n",
      "Epoch [816/3000], Loss: 0.010602962225675583\n",
      "Epoch [817/3000], Loss: 0.010601994581520557\n",
      "Epoch [818/3000], Loss: 0.010600998066365719\n",
      "Epoch [819/3000], Loss: 0.010599975474178791\n",
      "Epoch [820/3000], Loss: 0.0105989258736372\n",
      "Epoch [821/3000], Loss: 0.010597847402095795\n",
      "Epoch [822/3000], Loss: 0.010596731677651405\n",
      "Epoch [823/3000], Loss: 0.01059558056294918\n",
      "Epoch [824/3000], Loss: 0.010594388470053673\n",
      "Epoch [825/3000], Loss: 0.010593156330287457\n",
      "Epoch [826/3000], Loss: 0.010591879487037659\n",
      "Epoch [827/3000], Loss: 0.010590555146336555\n",
      "Epoch [828/3000], Loss: 0.010589179582893848\n",
      "Epoch [829/3000], Loss: 0.010587749071419239\n",
      "Epoch [830/3000], Loss: 0.010586263611912727\n",
      "Epoch [831/3000], Loss: 0.01058472041040659\n",
      "Epoch [832/3000], Loss: 0.010583116672933102\n",
      "Epoch [833/3000], Loss: 0.01058144960552454\n",
      "Epoch [834/3000], Loss: 0.01057971827685833\n",
      "Epoch [835/3000], Loss: 0.010577925480902195\n",
      "Epoch [836/3000], Loss: 0.01057607214897871\n",
      "Epoch [837/3000], Loss: 0.0105741610750556\n",
      "Epoch [838/3000], Loss: 0.010572200641036034\n",
      "Epoch [839/3000], Loss: 0.010570199228823185\n",
      "Epoch [840/3000], Loss: 0.010568165220320225\n",
      "Epoch [841/3000], Loss: 0.01056610606610775\n",
      "Epoch [842/3000], Loss: 0.010564020834863186\n",
      "Epoch [843/3000], Loss: 0.010561896488070488\n",
      "Epoch [844/3000], Loss: 0.010559742338955402\n",
      "Epoch [845/3000], Loss: 0.010557606816291809\n",
      "Epoch [846/3000], Loss: 0.010555548593401909\n",
      "Epoch [847/3000], Loss: 0.01055359747260809\n",
      "Epoch [848/3000], Loss: 0.010551734827458858\n",
      "Epoch [849/3000], Loss: 0.010549936443567276\n",
      "Epoch [850/3000], Loss: 0.010548188351094723\n",
      "Epoch [851/3000], Loss: 0.010546506382524967\n",
      "Epoch [852/3000], Loss: 0.010544908232986927\n",
      "Epoch [853/3000], Loss: 0.01054338738322258\n",
      "Epoch [854/3000], Loss: 0.01054191030561924\n",
      "Epoch [855/3000], Loss: 0.010540435090661049\n",
      "Epoch [856/3000], Loss: 0.01053893193602562\n",
      "Epoch [857/3000], Loss: 0.010537389665842056\n",
      "Epoch [858/3000], Loss: 0.010535809211432934\n",
      "Epoch [859/3000], Loss: 0.010534192435443401\n",
      "Epoch [860/3000], Loss: 0.010532539337873459\n",
      "Epoch [861/3000], Loss: 0.010530859231948853\n",
      "Epoch [862/3000], Loss: 0.010529166087508202\n",
      "Epoch [863/3000], Loss: 0.010527479462325573\n",
      "Epoch [864/3000], Loss: 0.010525811463594437\n",
      "Epoch [865/3000], Loss: 0.010524163022637367\n",
      "Epoch [866/3000], Loss: 0.010522527620196342\n",
      "Epoch [867/3000], Loss: 0.010520901530981064\n",
      "Epoch [868/3000], Loss: 0.010519282892346382\n",
      "Epoch [869/3000], Loss: 0.010517669841647148\n",
      "Epoch [870/3000], Loss: 0.010516060516238213\n",
      "Epoch [871/3000], Loss: 0.010514442808926105\n",
      "Epoch [872/3000], Loss: 0.010512808337807655\n",
      "Epoch [873/3000], Loss: 0.010511146858334541\n",
      "Epoch [874/3000], Loss: 0.010509462095797062\n",
      "Epoch [875/3000], Loss: 0.010507752187550068\n",
      "Epoch [876/3000], Loss: 0.010506021790206432\n",
      "Epoch [877/3000], Loss: 0.010504272766411304\n",
      "Epoch [878/3000], Loss: 0.010502510704100132\n",
      "Epoch [879/3000], Loss: 0.010500737465918064\n",
      "Epoch [880/3000], Loss: 0.01049896702170372\n",
      "Epoch [881/3000], Loss: 0.010497208684682846\n",
      "Epoch [882/3000], Loss: 0.010495473630726337\n",
      "Epoch [883/3000], Loss: 0.01049377303570509\n",
      "Epoch [884/3000], Loss: 0.010492118075489998\n",
      "Epoch [885/3000], Loss: 0.010490518063306808\n",
      "Epoch [886/3000], Loss: 0.010488980449736118\n",
      "Epoch [887/3000], Loss: 0.010487508028745651\n",
      "Epoch [888/3000], Loss: 0.010486102662980556\n",
      "Epoch [889/3000], Loss: 0.010484760627150536\n",
      "Epoch [890/3000], Loss: 0.010483475401997566\n",
      "Epoch [891/3000], Loss: 0.010482236742973328\n",
      "Epoch [892/3000], Loss: 0.010481033474206924\n",
      "Epoch [893/3000], Loss: 0.010479847900569439\n",
      "Epoch [894/3000], Loss: 0.010478673502802849\n",
      "Epoch [895/3000], Loss: 0.010477499105036259\n",
      "Epoch [896/3000], Loss: 0.010476320050656796\n",
      "Epoch [897/3000], Loss: 0.01047513633966446\n",
      "Epoch [898/3000], Loss: 0.010473955422639847\n",
      "Epoch [899/3000], Loss: 0.010472786612808704\n",
      "Epoch [900/3000], Loss: 0.010471648536622524\n",
      "Epoch [901/3000], Loss: 0.010470567271113396\n",
      "Epoch [902/3000], Loss: 0.010469562374055386\n",
      "Epoch [903/3000], Loss: 0.010468609631061554\n",
      "Epoch [904/3000], Loss: 0.010467571206390858\n",
      "Epoch [905/3000], Loss: 0.01046637911349535\n",
      "Epoch [906/3000], Loss: 0.010465223342180252\n",
      "Epoch [907/3000], Loss: 0.010464315302670002\n",
      "Epoch [908/3000], Loss: 0.010463518090546131\n",
      "Epoch [909/3000], Loss: 0.010462597012519836\n",
      "Epoch [910/3000], Loss: 0.01046161912381649\n",
      "Epoch [911/3000], Loss: 0.010460779070854187\n",
      "Epoch [912/3000], Loss: 0.010459999553859234\n",
      "Epoch [913/3000], Loss: 0.01045912317931652\n",
      "Epoch [914/3000], Loss: 0.010458236560225487\n",
      "Epoch [915/3000], Loss: 0.010457453317940235\n",
      "Epoch [916/3000], Loss: 0.010456658899784088\n",
      "Epoch [917/3000], Loss: 0.010455784387886524\n",
      "Epoch [918/3000], Loss: 0.01045494619756937\n",
      "Epoch [919/3000], Loss: 0.010454169474542141\n",
      "Epoch [920/3000], Loss: 0.010453343391418457\n",
      "Epoch [921/3000], Loss: 0.010452475398778915\n",
      "Epoch [922/3000], Loss: 0.010451662354171276\n",
      "Epoch [923/3000], Loss: 0.010450862348079681\n",
      "Epoch [924/3000], Loss: 0.010450011119246483\n",
      "Epoch [925/3000], Loss: 0.010449155233800411\n",
      "Epoch [926/3000], Loss: 0.010448331013321877\n",
      "Epoch [927/3000], Loss: 0.010447489097714424\n",
      "Epoch [928/3000], Loss: 0.01044661458581686\n",
      "Epoch [929/3000], Loss: 0.010445749387145042\n",
      "Epoch [930/3000], Loss: 0.0104448851197958\n",
      "Epoch [931/3000], Loss: 0.01044398918747902\n",
      "Epoch [932/3000], Loss: 0.010443075560033321\n",
      "Epoch [933/3000], Loss: 0.010442165657877922\n",
      "Epoch [934/3000], Loss: 0.010441241785883904\n",
      "Epoch [935/3000], Loss: 0.010440285317599773\n",
      "Epoch [936/3000], Loss: 0.010439313016831875\n",
      "Epoch [937/3000], Loss: 0.01043833140283823\n",
      "Epoch [938/3000], Loss: 0.010437325574457645\n",
      "Epoch [939/3000], Loss: 0.010436291806399822\n",
      "Epoch [940/3000], Loss: 0.010435238480567932\n",
      "Epoch [941/3000], Loss: 0.0104341646656394\n",
      "Epoch [942/3000], Loss: 0.010433059185743332\n",
      "Epoch [943/3000], Loss: 0.010431923903524876\n",
      "Epoch [944/3000], Loss: 0.01043076254427433\n",
      "Epoch [945/3000], Loss: 0.010429571382701397\n",
      "Epoch [946/3000], Loss: 0.010428343899548054\n",
      "Epoch [947/3000], Loss: 0.010427081026136875\n",
      "Epoch [948/3000], Loss: 0.01042578648775816\n",
      "Epoch [949/3000], Loss: 0.010424455627799034\n",
      "Epoch [950/3000], Loss: 0.010423085652291775\n",
      "Epoch [951/3000], Loss: 0.010421677492558956\n",
      "Epoch [952/3000], Loss: 0.010420233011245728\n",
      "Epoch [953/3000], Loss: 0.01041875034570694\n",
      "Epoch [954/3000], Loss: 0.010417230427265167\n",
      "Epoch [955/3000], Loss: 0.01041567511856556\n",
      "Epoch [956/3000], Loss: 0.01041408907622099\n",
      "Epoch [957/3000], Loss: 0.010412472300231457\n",
      "Epoch [958/3000], Loss: 0.010410827584564686\n",
      "Epoch [959/3000], Loss: 0.010409161448478699\n",
      "Epoch [960/3000], Loss: 0.010407481342554092\n",
      "Epoch [961/3000], Loss: 0.01040579192340374\n",
      "Epoch [962/3000], Loss: 0.010404103435575962\n",
      "Epoch [963/3000], Loss: 0.010402418673038483\n",
      "Epoch [964/3000], Loss: 0.010400751605629921\n",
      "Epoch [965/3000], Loss: 0.010399105027318\n",
      "Epoch [966/3000], Loss: 0.010397491045296192\n",
      "Epoch [967/3000], Loss: 0.010395912453532219\n",
      "Epoch [968/3000], Loss: 0.010394373908638954\n",
      "Epoch [969/3000], Loss: 0.010392879135906696\n",
      "Epoch [970/3000], Loss: 0.010391423478722572\n",
      "Epoch [971/3000], Loss: 0.010390005074441433\n",
      "Epoch [972/3000], Loss: 0.010388615541160107\n",
      "Epoch [973/3000], Loss: 0.010387244634330273\n",
      "Epoch [974/3000], Loss: 0.010385879315435886\n",
      "Epoch [975/3000], Loss: 0.01038451213389635\n",
      "Epoch [976/3000], Loss: 0.01038313563913107\n",
      "Epoch [977/3000], Loss: 0.010381747037172318\n",
      "Epoch [978/3000], Loss: 0.010380367748439312\n",
      "Epoch [979/3000], Loss: 0.010379050858318806\n",
      "Epoch [980/3000], Loss: 0.01037785317748785\n",
      "Epoch [981/3000], Loss: 0.010376662947237492\n",
      "Epoch [982/3000], Loss: 0.010375000536441803\n",
      "Epoch [983/3000], Loss: 0.010372853837907314\n",
      "Epoch [984/3000], Loss: 0.010371136479079723\n",
      "Epoch [985/3000], Loss: 0.010369923897087574\n",
      "Epoch [986/3000], Loss: 0.010368301533162594\n",
      "Epoch [987/3000], Loss: 0.010366334579885006\n",
      "Epoch [988/3000], Loss: 0.010364796966314316\n",
      "Epoch [989/3000], Loss: 0.010363327339291573\n",
      "Epoch [990/3000], Loss: 0.01036146655678749\n",
      "Epoch [991/3000], Loss: 0.010359731502830982\n",
      "Epoch [992/3000], Loss: 0.010358159430325031\n",
      "Epoch [993/3000], Loss: 0.010356350801885128\n",
      "Epoch [994/3000], Loss: 0.010354527272284031\n",
      "Epoch [995/3000], Loss: 0.01035279780626297\n",
      "Epoch [996/3000], Loss: 0.010350948199629784\n",
      "Epoch [997/3000], Loss: 0.010349069721996784\n",
      "Epoch [998/3000], Loss: 0.010347210802137852\n",
      "Epoch [999/3000], Loss: 0.01034529134631157\n",
      "Epoch [1000/3000], Loss: 0.010343365371227264\n",
      "Epoch [1001/3000], Loss: 0.010341430082917213\n",
      "Epoch [1002/3000], Loss: 0.010339461266994476\n",
      "Epoch [1003/3000], Loss: 0.010337519459426403\n",
      "Epoch [1004/3000], Loss: 0.010335578583180904\n",
      "Epoch [1005/3000], Loss: 0.010333620011806488\n",
      "Epoch [1006/3000], Loss: 0.010331704281270504\n",
      "Epoch [1007/3000], Loss: 0.01032982300966978\n",
      "Epoch [1008/3000], Loss: 0.01032793615013361\n",
      "Epoch [1009/3000], Loss: 0.010326092131435871\n",
      "Epoch [1010/3000], Loss: 0.010324305854737759\n",
      "Epoch [1011/3000], Loss: 0.010322525165975094\n",
      "Epoch [1012/3000], Loss: 0.01032076496630907\n",
      "Epoch [1013/3000], Loss: 0.010319055058062077\n",
      "Epoch [1014/3000], Loss: 0.010317354463040829\n",
      "Epoch [1015/3000], Loss: 0.010315646417438984\n",
      "Epoch [1016/3000], Loss: 0.010313952341675758\n",
      "Epoch [1017/3000], Loss: 0.010312254540622234\n",
      "Epoch [1018/3000], Loss: 0.010310528799891472\n",
      "Epoch [1019/3000], Loss: 0.010308786295354366\n",
      "Epoch [1020/3000], Loss: 0.010307024233043194\n",
      "Epoch [1021/3000], Loss: 0.010305228643119335\n",
      "Epoch [1022/3000], Loss: 0.010303409770131111\n",
      "Epoch [1023/3000], Loss: 0.01030157133936882\n",
      "Epoch [1024/3000], Loss: 0.010299710556864738\n",
      "Epoch [1025/3000], Loss: 0.010297851637005806\n",
      "Epoch [1026/3000], Loss: 0.010296019725501537\n",
      "Epoch [1027/3000], Loss: 0.010294239036738873\n",
      "Epoch [1028/3000], Loss: 0.01029251143336296\n",
      "Epoch [1029/3000], Loss: 0.010290702804923058\n",
      "Epoch [1030/3000], Loss: 0.010288595221936703\n",
      "Epoch [1031/3000], Loss: 0.010286323726177216\n",
      "Epoch [1032/3000], Loss: 0.010284315794706345\n",
      "Epoch [1033/3000], Loss: 0.010282575152814388\n",
      "Epoch [1034/3000], Loss: 0.010280704125761986\n",
      "Epoch [1035/3000], Loss: 0.01027858816087246\n",
      "Epoch [1036/3000], Loss: 0.01027656439691782\n",
      "Epoch [1037/3000], Loss: 0.010274774394929409\n",
      "Epoch [1038/3000], Loss: 0.01027294248342514\n",
      "Epoch [1039/3000], Loss: 0.010270954109728336\n",
      "Epoch [1040/3000], Loss: 0.010269058868288994\n",
      "Epoch [1041/3000], Loss: 0.010267351754009724\n",
      "Epoch [1042/3000], Loss: 0.010265628807246685\n",
      "Epoch [1043/3000], Loss: 0.0102638378739357\n",
      "Epoch [1044/3000], Loss: 0.010262155905365944\n",
      "Epoch [1045/3000], Loss: 0.010260611772537231\n",
      "Epoch [1046/3000], Loss: 0.010259067639708519\n",
      "Epoch [1047/3000], Loss: 0.010257512331008911\n",
      "Epoch [1048/3000], Loss: 0.010256045497953892\n",
      "Epoch [1049/3000], Loss: 0.010254659689962864\n",
      "Epoch [1050/3000], Loss: 0.010253271088004112\n",
      "Epoch [1051/3000], Loss: 0.010251886211335659\n",
      "Epoch [1052/3000], Loss: 0.010250551626086235\n",
      "Epoch [1053/3000], Loss: 0.01024925522506237\n",
      "Epoch [1054/3000], Loss: 0.010247945785522461\n",
      "Epoch [1055/3000], Loss: 0.010246632620692253\n",
      "Epoch [1056/3000], Loss: 0.010245357640087605\n",
      "Epoch [1057/3000], Loss: 0.010244113393127918\n",
      "Epoch [1058/3000], Loss: 0.010242867283523083\n",
      "Epoch [1059/3000], Loss: 0.010241629555821419\n",
      "Epoch [1060/3000], Loss: 0.010240436531603336\n",
      "Epoch [1061/3000], Loss: 0.010239304974675179\n",
      "Epoch [1062/3000], Loss: 0.010238226503133774\n",
      "Epoch [1063/3000], Loss: 0.010237198323011398\n",
      "Epoch [1064/3000], Loss: 0.010236185044050217\n",
      "Epoch [1065/3000], Loss: 0.010235084220767021\n",
      "Epoch [1066/3000], Loss: 0.010233817622065544\n",
      "Epoch [1067/3000], Loss: 0.01023253332823515\n",
      "Epoch [1068/3000], Loss: 0.0102314418181777\n",
      "Epoch [1069/3000], Loss: 0.01023051980882883\n",
      "Epoch [1070/3000], Loss: 0.010229557752609253\n",
      "Epoch [1071/3000], Loss: 0.010228445753455162\n",
      "Epoch [1072/3000], Loss: 0.010227303020656109\n",
      "Epoch [1073/3000], Loss: 0.010226275771856308\n",
      "Epoch [1074/3000], Loss: 0.010225323960185051\n",
      "Epoch [1075/3000], Loss: 0.01022431068122387\n",
      "Epoch [1076/3000], Loss: 0.010223211720585823\n",
      "Epoch [1077/3000], Loss: 0.010222128592431545\n",
      "Epoch [1078/3000], Loss: 0.010221115313470364\n",
      "Epoch [1079/3000], Loss: 0.010220101103186607\n",
      "Epoch [1080/3000], Loss: 0.010219022631645203\n",
      "Epoch [1081/3000], Loss: 0.010217909701168537\n",
      "Epoch [1082/3000], Loss: 0.010216828435659409\n",
      "Epoch [1083/3000], Loss: 0.010215768590569496\n",
      "Epoch [1084/3000], Loss: 0.010214673355221748\n",
      "Epoch [1085/3000], Loss: 0.01021352969110012\n",
      "Epoch [1086/3000], Loss: 0.010212384164333344\n",
      "Epoch [1087/3000], Loss: 0.01021125540137291\n",
      "Epoch [1088/3000], Loss: 0.010210109874606133\n",
      "Epoch [1089/3000], Loss: 0.0102089187130332\n",
      "Epoch [1090/3000], Loss: 0.01020770613104105\n",
      "Epoch [1091/3000], Loss: 0.010206492617726326\n",
      "Epoch [1092/3000], Loss: 0.010205273516476154\n",
      "Epoch [1093/3000], Loss: 0.010204021818935871\n",
      "Epoch [1094/3000], Loss: 0.010202734731137753\n",
      "Epoch [1095/3000], Loss: 0.010201438330113888\n",
      "Epoch [1096/3000], Loss: 0.010200136341154575\n",
      "Epoch [1097/3000], Loss: 0.010198812000453472\n",
      "Epoch [1098/3000], Loss: 0.01019746158272028\n",
      "Epoch [1099/3000], Loss: 0.01019609346985817\n",
      "Epoch [1100/3000], Loss: 0.010194717906415462\n",
      "Epoch [1101/3000], Loss: 0.010193338617682457\n",
      "Epoch [1102/3000], Loss: 0.010191947221755981\n",
      "Epoch [1103/3000], Loss: 0.01019054464995861\n",
      "Epoch [1104/3000], Loss: 0.010189145803451538\n",
      "Epoch [1105/3000], Loss: 0.010187752544879913\n",
      "Epoch [1106/3000], Loss: 0.010186371393501759\n",
      "Epoch [1107/3000], Loss: 0.010185002349317074\n",
      "Epoch [1108/3000], Loss: 0.010183651000261307\n",
      "Epoch [1109/3000], Loss: 0.010182322934269905\n",
      "Epoch [1110/3000], Loss: 0.010181027464568615\n",
      "Epoch [1111/3000], Loss: 0.010179766453802586\n",
      "Epoch [1112/3000], Loss: 0.010178539901971817\n",
      "Epoch [1113/3000], Loss: 0.010177353397011757\n",
      "Epoch [1114/3000], Loss: 0.010176205076277256\n",
      "Epoch [1115/3000], Loss: 0.010175099596381187\n",
      "Epoch [1116/3000], Loss: 0.010174031369388103\n",
      "Epoch [1117/3000], Loss: 0.01017299946397543\n",
      "Epoch [1118/3000], Loss: 0.010171997360885143\n",
      "Epoch [1119/3000], Loss: 0.010171021334826946\n",
      "Epoch [1120/3000], Loss: 0.010170064866542816\n",
      "Epoch [1121/3000], Loss: 0.010169124230742455\n",
      "Epoch [1122/3000], Loss: 0.010168195702135563\n",
      "Epoch [1123/3000], Loss: 0.010167273692786694\n",
      "Epoch [1124/3000], Loss: 0.010166355408728123\n",
      "Epoch [1125/3000], Loss: 0.010165441781282425\n",
      "Epoch [1126/3000], Loss: 0.010164530947804451\n",
      "Epoch [1127/3000], Loss: 0.010163622908294201\n",
      "Epoch [1128/3000], Loss: 0.010162721388041973\n",
      "Epoch [1129/3000], Loss: 0.010161826387047768\n",
      "Epoch [1130/3000], Loss: 0.010160940699279308\n",
      "Epoch [1131/3000], Loss: 0.010160066187381744\n",
      "Epoch [1132/3000], Loss: 0.010159204714000225\n",
      "Epoch [1133/3000], Loss: 0.010158357210457325\n",
      "Epoch [1134/3000], Loss: 0.010157523676753044\n",
      "Epoch [1135/3000], Loss: 0.010156703181564808\n",
      "Epoch [1136/3000], Loss: 0.010155895724892616\n",
      "Epoch [1137/3000], Loss: 0.01015510130673647\n",
      "Epoch [1138/3000], Loss: 0.010154317133128643\n",
      "Epoch [1139/3000], Loss: 0.010153539478778839\n",
      "Epoch [1140/3000], Loss: 0.010152770206332207\n",
      "Epoch [1141/3000], Loss: 0.010152006521821022\n",
      "Epoch [1142/3000], Loss: 0.010151245631277561\n",
      "Epoch [1143/3000], Loss: 0.0101504847407341\n",
      "Epoch [1144/3000], Loss: 0.010149722918868065\n",
      "Epoch [1145/3000], Loss: 0.01014895923435688\n",
      "Epoch [1146/3000], Loss: 0.010148191824555397\n",
      "Epoch [1147/3000], Loss: 0.01014741975814104\n",
      "Epoch [1148/3000], Loss: 0.010146641172468662\n",
      "Epoch [1149/3000], Loss: 0.010145856998860836\n",
      "Epoch [1150/3000], Loss: 0.010145063512027264\n",
      "Epoch [1151/3000], Loss: 0.010144263505935669\n",
      "Epoch [1152/3000], Loss: 0.010143453255295753\n",
      "Epoch [1153/3000], Loss: 0.010142631828784943\n",
      "Epoch [1154/3000], Loss: 0.010141798295080662\n",
      "Epoch [1155/3000], Loss: 0.01014095451682806\n",
      "Epoch [1156/3000], Loss: 0.01014009490609169\n",
      "Epoch [1157/3000], Loss: 0.01013922318816185\n",
      "Epoch [1158/3000], Loss: 0.010138335637748241\n",
      "Epoch [1159/3000], Loss: 0.01013743132352829\n",
      "Epoch [1160/3000], Loss: 0.01013650931417942\n",
      "Epoch [1161/3000], Loss: 0.010135569609701633\n",
      "Epoch [1162/3000], Loss: 0.01013461034744978\n",
      "Epoch [1163/3000], Loss: 0.010133628733456135\n",
      "Epoch [1164/3000], Loss: 0.010132623836398125\n",
      "Epoch [1165/3000], Loss: 0.010131597518920898\n",
      "Epoch [1166/3000], Loss: 0.010130546055734158\n",
      "Epoch [1167/3000], Loss: 0.010129468515515327\n",
      "Epoch [1168/3000], Loss: 0.010128364898264408\n",
      "Epoch [1169/3000], Loss: 0.010127234272658825\n",
      "Epoch [1170/3000], Loss: 0.010126076638698578\n",
      "Epoch [1171/3000], Loss: 0.010124890133738518\n",
      "Epoch [1172/3000], Loss: 0.010123677551746368\n",
      "Epoch [1173/3000], Loss: 0.010122436098754406\n",
      "Epoch [1174/3000], Loss: 0.010121166706085205\n",
      "Epoch [1175/3000], Loss: 0.010119873099029064\n",
      "Epoch [1176/3000], Loss: 0.010118553414940834\n",
      "Epoch [1177/3000], Loss: 0.010117212310433388\n",
      "Epoch [1178/3000], Loss: 0.0101158507168293\n",
      "Epoch [1179/3000], Loss: 0.010114472359418869\n",
      "Epoch [1180/3000], Loss: 0.010113079100847244\n",
      "Epoch [1181/3000], Loss: 0.010111680254340172\n",
      "Epoch [1182/3000], Loss: 0.010110274888575077\n",
      "Epoch [1183/3000], Loss: 0.010108872316777706\n",
      "Epoch [1184/3000], Loss: 0.010107476264238358\n",
      "Epoch [1185/3000], Loss: 0.010106096044182777\n",
      "Epoch [1186/3000], Loss: 0.01010473445057869\n",
      "Epoch [1187/3000], Loss: 0.010103399865329266\n",
      "Epoch [1188/3000], Loss: 0.010102096945047379\n",
      "Epoch [1189/3000], Loss: 0.01010083220899105\n",
      "Epoch [1190/3000], Loss: 0.01009960938245058\n",
      "Epoch [1191/3000], Loss: 0.010098431259393692\n",
      "Epoch [1192/3000], Loss: 0.010097303427755833\n",
      "Epoch [1193/3000], Loss: 0.01009623147547245\n",
      "Epoch [1194/3000], Loss: 0.010095223784446716\n",
      "Epoch [1195/3000], Loss: 0.01009428221732378\n",
      "Epoch [1196/3000], Loss: 0.010093390941619873\n",
      "Epoch [1197/3000], Loss: 0.01009250245988369\n",
      "Epoch [1198/3000], Loss: 0.010091585107147694\n",
      "Epoch [1199/3000], Loss: 0.01009077113121748\n",
      "Epoch [1200/3000], Loss: 0.010090355761349201\n",
      "Epoch [1201/3000], Loss: 0.010090019553899765\n",
      "Epoch [1202/3000], Loss: 0.010088417679071426\n",
      "Epoch [1203/3000], Loss: 0.010086223483085632\n",
      "Epoch [1204/3000], Loss: 0.010085636749863625\n",
      "Epoch [1205/3000], Loss: 0.010085206478834152\n",
      "Epoch [1206/3000], Loss: 0.010083362460136414\n",
      "Epoch [1207/3000], Loss: 0.010082357563078403\n",
      "Epoch [1208/3000], Loss: 0.010082014836370945\n",
      "Epoch [1209/3000], Loss: 0.010080453008413315\n",
      "Epoch [1210/3000], Loss: 0.01007946114987135\n",
      "Epoch [1211/3000], Loss: 0.010078995488584042\n",
      "Epoch [1212/3000], Loss: 0.010077638551592827\n",
      "Epoch [1213/3000], Loss: 0.010076813399791718\n",
      "Epoch [1214/3000], Loss: 0.010076100006699562\n",
      "Epoch [1215/3000], Loss: 0.010074918158352375\n",
      "Epoch [1216/3000], Loss: 0.010074238292872906\n",
      "Epoch [1217/3000], Loss: 0.010073302313685417\n",
      "Epoch [1218/3000], Loss: 0.010072289034724236\n",
      "Epoch [1219/3000], Loss: 0.010071586817502975\n",
      "Epoch [1220/3000], Loss: 0.010070562362670898\n",
      "Epoch [1221/3000], Loss: 0.010069678537547588\n",
      "Epoch [1222/3000], Loss: 0.010068823583424091\n",
      "Epoch [1223/3000], Loss: 0.010067827999591827\n",
      "Epoch [1224/3000], Loss: 0.010066972114145756\n",
      "Epoch [1225/3000], Loss: 0.010065985843539238\n",
      "Epoch [1226/3000], Loss: 0.01006503589451313\n",
      "Epoch [1227/3000], Loss: 0.01006411574780941\n",
      "Epoch [1228/3000], Loss: 0.01006308477371931\n",
      "Epoch [1229/3000], Loss: 0.01006213016808033\n",
      "Epoch [1230/3000], Loss: 0.010061115957796574\n",
      "Epoch [1231/3000], Loss: 0.010060083121061325\n",
      "Epoch [1232/3000], Loss: 0.01005907729268074\n",
      "Epoch [1233/3000], Loss: 0.010057995095849037\n",
      "Epoch [1234/3000], Loss: 0.01005694642663002\n",
      "Epoch [1235/3000], Loss: 0.01005586888641119\n",
      "Epoch [1236/3000], Loss: 0.010054755955934525\n",
      "Epoch [1237/3000], Loss: 0.010053664445877075\n",
      "Epoch [1238/3000], Loss: 0.010052526369690895\n",
      "Epoch [1239/3000], Loss: 0.010051391087472439\n",
      "Epoch [1240/3000], Loss: 0.010050252079963684\n",
      "Epoch [1241/3000], Loss: 0.010049078613519669\n",
      "Epoch [1242/3000], Loss: 0.010047920979559422\n",
      "Epoch [1243/3000], Loss: 0.010046749375760555\n",
      "Epoch [1244/3000], Loss: 0.010045568458735943\n",
      "Epoch [1245/3000], Loss: 0.010044400580227375\n",
      "Epoch [1246/3000], Loss: 0.010043224319815636\n",
      "Epoch [1247/3000], Loss: 0.010042061097919941\n",
      "Epoch [1248/3000], Loss: 0.01004091091454029\n",
      "Epoch [1249/3000], Loss: 0.010039767250418663\n",
      "Epoch [1250/3000], Loss: 0.010038649663329124\n",
      "Epoch [1251/3000], Loss: 0.010037550702691078\n",
      "Epoch [1252/3000], Loss: 0.010036476887762547\n",
      "Epoch [1253/3000], Loss: 0.010035434737801552\n",
      "Epoch [1254/3000], Loss: 0.010034418664872646\n",
      "Epoch [1255/3000], Loss: 0.010033436119556427\n",
      "Epoch [1256/3000], Loss: 0.010032488964498043\n",
      "Epoch [1257/3000], Loss: 0.010031570680439472\n",
      "Epoch [1258/3000], Loss: 0.010030685923993587\n",
      "Epoch [1259/3000], Loss: 0.010029829107224941\n",
      "Epoch [1260/3000], Loss: 0.010028998367488384\n",
      "Epoch [1261/3000], Loss: 0.010028190910816193\n",
      "Epoch [1262/3000], Loss: 0.010027401149272919\n",
      "Epoch [1263/3000], Loss: 0.010026625357568264\n",
      "Epoch [1264/3000], Loss: 0.01002585981041193\n",
      "Epoch [1265/3000], Loss: 0.010025101713836193\n",
      "Epoch [1266/3000], Loss: 0.01002434827387333\n",
      "Epoch [1267/3000], Loss: 0.010023594833910465\n",
      "Epoch [1268/3000], Loss: 0.0100228451192379\n",
      "Epoch [1269/3000], Loss: 0.010022096335887909\n",
      "Epoch [1270/3000], Loss: 0.010021353140473366\n",
      "Epoch [1271/3000], Loss: 0.010020620189607143\n",
      "Epoch [1272/3000], Loss: 0.010019910521805286\n",
      "Epoch [1273/3000], Loss: 0.010019240900874138\n",
      "Epoch [1274/3000], Loss: 0.010018625296652317\n",
      "Epoch [1275/3000], Loss: 0.010018045082688332\n",
      "Epoch [1276/3000], Loss: 0.010017404332756996\n",
      "Epoch [1277/3000], Loss: 0.010016574524343014\n",
      "Epoch [1278/3000], Loss: 0.010015596635639668\n",
      "Epoch [1279/3000], Loss: 0.010014714673161507\n",
      "Epoch [1280/3000], Loss: 0.010014078579843044\n",
      "Epoch [1281/3000], Loss: 0.010013527236878872\n",
      "Epoch [1282/3000], Loss: 0.010012822225689888\n",
      "Epoch [1283/3000], Loss: 0.010011980310082436\n",
      "Epoch [1284/3000], Loss: 0.010011224076151848\n",
      "Epoch [1285/3000], Loss: 0.010010614059865475\n",
      "Epoch [1286/3000], Loss: 0.01000997144728899\n",
      "Epoch [1287/3000], Loss: 0.010009204037487507\n",
      "Epoch [1288/3000], Loss: 0.010008437559008598\n",
      "Epoch [1289/3000], Loss: 0.01000776793807745\n",
      "Epoch [1290/3000], Loss: 0.010007101111114025\n",
      "Epoch [1291/3000], Loss: 0.01000635139644146\n",
      "Epoch [1292/3000], Loss: 0.010005583055317402\n",
      "Epoch [1293/3000], Loss: 0.01000486221164465\n",
      "Epoch [1294/3000], Loss: 0.010004148818552494\n",
      "Epoch [1295/3000], Loss: 0.01000338327139616\n",
      "Epoch [1296/3000], Loss: 0.010002595372498035\n",
      "Epoch [1297/3000], Loss: 0.010001827962696552\n",
      "Epoch [1298/3000], Loss: 0.010001062415540218\n",
      "Epoch [1299/3000], Loss: 0.010000264272093773\n",
      "Epoch [1300/3000], Loss: 0.009999442845582962\n",
      "Epoch [1301/3000], Loss: 0.00999862514436245\n",
      "Epoch [1302/3000], Loss: 0.009997805580496788\n",
      "Epoch [1303/3000], Loss: 0.00999696645885706\n",
      "Epoch [1304/3000], Loss: 0.009996103122830391\n",
      "Epoch [1305/3000], Loss: 0.009995232336223125\n",
      "Epoch [1306/3000], Loss: 0.009994356893002987\n",
      "Epoch [1307/3000], Loss: 0.009993469342589378\n",
      "Epoch [1308/3000], Loss: 0.009992565028369427\n",
      "Epoch [1309/3000], Loss: 0.009991648606956005\n",
      "Epoch [1310/3000], Loss: 0.009990724734961987\n",
      "Epoch [1311/3000], Loss: 0.009989799000322819\n",
      "Epoch [1312/3000], Loss: 0.00998886488378048\n",
      "Epoch [1313/3000], Loss: 0.009987922385334969\n",
      "Epoch [1314/3000], Loss: 0.00998697429895401\n",
      "Epoch [1315/3000], Loss: 0.009986026212573051\n",
      "Epoch [1316/3000], Loss: 0.009985080920159817\n",
      "Epoch [1317/3000], Loss: 0.009984137490391731\n",
      "Epoch [1318/3000], Loss: 0.009983195923268795\n",
      "Epoch [1319/3000], Loss: 0.009982259012758732\n",
      "Epoch [1320/3000], Loss: 0.009981333278119564\n",
      "Epoch [1321/3000], Loss: 0.009980425238609314\n",
      "Epoch [1322/3000], Loss: 0.009979529306292534\n",
      "Epoch [1323/3000], Loss: 0.009978649206459522\n",
      "Epoch [1324/3000], Loss: 0.009977788664400578\n",
      "Epoch [1325/3000], Loss: 0.009976951405405998\n",
      "Epoch [1326/3000], Loss: 0.009976139292120934\n",
      "Epoch [1327/3000], Loss: 0.009975350461900234\n",
      "Epoch [1328/3000], Loss: 0.00997458677738905\n",
      "Epoch [1329/3000], Loss: 0.009973850101232529\n",
      "Epoch [1330/3000], Loss: 0.009973137639462948\n",
      "Epoch [1331/3000], Loss: 0.00997245218604803\n",
      "Epoch [1332/3000], Loss: 0.00997178815305233\n",
      "Epoch [1333/3000], Loss: 0.00997114833444357\n",
      "Epoch [1334/3000], Loss: 0.009970526210963726\n",
      "Epoch [1335/3000], Loss: 0.0099699217826128\n",
      "Epoch [1336/3000], Loss: 0.009969335980713367\n",
      "Epoch [1337/3000], Loss: 0.009968766942620277\n",
      "Epoch [1338/3000], Loss: 0.009968220256268978\n",
      "Epoch [1339/3000], Loss: 0.009967711754143238\n",
      "Epoch [1340/3000], Loss: 0.009967280551791191\n",
      "Epoch [1341/3000], Loss: 0.009967018850147724\n",
      "Epoch [1342/3000], Loss: 0.009967047721147537\n",
      "Epoch [1343/3000], Loss: 0.009967266581952572\n",
      "Epoch [1344/3000], Loss: 0.009966901503503323\n",
      "Epoch [1345/3000], Loss: 0.009965350851416588\n",
      "Epoch [1346/3000], Loss: 0.009963973425328732\n",
      "Epoch [1347/3000], Loss: 0.009963984601199627\n",
      "Epoch [1348/3000], Loss: 0.009964078664779663\n",
      "Epoch [1349/3000], Loss: 0.009963012300431728\n",
      "Epoch [1350/3000], Loss: 0.009962056763470173\n",
      "Epoch [1351/3000], Loss: 0.00996208842843771\n",
      "Epoch [1352/3000], Loss: 0.009961756877601147\n",
      "Epoch [1353/3000], Loss: 0.009960792027413845\n",
      "Epoch [1354/3000], Loss: 0.009960428811609745\n",
      "Epoch [1355/3000], Loss: 0.009960309602320194\n",
      "Epoch [1356/3000], Loss: 0.009959609247744083\n",
      "Epoch [1357/3000], Loss: 0.009959070011973381\n",
      "Epoch [1358/3000], Loss: 0.009958918206393719\n",
      "Epoch [1359/3000], Loss: 0.009958415292203426\n",
      "Epoch [1360/3000], Loss: 0.00995786115527153\n",
      "Epoch [1361/3000], Loss: 0.00995763298124075\n",
      "Epoch [1362/3000], Loss: 0.00995722971856594\n",
      "Epoch [1363/3000], Loss: 0.009956713765859604\n",
      "Epoch [1364/3000], Loss: 0.009956437163054943\n",
      "Epoch [1365/3000], Loss: 0.009956078603863716\n",
      "Epoch [1366/3000], Loss: 0.00995559897273779\n",
      "Epoch [1367/3000], Loss: 0.009955295361578465\n",
      "Epoch [1368/3000], Loss: 0.009954961948096752\n",
      "Epoch [1369/3000], Loss: 0.009954513050615788\n",
      "Epoch [1370/3000], Loss: 0.009954187087714672\n",
      "Epoch [1371/3000], Loss: 0.009953868575394154\n",
      "Epoch [1372/3000], Loss: 0.009953449480235577\n",
      "Epoch [1373/3000], Loss: 0.009953104890882969\n",
      "Epoch [1374/3000], Loss: 0.009952793829143047\n",
      "Epoch [1375/3000], Loss: 0.009952402673661709\n",
      "Epoch [1376/3000], Loss: 0.009952045045793056\n",
      "Epoch [1377/3000], Loss: 0.009951731190085411\n",
      "Epoch [1378/3000], Loss: 0.009951364248991013\n",
      "Epoch [1379/3000], Loss: 0.009951002895832062\n",
      "Epoch [1380/3000], Loss: 0.009950680658221245\n",
      "Epoch [1381/3000], Loss: 0.009950329549610615\n",
      "Epoch [1382/3000], Loss: 0.009949970059096813\n",
      "Epoch [1383/3000], Loss: 0.009949639439582825\n",
      "Epoch [1384/3000], Loss: 0.009949296712875366\n",
      "Epoch [1385/3000], Loss: 0.009948940016329288\n",
      "Epoch [1386/3000], Loss: 0.009948602877557278\n",
      "Epoch [1387/3000], Loss: 0.009948262944817543\n",
      "Epoch [1388/3000], Loss: 0.00994790717959404\n",
      "Epoch [1389/3000], Loss: 0.009947565384209156\n",
      "Epoch [1390/3000], Loss: 0.009947224520146847\n",
      "Epoch [1391/3000], Loss: 0.009946871548891068\n",
      "Epoch [1392/3000], Loss: 0.009946523234248161\n",
      "Epoch [1393/3000], Loss: 0.009946180507540703\n",
      "Epoch [1394/3000], Loss: 0.009945825673639774\n",
      "Epoch [1395/3000], Loss: 0.00994547177106142\n",
      "Epoch [1396/3000], Loss: 0.009945123456418514\n",
      "Epoch [1397/3000], Loss: 0.009944766759872437\n",
      "Epoch [1398/3000], Loss: 0.009944407269358635\n",
      "Epoch [1399/3000], Loss: 0.009944051504135132\n",
      "Epoch [1400/3000], Loss: 0.00994369201362133\n",
      "Epoch [1401/3000], Loss: 0.009943326003849506\n",
      "Epoch [1402/3000], Loss: 0.009942960925400257\n",
      "Epoch [1403/3000], Loss: 0.009942594915628433\n",
      "Epoch [1404/3000], Loss: 0.009942222386598587\n",
      "Epoch [1405/3000], Loss: 0.009941847994923592\n",
      "Epoch [1406/3000], Loss: 0.009941471740603447\n",
      "Epoch [1407/3000], Loss: 0.009941091760993004\n",
      "Epoch [1408/3000], Loss: 0.009940706193447113\n",
      "Epoch [1409/3000], Loss: 0.009940319694578648\n",
      "Epoch [1410/3000], Loss: 0.009939927607774734\n",
      "Epoch [1411/3000], Loss: 0.009939529933035374\n",
      "Epoch [1412/3000], Loss: 0.009939130395650864\n",
      "Epoch [1413/3000], Loss: 0.00993872620165348\n",
      "Epoch [1414/3000], Loss: 0.0099383145570755\n",
      "Epoch [1415/3000], Loss: 0.009937899187207222\n",
      "Epoch [1416/3000], Loss: 0.00993747916072607\n",
      "Epoch [1417/3000], Loss: 0.009937050752341747\n",
      "Epoch [1418/3000], Loss: 0.009936616756021976\n",
      "Epoch [1419/3000], Loss: 0.009936178103089333\n",
      "Epoch [1420/3000], Loss: 0.009935731068253517\n",
      "Epoch [1421/3000], Loss: 0.00993527751415968\n",
      "Epoch [1422/3000], Loss: 0.00993481744080782\n",
      "Epoch [1423/3000], Loss: 0.009934348054230213\n",
      "Epoch [1424/3000], Loss: 0.00993386935442686\n",
      "Epoch [1425/3000], Loss: 0.009933384135365486\n",
      "Epoch [1426/3000], Loss: 0.00993288867175579\n",
      "Epoch [1427/3000], Loss: 0.009932385757565498\n",
      "Epoch [1428/3000], Loss: 0.009931869804859161\n",
      "Epoch [1429/3000], Loss: 0.009931345470249653\n",
      "Epoch [1430/3000], Loss: 0.009930809959769249\n",
      "Epoch [1431/3000], Loss: 0.00993026327341795\n",
      "Epoch [1432/3000], Loss: 0.009929705411195755\n",
      "Epoch [1433/3000], Loss: 0.00992913544178009\n",
      "Epoch [1434/3000], Loss: 0.009928552433848381\n",
      "Epoch [1435/3000], Loss: 0.009927956387400627\n",
      "Epoch [1436/3000], Loss: 0.009927346371114254\n",
      "Epoch [1437/3000], Loss: 0.009926723316311836\n",
      "Epoch [1438/3000], Loss: 0.0099260862916708\n",
      "Epoch [1439/3000], Loss: 0.009925433434545994\n",
      "Epoch [1440/3000], Loss: 0.009924767538905144\n",
      "Epoch [1441/3000], Loss: 0.00992408487945795\n",
      "Epoch [1442/3000], Loss: 0.009923387318849564\n",
      "Epoch [1443/3000], Loss: 0.009922673925757408\n",
      "Epoch [1444/3000], Loss: 0.009921944700181484\n",
      "Epoch [1445/3000], Loss: 0.009921199642121792\n",
      "Epoch [1446/3000], Loss: 0.009920438751578331\n",
      "Epoch [1447/3000], Loss: 0.009919662028551102\n",
      "Epoch [1448/3000], Loss: 0.009918870404362679\n",
      "Epoch [1449/3000], Loss: 0.009918062947690487\n",
      "Epoch [1450/3000], Loss: 0.009917243383824825\n",
      "Epoch [1451/3000], Loss: 0.00991640891879797\n",
      "Epoch [1452/3000], Loss: 0.009915564209222794\n",
      "Epoch [1453/3000], Loss: 0.009914707392454147\n",
      "Epoch [1454/3000], Loss: 0.009913839399814606\n",
      "Epoch [1455/3000], Loss: 0.009912965819239616\n",
      "Epoch [1456/3000], Loss: 0.009912085719406605\n",
      "Epoch [1457/3000], Loss: 0.00991120282560587\n",
      "Epoch [1458/3000], Loss: 0.009910319931805134\n",
      "Epoch [1459/3000], Loss: 0.009909437038004398\n",
      "Epoch [1460/3000], Loss: 0.009908560663461685\n",
      "Epoch [1461/3000], Loss: 0.009907691739499569\n",
      "Epoch [1462/3000], Loss: 0.009906833060085773\n",
      "Epoch [1463/3000], Loss: 0.009905990213155746\n",
      "Epoch [1464/3000], Loss: 0.009905163198709488\n",
      "Epoch [1465/3000], Loss: 0.009904356673359871\n",
      "Epoch [1466/3000], Loss: 0.009903572499752045\n",
      "Epoch [1467/3000], Loss: 0.009902813471853733\n",
      "Epoch [1468/3000], Loss: 0.009902081452310085\n",
      "Epoch [1469/3000], Loss: 0.009901375509798527\n",
      "Epoch [1470/3000], Loss: 0.009900699369609356\n",
      "Epoch [1471/3000], Loss: 0.0099000483751297\n",
      "Epoch [1472/3000], Loss: 0.009899426251649857\n",
      "Epoch [1473/3000], Loss: 0.009898828342556953\n",
      "Epoch [1474/3000], Loss: 0.009898263029754162\n",
      "Epoch [1475/3000], Loss: 0.00989776011556387\n",
      "Epoch [1476/3000], Loss: 0.009897482581436634\n",
      "Epoch [1477/3000], Loss: 0.009898172691464424\n",
      "Epoch [1478/3000], Loss: 0.009901853278279305\n",
      "Epoch [1479/3000], Loss: 0.009906645864248276\n",
      "Epoch [1480/3000], Loss: 0.009900391101837158\n",
      "Epoch [1481/3000], Loss: 0.009895089082419872\n",
      "Epoch [1482/3000], Loss: 0.009900636970996857\n",
      "Epoch [1483/3000], Loss: 0.009895890951156616\n",
      "Epoch [1484/3000], Loss: 0.009894856251776218\n",
      "Epoch [1485/3000], Loss: 0.009896878153085709\n",
      "Epoch [1486/3000], Loss: 0.009892389178276062\n",
      "Epoch [1487/3000], Loss: 0.009895003400743008\n",
      "Epoch [1488/3000], Loss: 0.00989206600934267\n",
      "Epoch [1489/3000], Loss: 0.009892504662275314\n",
      "Epoch [1490/3000], Loss: 0.009891842491924763\n",
      "Epoch [1491/3000], Loss: 0.009890546090900898\n",
      "Epoch [1492/3000], Loss: 0.00989118404686451\n",
      "Epoch [1493/3000], Loss: 0.009889223612844944\n",
      "Epoch [1494/3000], Loss: 0.009890181943774223\n",
      "Epoch [1495/3000], Loss: 0.009888317435979843\n",
      "Epoch [1496/3000], Loss: 0.009889033623039722\n",
      "Epoch [1497/3000], Loss: 0.009887589141726494\n",
      "Epoch [1498/3000], Loss: 0.00988791324198246\n",
      "Epoch [1499/3000], Loss: 0.009886892512440681\n",
      "Epoch [1500/3000], Loss: 0.00988688226789236\n",
      "Epoch [1501/3000], Loss: 0.009886190295219421\n",
      "Epoch [1502/3000], Loss: 0.00988592766225338\n",
      "Epoch [1503/3000], Loss: 0.009885489009320736\n",
      "Epoch [1504/3000], Loss: 0.009885036386549473\n",
      "Epoch [1505/3000], Loss: 0.009884776547551155\n",
      "Epoch [1506/3000], Loss: 0.009884207509458065\n",
      "Epoch [1507/3000], Loss: 0.009884043596684933\n",
      "Epoch [1508/3000], Loss: 0.009883437305688858\n",
      "Epoch [1509/3000], Loss: 0.00988329853862524\n",
      "Epoch [1510/3000], Loss: 0.009882702492177486\n",
      "Epoch [1511/3000], Loss: 0.009882555343210697\n",
      "Epoch [1512/3000], Loss: 0.009881988167762756\n",
      "Epoch [1513/3000], Loss: 0.009881817735731602\n",
      "Epoch [1514/3000], Loss: 0.00988129060715437\n",
      "Epoch [1515/3000], Loss: 0.009881085716187954\n",
      "Epoch [1516/3000], Loss: 0.009880603291094303\n",
      "Epoch [1517/3000], Loss: 0.009880363009870052\n",
      "Epoch [1518/3000], Loss: 0.00987991876900196\n",
      "Epoch [1519/3000], Loss: 0.009879650548100471\n",
      "Epoch [1520/3000], Loss: 0.009879237972199917\n",
      "Epoch [1521/3000], Loss: 0.009878941811621189\n",
      "Epoch [1522/3000], Loss: 0.009878559969365597\n",
      "Epoch [1523/3000], Loss: 0.00987823773175478\n",
      "Epoch [1524/3000], Loss: 0.009877881966531277\n",
      "Epoch [1525/3000], Loss: 0.009877537377178669\n",
      "Epoch [1526/3000], Loss: 0.009877200238406658\n",
      "Epoch [1527/3000], Loss: 0.009876840747892857\n",
      "Epoch [1528/3000], Loss: 0.009876513853669167\n",
      "Epoch [1529/3000], Loss: 0.009876145981252193\n",
      "Epoch [1530/3000], Loss: 0.009875822812318802\n",
      "Epoch [1531/3000], Loss: 0.00987545121461153\n",
      "Epoch [1532/3000], Loss: 0.009875125251710415\n",
      "Epoch [1533/3000], Loss: 0.009874754585325718\n",
      "Epoch [1534/3000], Loss: 0.009874423034489155\n",
      "Epoch [1535/3000], Loss: 0.009874051436781883\n",
      "Epoch [1536/3000], Loss: 0.009873714298009872\n",
      "Epoch [1537/3000], Loss: 0.00987334456294775\n",
      "Epoch [1538/3000], Loss: 0.009872999042272568\n",
      "Epoch [1539/3000], Loss: 0.00987262837588787\n",
      "Epoch [1540/3000], Loss: 0.009872275404632092\n",
      "Epoch [1541/3000], Loss: 0.009871904738247395\n",
      "Epoch [1542/3000], Loss: 0.009871541522443295\n",
      "Epoch [1543/3000], Loss: 0.009871169924736023\n",
      "Epoch [1544/3000], Loss: 0.009870798327028751\n",
      "Epoch [1545/3000], Loss: 0.009870423935353756\n",
      "Epoch [1546/3000], Loss: 0.009870043024420738\n",
      "Epoch [1547/3000], Loss: 0.00986966397613287\n",
      "Epoch [1548/3000], Loss: 0.009869275614619255\n",
      "Epoch [1549/3000], Loss: 0.009868890047073364\n",
      "Epoch [1550/3000], Loss: 0.009868492372334003\n",
      "Epoch [1551/3000], Loss: 0.009868097491562366\n",
      "Epoch [1552/3000], Loss: 0.009867692366242409\n",
      "Epoch [1553/3000], Loss: 0.009867287240922451\n",
      "Epoch [1554/3000], Loss: 0.009866873733699322\n",
      "Epoch [1555/3000], Loss: 0.009866459295153618\n",
      "Epoch [1556/3000], Loss: 0.009866033680737019\n",
      "Epoch [1557/3000], Loss: 0.009865607134997845\n",
      "Epoch [1558/3000], Loss: 0.009865171276032925\n",
      "Epoch [1559/3000], Loss: 0.009864731691777706\n",
      "Epoch [1560/3000], Loss: 0.009864282794296741\n",
      "Epoch [1561/3000], Loss: 0.009863828308880329\n",
      "Epoch [1562/3000], Loss: 0.009863365441560745\n",
      "Epoch [1563/3000], Loss: 0.00986289419233799\n",
      "Epoch [1564/3000], Loss: 0.009862417355179787\n",
      "Epoch [1565/3000], Loss: 0.009861928410828114\n",
      "Epoch [1566/3000], Loss: 0.009861433878540993\n",
      "Epoch [1567/3000], Loss: 0.009860927239060402\n",
      "Epoch [1568/3000], Loss: 0.00986041221767664\n",
      "Epoch [1569/3000], Loss: 0.009859886951744556\n",
      "Epoch [1570/3000], Loss: 0.009859351441264153\n",
      "Epoch [1571/3000], Loss: 0.009858803823590279\n",
      "Epoch [1572/3000], Loss: 0.009858244098722935\n",
      "Epoch [1573/3000], Loss: 0.00985767226666212\n",
      "Epoch [1574/3000], Loss: 0.009857089258730412\n",
      "Epoch [1575/3000], Loss: 0.009856492280960083\n",
      "Epoch [1576/3000], Loss: 0.009855883195996284\n",
      "Epoch [1577/3000], Loss: 0.009855260141193867\n",
      "Epoch [1578/3000], Loss: 0.00985462311655283\n",
      "Epoch [1579/3000], Loss: 0.009853972122073174\n",
      "Epoch [1580/3000], Loss: 0.009853305295109749\n",
      "Epoch [1581/3000], Loss: 0.009852624498307705\n",
      "Epoch [1582/3000], Loss: 0.009851929731667042\n",
      "Epoch [1583/3000], Loss: 0.009851218201220036\n",
      "Epoch [1584/3000], Loss: 0.00985049083828926\n",
      "Epoch [1585/3000], Loss: 0.009849748574197292\n",
      "Epoch [1586/3000], Loss: 0.00984899140894413\n",
      "Epoch [1587/3000], Loss: 0.009848217479884624\n",
      "Epoch [1588/3000], Loss: 0.00984742771834135\n",
      "Epoch [1589/3000], Loss: 0.009846623055636883\n",
      "Epoch [1590/3000], Loss: 0.009845803491771221\n",
      "Epoch [1591/3000], Loss: 0.009844968095421791\n",
      "Epoch [1592/3000], Loss: 0.009844120591878891\n",
      "Epoch [1593/3000], Loss: 0.009843259118497372\n",
      "Epoch [1594/3000], Loss: 0.009842387400567532\n",
      "Epoch [1595/3000], Loss: 0.009841504506766796\n",
      "Epoch [1596/3000], Loss: 0.009840614162385464\n",
      "Epoch [1597/3000], Loss: 0.00983971543610096\n",
      "Epoch [1598/3000], Loss: 0.009838812053203583\n",
      "Epoch [1599/3000], Loss: 0.009837905876338482\n",
      "Epoch [1600/3000], Loss: 0.009836997836828232\n",
      "Epoch [1601/3000], Loss: 0.009836094453930855\n",
      "Epoch [1602/3000], Loss: 0.00983519572764635\n",
      "Epoch [1603/3000], Loss: 0.00983430352061987\n",
      "Epoch [1604/3000], Loss: 0.009833422489464283\n",
      "Epoch [1605/3000], Loss: 0.009832554496824741\n",
      "Epoch [1606/3000], Loss: 0.009831705130636692\n",
      "Epoch [1607/3000], Loss: 0.00983087345957756\n",
      "Epoch [1608/3000], Loss: 0.009830063208937645\n",
      "Epoch [1609/3000], Loss: 0.00982927717268467\n",
      "Epoch [1610/3000], Loss: 0.009828516282141209\n",
      "Epoch [1611/3000], Loss: 0.009827780537307262\n",
      "Epoch [1612/3000], Loss: 0.00982707180082798\n",
      "Epoch [1613/3000], Loss: 0.009826389141380787\n",
      "Epoch [1614/3000], Loss: 0.009825730696320534\n",
      "Epoch [1615/3000], Loss: 0.00982509832829237\n",
      "Epoch [1616/3000], Loss: 0.009824483655393124\n",
      "Epoch [1617/3000], Loss: 0.009823890402913094\n",
      "Epoch [1618/3000], Loss: 0.009823312051594257\n",
      "Epoch [1619/3000], Loss: 0.009822745807468891\n",
      "Epoch [1620/3000], Loss: 0.009822189807891846\n",
      "Epoch [1621/3000], Loss: 0.009821639396250248\n",
      "Epoch [1622/3000], Loss: 0.009821094572544098\n",
      "Epoch [1623/3000], Loss: 0.009820551611483097\n",
      "Epoch [1624/3000], Loss: 0.009820010513067245\n",
      "Epoch [1625/3000], Loss: 0.009819472208619118\n",
      "Epoch [1626/3000], Loss: 0.00981894787400961\n",
      "Epoch [1627/3000], Loss: 0.009818464517593384\n",
      "Epoch [1628/3000], Loss: 0.009818111546337605\n",
      "Epoch [1629/3000], Loss: 0.00981808640062809\n",
      "Epoch [1630/3000], Loss: 0.00981863308697939\n",
      "Epoch [1631/3000], Loss: 0.00981924682855606\n",
      "Epoch [1632/3000], Loss: 0.009818184189498425\n",
      "Epoch [1633/3000], Loss: 0.009815595112740993\n",
      "Epoch [1634/3000], Loss: 0.009814861230552197\n",
      "Epoch [1635/3000], Loss: 0.009815680794417858\n",
      "Epoch [1636/3000], Loss: 0.009814689867198467\n",
      "Epoch [1637/3000], Loss: 0.00981304794549942\n",
      "Epoch [1638/3000], Loss: 0.009813264943659306\n",
      "Epoch [1639/3000], Loss: 0.009813006967306137\n",
      "Epoch [1640/3000], Loss: 0.009811589494347572\n",
      "Epoch [1641/3000], Loss: 0.00981142371892929\n",
      "Epoch [1642/3000], Loss: 0.009811235591769218\n",
      "Epoch [1643/3000], Loss: 0.009810086339712143\n",
      "Epoch [1644/3000], Loss: 0.009809818118810654\n",
      "Epoch [1645/3000], Loss: 0.009809521026909351\n",
      "Epoch [1646/3000], Loss: 0.009808557108044624\n",
      "Epoch [1647/3000], Loss: 0.009808274917304516\n",
      "Epoch [1648/3000], Loss: 0.009807843714952469\n",
      "Epoch [1649/3000], Loss: 0.009807021357119083\n",
      "Epoch [1650/3000], Loss: 0.009806721471250057\n",
      "Epoch [1651/3000], Loss: 0.009806176647543907\n",
      "Epoch [1652/3000], Loss: 0.009805469773709774\n",
      "Epoch [1653/3000], Loss: 0.009805121459066868\n",
      "Epoch [1654/3000], Loss: 0.009804504923522472\n",
      "Epoch [1655/3000], Loss: 0.009803880006074905\n",
      "Epoch [1656/3000], Loss: 0.009803462773561478\n",
      "Epoch [1657/3000], Loss: 0.009802808985114098\n",
      "Epoch [1658/3000], Loss: 0.009802231565117836\n",
      "Epoch [1659/3000], Loss: 0.009801739826798439\n",
      "Epoch [1660/3000], Loss: 0.009801074862480164\n",
      "Epoch [1661/3000], Loss: 0.009800509549677372\n",
      "Epoch [1662/3000], Loss: 0.009799950756132603\n",
      "Epoch [1663/3000], Loss: 0.009799282066524029\n",
      "Epoch [1664/3000], Loss: 0.009798699989914894\n",
      "Epoch [1665/3000], Loss: 0.009798086248338223\n",
      "Epoch [1666/3000], Loss: 0.009797411039471626\n",
      "Epoch [1667/3000], Loss: 0.00979679822921753\n",
      "Epoch [1668/3000], Loss: 0.009796135127544403\n",
      "Epoch [1669/3000], Loss: 0.009795445017516613\n",
      "Epoch [1670/3000], Loss: 0.009794789366424084\n",
      "Epoch [1671/3000], Loss: 0.009794086217880249\n",
      "Epoch [1672/3000], Loss: 0.009793370962142944\n",
      "Epoch [1673/3000], Loss: 0.009792668744921684\n",
      "Epoch [1674/3000], Loss: 0.009791925549507141\n",
      "Epoch [1675/3000], Loss: 0.00979117676615715\n",
      "Epoch [1676/3000], Loss: 0.00979042612016201\n",
      "Epoch [1677/3000], Loss: 0.009789642877876759\n",
      "Epoch [1678/3000], Loss: 0.009788855910301208\n",
      "Epoch [1679/3000], Loss: 0.009788055904209614\n",
      "Epoch [1680/3000], Loss: 0.009787231683731079\n",
      "Epoch [1681/3000], Loss: 0.009786401875317097\n",
      "Epoch [1682/3000], Loss: 0.009785556234419346\n",
      "Epoch [1683/3000], Loss: 0.009784689173102379\n",
      "Epoch [1684/3000], Loss: 0.009783816523849964\n",
      "Epoch [1685/3000], Loss: 0.009782928042113781\n",
      "Epoch [1686/3000], Loss: 0.00978202186524868\n",
      "Epoch [1687/3000], Loss: 0.009781108237802982\n",
      "Epoch [1688/3000], Loss: 0.009780180640518665\n",
      "Epoch [1689/3000], Loss: 0.009779240936040878\n",
      "Epoch [1690/3000], Loss: 0.009778293780982494\n",
      "Epoch [1691/3000], Loss: 0.009777335450053215\n",
      "Epoch [1692/3000], Loss: 0.009776370599865913\n",
      "Epoch [1693/3000], Loss: 0.009775401093065739\n",
      "Epoch [1694/3000], Loss: 0.009774422273039818\n",
      "Epoch [1695/3000], Loss: 0.009773443453013897\n",
      "Epoch [1696/3000], Loss: 0.009772462770342827\n",
      "Epoch [1697/3000], Loss: 0.009771482087671757\n",
      "Epoch [1698/3000], Loss: 0.009770503267645836\n",
      "Epoch [1699/3000], Loss: 0.009769528172910213\n",
      "Epoch [1700/3000], Loss: 0.009768557734787464\n",
      "Epoch [1701/3000], Loss: 0.009767595678567886\n",
      "Epoch [1702/3000], Loss: 0.009766641072928905\n",
      "Epoch [1703/3000], Loss: 0.009765696711838245\n",
      "Epoch [1704/3000], Loss: 0.00976476352661848\n",
      "Epoch [1705/3000], Loss: 0.009763841517269611\n",
      "Epoch [1706/3000], Loss: 0.009762928821146488\n",
      "Epoch [1707/3000], Loss: 0.009762030094861984\n",
      "Epoch [1708/3000], Loss: 0.00976114347577095\n",
      "Epoch [1709/3000], Loss: 0.009760264307260513\n",
      "Epoch [1710/3000], Loss: 0.009759398177266121\n",
      "Epoch [1711/3000], Loss: 0.009758535772562027\n",
      "Epoch [1712/3000], Loss: 0.009757681749761105\n",
      "Epoch [1713/3000], Loss: 0.009756830520927906\n",
      "Epoch [1714/3000], Loss: 0.009755981154739857\n",
      "Epoch [1715/3000], Loss: 0.009755131788551807\n",
      "Epoch [1716/3000], Loss: 0.009754280559718609\n",
      "Epoch [1717/3000], Loss: 0.009753424674272537\n",
      "Epoch [1718/3000], Loss: 0.009752561338245869\n",
      "Epoch [1719/3000], Loss: 0.009751691482961178\n",
      "Epoch [1720/3000], Loss: 0.009750811383128166\n",
      "Epoch [1721/3000], Loss: 0.009749923832714558\n",
      "Epoch [1722/3000], Loss: 0.009749026037752628\n",
      "Epoch [1723/3000], Loss: 0.009748119860887527\n",
      "Epoch [1724/3000], Loss: 0.009747203439474106\n",
      "Epoch [1725/3000], Loss: 0.009746279567480087\n",
      "Epoch [1726/3000], Loss: 0.009745349176228046\n",
      "Epoch [1727/3000], Loss: 0.009744414128363132\n",
      "Epoch [1728/3000], Loss: 0.009743476286530495\n",
      "Epoch [1729/3000], Loss: 0.009742536582052708\n",
      "Epoch [1730/3000], Loss: 0.009741597808897495\n",
      "Epoch [1731/3000], Loss: 0.009740660898387432\n",
      "Epoch [1732/3000], Loss: 0.009739728644490242\n",
      "Epoch [1733/3000], Loss: 0.009738804772496223\n",
      "Epoch [1734/3000], Loss: 0.009737889282405376\n",
      "Epoch [1735/3000], Loss: 0.009736986830830574\n",
      "Epoch [1736/3000], Loss: 0.009736096486449242\n",
      "Epoch [1737/3000], Loss: 0.009735221974551678\n",
      "Epoch [1738/3000], Loss: 0.009734365157783031\n",
      "Epoch [1739/3000], Loss: 0.009733527898788452\n",
      "Epoch [1740/3000], Loss: 0.009732712060213089\n",
      "Epoch [1741/3000], Loss: 0.009731919504702091\n",
      "Epoch [1742/3000], Loss: 0.009731152094900608\n",
      "Epoch [1743/3000], Loss: 0.009730408899486065\n",
      "Epoch [1744/3000], Loss: 0.009729694575071335\n",
      "Epoch [1745/3000], Loss: 0.009729008190333843\n",
      "Epoch [1746/3000], Loss: 0.009728346951305866\n",
      "Epoch [1747/3000], Loss: 0.009727712720632553\n",
      "Epoch [1748/3000], Loss: 0.009727103635668755\n",
      "Epoch [1749/3000], Loss: 0.009726517833769321\n",
      "Epoch [1750/3000], Loss: 0.009725954383611679\n",
      "Epoch [1751/3000], Loss: 0.009725408628582954\n",
      "Epoch [1752/3000], Loss: 0.009724880568683147\n",
      "Epoch [1753/3000], Loss: 0.009724369272589684\n",
      "Epoch [1754/3000], Loss: 0.009723871946334839\n",
      "Epoch [1755/3000], Loss: 0.009723392315208912\n",
      "Epoch [1756/3000], Loss: 0.009722942486405373\n",
      "Epoch [1757/3000], Loss: 0.00972256064414978\n",
      "Epoch [1758/3000], Loss: 0.009722378104925156\n",
      "Epoch [1759/3000], Loss: 0.009722717106342316\n",
      "Epoch [1760/3000], Loss: 0.009724053554236889\n",
      "Epoch [1761/3000], Loss: 0.009725671261548996\n",
      "Epoch [1762/3000], Loss: 0.009724345058202744\n",
      "Epoch [1763/3000], Loss: 0.009720344096422195\n",
      "Epoch [1764/3000], Loss: 0.00971988309174776\n",
      "Epoch [1765/3000], Loss: 0.009721705690026283\n",
      "Epoch [1766/3000], Loss: 0.00971990916877985\n",
      "Epoch [1767/3000], Loss: 0.009717983193695545\n",
      "Epoch [1768/3000], Loss: 0.009719216264784336\n",
      "Epoch [1769/3000], Loss: 0.009718416258692741\n",
      "Epoch [1770/3000], Loss: 0.00971673708409071\n",
      "Epoch [1771/3000], Loss: 0.009717504493892193\n",
      "Epoch [1772/3000], Loss: 0.009716779924929142\n",
      "Epoch [1773/3000], Loss: 0.009715618565678596\n",
      "Epoch [1774/3000], Loss: 0.009716127067804337\n",
      "Epoch [1775/3000], Loss: 0.009715257212519646\n",
      "Epoch [1776/3000], Loss: 0.009714624844491482\n",
      "Epoch [1777/3000], Loss: 0.009714839048683643\n",
      "Epoch [1778/3000], Loss: 0.009713923558592796\n",
      "Epoch [1779/3000], Loss: 0.009713703766465187\n",
      "Epoch [1780/3000], Loss: 0.009713559411466122\n",
      "Epoch [1781/3000], Loss: 0.009712803177535534\n",
      "Epoch [1782/3000], Loss: 0.009712763130664825\n",
      "Epoch [1783/3000], Loss: 0.00971233006566763\n",
      "Epoch [1784/3000], Loss: 0.009711847640573978\n",
      "Epoch [1785/3000], Loss: 0.009711752645671368\n",
      "Epoch [1786/3000], Loss: 0.009711217135190964\n",
      "Epoch [1787/3000], Loss: 0.009710957296192646\n",
      "Epoch [1788/3000], Loss: 0.009710702113807201\n",
      "Epoch [1789/3000], Loss: 0.009710232727229595\n",
      "Epoch [1790/3000], Loss: 0.009710052981972694\n",
      "Epoch [1791/3000], Loss: 0.009709675796329975\n",
      "Epoch [1792/3000], Loss: 0.009709334932267666\n",
      "Epoch [1793/3000], Loss: 0.009709117002785206\n",
      "Epoch [1794/3000], Loss: 0.009708716534078121\n",
      "Epoch [1795/3000], Loss: 0.0097084641456604\n",
      "Epoch [1796/3000], Loss: 0.00970817543566227\n",
      "Epoch [1797/3000], Loss: 0.009707820601761341\n",
      "Epoch [1798/3000], Loss: 0.009707589633762836\n",
      "Epoch [1799/3000], Loss: 0.009707259014248848\n",
      "Epoch [1800/3000], Loss: 0.00970696285367012\n",
      "Epoch [1801/3000], Loss: 0.009706711396574974\n",
      "Epoch [1802/3000], Loss: 0.00970638170838356\n",
      "Epoch [1803/3000], Loss: 0.009706119075417519\n",
      "Epoch [1804/3000], Loss: 0.009705840609967709\n",
      "Epoch [1805/3000], Loss: 0.009705533273518085\n",
      "Epoch [1806/3000], Loss: 0.00970527995377779\n",
      "Epoch [1807/3000], Loss: 0.00970498751848936\n",
      "Epoch [1808/3000], Loss: 0.009704705327749252\n",
      "Epoch [1809/3000], Loss: 0.00970444642007351\n",
      "Epoch [1810/3000], Loss: 0.00970415398478508\n",
      "Epoch [1811/3000], Loss: 0.00970388762652874\n",
      "Epoch [1812/3000], Loss: 0.009703618474304676\n",
      "Epoch [1813/3000], Loss: 0.009703335352241993\n",
      "Epoch [1814/3000], Loss: 0.009703075513243675\n",
      "Epoch [1815/3000], Loss: 0.009702801704406738\n",
      "Epoch [1816/3000], Loss: 0.009702530689537525\n",
      "Epoch [1817/3000], Loss: 0.009702269919216633\n",
      "Epoch [1818/3000], Loss: 0.009701996110379696\n",
      "Epoch [1819/3000], Loss: 0.00970173254609108\n",
      "Epoch [1820/3000], Loss: 0.009701470844447613\n",
      "Epoch [1821/3000], Loss: 0.009701200760900974\n",
      "Epoch [1822/3000], Loss: 0.009700942784547806\n",
      "Epoch [1823/3000], Loss: 0.00970067735761404\n",
      "Epoch [1824/3000], Loss: 0.009700413793325424\n",
      "Epoch [1825/3000], Loss: 0.00970015674829483\n",
      "Epoch [1826/3000], Loss: 0.00969989225268364\n",
      "Epoch [1827/3000], Loss: 0.009699632413685322\n",
      "Epoch [1828/3000], Loss: 0.009699375368654728\n",
      "Epoch [1829/3000], Loss: 0.009699113667011261\n",
      "Epoch [1830/3000], Loss: 0.009698855690658092\n",
      "Epoch [1831/3000], Loss: 0.009698597714304924\n",
      "Epoch [1832/3000], Loss: 0.009698338806629181\n",
      "Epoch [1833/3000], Loss: 0.009698081761598587\n",
      "Epoch [1834/3000], Loss: 0.009697822853922844\n",
      "Epoch [1835/3000], Loss: 0.009697564877569675\n",
      "Epoch [1836/3000], Loss: 0.009697308763861656\n",
      "Epoch [1837/3000], Loss: 0.009697049856185913\n",
      "Epoch [1838/3000], Loss: 0.009696794673800468\n",
      "Epoch [1839/3000], Loss: 0.0096965366974473\n",
      "Epoch [1840/3000], Loss: 0.009696278721094131\n",
      "Epoch [1841/3000], Loss: 0.009696020744740963\n",
      "Epoch [1842/3000], Loss: 0.009695764631032944\n",
      "Epoch [1843/3000], Loss: 0.0096955057233572\n",
      "Epoch [1844/3000], Loss: 0.009695248678326607\n",
      "Epoch [1845/3000], Loss: 0.009694990701973438\n",
      "Epoch [1846/3000], Loss: 0.00969473272562027\n",
      "Epoch [1847/3000], Loss: 0.009694474749267101\n",
      "Epoch [1848/3000], Loss: 0.009694215841591358\n",
      "Epoch [1849/3000], Loss: 0.00969395786523819\n",
      "Epoch [1850/3000], Loss: 0.009693698957562447\n",
      "Epoch [1851/3000], Loss: 0.009693438187241554\n",
      "Epoch [1852/3000], Loss: 0.009693178348243237\n",
      "Epoch [1853/3000], Loss: 0.009692918509244919\n",
      "Epoch [1854/3000], Loss: 0.009692656807601452\n",
      "Epoch [1855/3000], Loss: 0.009692395105957985\n",
      "Epoch [1856/3000], Loss: 0.009692132472991943\n",
      "Epoch [1857/3000], Loss: 0.009691869840025902\n",
      "Epoch [1858/3000], Loss: 0.009691606275737286\n",
      "Epoch [1859/3000], Loss: 0.009691339917480946\n",
      "Epoch [1860/3000], Loss: 0.00969107449054718\n",
      "Epoch [1861/3000], Loss: 0.00969080813229084\n",
      "Epoch [1862/3000], Loss: 0.009690539911389351\n",
      "Epoch [1863/3000], Loss: 0.009690271690487862\n",
      "Epoch [1864/3000], Loss: 0.009690001606941223\n",
      "Epoch [1865/3000], Loss: 0.009689729660749435\n",
      "Epoch [1866/3000], Loss: 0.009689457714557648\n",
      "Epoch [1867/3000], Loss: 0.009689182974398136\n",
      "Epoch [1868/3000], Loss: 0.009688906371593475\n",
      "Epoch [1869/3000], Loss: 0.009688629768788815\n",
      "Epoch [1870/3000], Loss: 0.00968835037201643\n",
      "Epoch [1871/3000], Loss: 0.009688069112598896\n",
      "Epoch [1872/3000], Loss: 0.009687786921858788\n",
      "Epoch [1873/3000], Loss: 0.009687501937150955\n",
      "Epoch [1874/3000], Loss: 0.009687215089797974\n",
      "Epoch [1875/3000], Loss: 0.009686926379799843\n",
      "Epoch [1876/3000], Loss: 0.009686635807156563\n",
      "Epoch [1877/3000], Loss: 0.009686341509222984\n",
      "Epoch [1878/3000], Loss: 0.009686045348644257\n",
      "Epoch [1879/3000], Loss: 0.00968574732542038\n",
      "Epoch [1880/3000], Loss: 0.009685445576906204\n",
      "Epoch [1881/3000], Loss: 0.009685141034424305\n",
      "Epoch [1882/3000], Loss: 0.009684833697974682\n",
      "Epoch [1883/3000], Loss: 0.009684523567557335\n",
      "Epoch [1884/3000], Loss: 0.00968420971184969\n",
      "Epoch [1885/3000], Loss: 0.009683891199529171\n",
      "Epoch [1886/3000], Loss: 0.009683570824563503\n",
      "Epoch [1887/3000], Loss: 0.009683245792984962\n",
      "Epoch [1888/3000], Loss: 0.009682917036116123\n",
      "Epoch [1889/3000], Loss: 0.009682584553956985\n",
      "Epoch [1890/3000], Loss: 0.00968224834650755\n",
      "Epoch [1891/3000], Loss: 0.009681908413767815\n",
      "Epoch [1892/3000], Loss: 0.009681561961770058\n",
      "Epoch [1893/3000], Loss: 0.009681210853159428\n",
      "Epoch [1894/3000], Loss: 0.009680855087935925\n",
      "Epoch [1895/3000], Loss: 0.009680494666099548\n",
      "Epoch [1896/3000], Loss: 0.009680128656327724\n",
      "Epoch [1897/3000], Loss: 0.009679757989943027\n",
      "Epoch [1898/3000], Loss: 0.009679380804300308\n",
      "Epoch [1899/3000], Loss: 0.009678997099399567\n",
      "Epoch [1900/3000], Loss: 0.009678608737885952\n",
      "Epoch [1901/3000], Loss: 0.00967821292579174\n",
      "Epoch [1902/3000], Loss: 0.009677810594439507\n",
      "Epoch [1903/3000], Loss: 0.009677402675151825\n",
      "Epoch [1904/3000], Loss: 0.009676986373960972\n",
      "Epoch [1905/3000], Loss: 0.009676562622189522\n",
      "Epoch [1906/3000], Loss: 0.009676131419837475\n",
      "Epoch [1907/3000], Loss: 0.009675693698227406\n",
      "Epoch [1908/3000], Loss: 0.00967524666339159\n",
      "Epoch [1909/3000], Loss: 0.009674792177975178\n",
      "Epoch [1910/3000], Loss: 0.009674329310655594\n",
      "Epoch [1911/3000], Loss: 0.009673857130110264\n",
      "Epoch [1912/3000], Loss: 0.009673378430306911\n",
      "Epoch [1913/3000], Loss: 0.009672887623310089\n",
      "Epoch [1914/3000], Loss: 0.00967238936573267\n",
      "Epoch [1915/3000], Loss: 0.009671881794929504\n",
      "Epoch [1916/3000], Loss: 0.009671364910900593\n",
      "Epoch [1917/3000], Loss: 0.00967083778232336\n",
      "Epoch [1918/3000], Loss: 0.009670300409197807\n",
      "Epoch [1919/3000], Loss: 0.009669754654169083\n",
      "Epoch [1920/3000], Loss: 0.009669198654592037\n",
      "Epoch [1921/3000], Loss: 0.009668633341789246\n",
      "Epoch [1922/3000], Loss: 0.009668056853115559\n",
      "Epoch [1923/3000], Loss: 0.00966747384518385\n",
      "Epoch [1924/3000], Loss: 0.009666879661381245\n",
      "Epoch [1925/3000], Loss: 0.00966627523303032\n",
      "Epoch [1926/3000], Loss: 0.009665663354098797\n",
      "Epoch [1927/3000], Loss: 0.009665043093264103\n",
      "Epoch [1928/3000], Loss: 0.009664415381848812\n",
      "Epoch [1929/3000], Loss: 0.009663781151175499\n",
      "Epoch [1930/3000], Loss: 0.009663139469921589\n",
      "Epoch [1931/3000], Loss: 0.009662492200732231\n",
      "Epoch [1932/3000], Loss: 0.00966184213757515\n",
      "Epoch [1933/3000], Loss: 0.009661187417805195\n",
      "Epoch [1934/3000], Loss: 0.009660529904067516\n",
      "Epoch [1935/3000], Loss: 0.009659871459007263\n",
      "Epoch [1936/3000], Loss: 0.00965921301394701\n",
      "Epoch [1937/3000], Loss: 0.009658556431531906\n",
      "Epoch [1938/3000], Loss: 0.009657902643084526\n",
      "Epoch [1939/3000], Loss: 0.009657254442572594\n",
      "Epoch [1940/3000], Loss: 0.009656612761318684\n",
      "Epoch [1941/3000], Loss: 0.009655979461967945\n",
      "Epoch [1942/3000], Loss: 0.009655355475842953\n",
      "Epoch [1943/3000], Loss: 0.00965474545955658\n",
      "Epoch [1944/3000], Loss: 0.009654145687818527\n",
      "Epoch [1945/3000], Loss: 0.009653561748564243\n",
      "Epoch [1946/3000], Loss: 0.009652993641793728\n",
      "Epoch [1947/3000], Loss: 0.009652441367506981\n",
      "Epoch [1948/3000], Loss: 0.009651907719671726\n",
      "Epoch [1949/3000], Loss: 0.00965139176696539\n",
      "Epoch [1950/3000], Loss: 0.00965089537203312\n",
      "Epoch [1951/3000], Loss: 0.009650417603552341\n",
      "Epoch [1952/3000], Loss: 0.009649956598877907\n",
      "Epoch [1953/3000], Loss: 0.009649515151977539\n",
      "Epoch [1954/3000], Loss: 0.00964908953756094\n",
      "Epoch [1955/3000], Loss: 0.009648678824305534\n",
      "Epoch [1956/3000], Loss: 0.009648283012211323\n",
      "Epoch [1957/3000], Loss: 0.009647899307310581\n",
      "Epoch [1958/3000], Loss: 0.00964752770960331\n",
      "Epoch [1959/3000], Loss: 0.009647163562476635\n",
      "Epoch [1960/3000], Loss: 0.009646809659898281\n",
      "Epoch [1961/3000], Loss: 0.009646459482610226\n",
      "Epoch [1962/3000], Loss: 0.009646114893257618\n",
      "Epoch [1963/3000], Loss: 0.009645774960517883\n",
      "Epoch [1964/3000], Loss: 0.009645436890423298\n",
      "Epoch [1965/3000], Loss: 0.009645100682973862\n",
      "Epoch [1966/3000], Loss: 0.009644766338169575\n",
      "Epoch [1967/3000], Loss: 0.009644431993365288\n",
      "Epoch [1968/3000], Loss: 0.009644104167819023\n",
      "Epoch [1969/3000], Loss: 0.00964378658682108\n",
      "Epoch [1970/3000], Loss: 0.009643499739468098\n",
      "Epoch [1971/3000], Loss: 0.009643305093050003\n",
      "Epoch [1972/3000], Loss: 0.00964335910975933\n",
      "Epoch [1973/3000], Loss: 0.009643970988690853\n",
      "Epoch [1974/3000], Loss: 0.009645338170230389\n",
      "Epoch [1975/3000], Loss: 0.009646285325288773\n",
      "Epoch [1976/3000], Loss: 0.009644445031881332\n",
      "Epoch [1977/3000], Loss: 0.009641442447900772\n",
      "Epoch [1978/3000], Loss: 0.009641622193157673\n",
      "Epoch [1979/3000], Loss: 0.00964299961924553\n",
      "Epoch [1980/3000], Loss: 0.009641522541642189\n",
      "Epoch [1981/3000], Loss: 0.009639989584684372\n",
      "Epoch [1982/3000], Loss: 0.009640918113291264\n",
      "Epoch [1983/3000], Loss: 0.009640641510486603\n",
      "Epoch [1984/3000], Loss: 0.009639156982302666\n",
      "Epoch [1985/3000], Loss: 0.009639481082558632\n",
      "Epoch [1986/3000], Loss: 0.009639473631978035\n",
      "Epoch [1987/3000], Loss: 0.009638340212404728\n",
      "Epoch [1988/3000], Loss: 0.009638411924242973\n",
      "Epoch [1989/3000], Loss: 0.009638333693146706\n",
      "Epoch [1990/3000], Loss: 0.009637483395636082\n",
      "Epoch [1991/3000], Loss: 0.009637484326958656\n",
      "Epoch [1992/3000], Loss: 0.00963727105408907\n",
      "Epoch [1993/3000], Loss: 0.009636620990931988\n",
      "Epoch [1994/3000], Loss: 0.009636597707867622\n",
      "Epoch [1995/3000], Loss: 0.009636270813643932\n",
      "Epoch [1996/3000], Loss: 0.009635766968131065\n",
      "Epoch [1997/3000], Loss: 0.009635712951421738\n",
      "Epoch [1998/3000], Loss: 0.0096353180706501\n",
      "Epoch [1999/3000], Loss: 0.009634921327233315\n",
      "Epoch [2000/3000], Loss: 0.009634819813072681\n",
      "Epoch [2001/3000], Loss: 0.009634401649236679\n",
      "Epoch [2002/3000], Loss: 0.009634073823690414\n",
      "Epoch [2003/3000], Loss: 0.009633916430175304\n",
      "Epoch [2004/3000], Loss: 0.009633504785597324\n",
      "Epoch [2005/3000], Loss: 0.009633216075599194\n",
      "Epoch [2006/3000], Loss: 0.00963300745934248\n",
      "Epoch [2007/3000], Loss: 0.009632615372538567\n",
      "Epoch [2008/3000], Loss: 0.009632344357669353\n",
      "Epoch [2009/3000], Loss: 0.009632093831896782\n",
      "Epoch [2010/3000], Loss: 0.00963172409683466\n",
      "Epoch [2011/3000], Loss: 0.009631454944610596\n",
      "Epoch [2012/3000], Loss: 0.009631171822547913\n",
      "Epoch [2013/3000], Loss: 0.009630820713937283\n",
      "Epoch [2014/3000], Loss: 0.009630545042455196\n",
      "Epoch [2015/3000], Loss: 0.009630238637328148\n",
      "Epoch [2016/3000], Loss: 0.00962989591062069\n",
      "Epoch [2017/3000], Loss: 0.009629609994590282\n",
      "Epoch [2018/3000], Loss: 0.009629287756979465\n",
      "Epoch [2019/3000], Loss: 0.00962894968688488\n",
      "Epoch [2020/3000], Loss: 0.009628649801015854\n",
      "Epoch [2021/3000], Loss: 0.009628314524888992\n",
      "Epoch [2022/3000], Loss: 0.009627973660826683\n",
      "Epoch [2023/3000], Loss: 0.009627658873796463\n",
      "Epoch [2024/3000], Loss: 0.00962731335312128\n",
      "Epoch [2025/3000], Loss: 0.009626965038478374\n",
      "Epoch [2026/3000], Loss: 0.009626632556319237\n",
      "Epoch [2027/3000], Loss: 0.009626276791095734\n",
      "Epoch [2028/3000], Loss: 0.009625918231904507\n",
      "Epoch [2029/3000], Loss: 0.009625568054616451\n",
      "Epoch [2030/3000], Loss: 0.009625198319554329\n",
      "Epoch [2031/3000], Loss: 0.009624826721847057\n",
      "Epoch [2032/3000], Loss: 0.009624459780752659\n",
      "Epoch [2033/3000], Loss: 0.009624075144529343\n",
      "Epoch [2034/3000], Loss: 0.009623688645660877\n",
      "Epoch [2035/3000], Loss: 0.009623302146792412\n",
      "Epoch [2036/3000], Loss: 0.009622900746762753\n",
      "Epoch [2037/3000], Loss: 0.00962249655276537\n",
      "Epoch [2038/3000], Loss: 0.009622088633477688\n",
      "Epoch [2039/3000], Loss: 0.009621669538319111\n",
      "Epoch [2040/3000], Loss: 0.00962124578654766\n",
      "Epoch [2041/3000], Loss: 0.009620815515518188\n",
      "Epoch [2042/3000], Loss: 0.009620374999940395\n",
      "Epoch [2043/3000], Loss: 0.009619928896427155\n",
      "Epoch [2044/3000], Loss: 0.009619475342333317\n",
      "Epoch [2045/3000], Loss: 0.009619012475013733\n",
      "Epoch [2046/3000], Loss: 0.009618541225790977\n",
      "Epoch [2047/3000], Loss: 0.009618064388632774\n",
      "Epoch [2048/3000], Loss: 0.009617576375603676\n",
      "Epoch [2049/3000], Loss: 0.009617079980671406\n",
      "Epoch [2050/3000], Loss: 0.009616577066481113\n",
      "Epoch [2051/3000], Loss: 0.009616062976419926\n",
      "Epoch [2052/3000], Loss: 0.009615539573132992\n",
      "Epoch [2053/3000], Loss: 0.00961501058191061\n",
      "Epoch [2054/3000], Loss: 0.009614468552172184\n",
      "Epoch [2055/3000], Loss: 0.00961391907185316\n",
      "Epoch [2056/3000], Loss: 0.009613361209630966\n",
      "Epoch [2057/3000], Loss: 0.009612794034183025\n",
      "Epoch [2058/3000], Loss: 0.009612219408154488\n",
      "Epoch [2059/3000], Loss: 0.009611636400222778\n",
      "Epoch [2060/3000], Loss: 0.009611043147742748\n",
      "Epoch [2061/3000], Loss: 0.009610443376004696\n",
      "Epoch [2062/3000], Loss: 0.009609836153686047\n",
      "Epoch [2063/3000], Loss: 0.009609222412109375\n",
      "Epoch [2064/3000], Loss: 0.009608602151274681\n",
      "Epoch [2065/3000], Loss: 0.00960797630250454\n",
      "Epoch [2066/3000], Loss: 0.009607345797121525\n",
      "Epoch [2067/3000], Loss: 0.009606711566448212\n",
      "Epoch [2068/3000], Loss: 0.00960607547312975\n",
      "Epoch [2069/3000], Loss: 0.009605437517166138\n",
      "Epoch [2070/3000], Loss: 0.009604797698557377\n",
      "Epoch [2071/3000], Loss: 0.009604159742593765\n",
      "Epoch [2072/3000], Loss: 0.009603523649275303\n",
      "Epoch [2073/3000], Loss: 0.009602890349924564\n",
      "Epoch [2074/3000], Loss: 0.009602264501154423\n",
      "Epoch [2075/3000], Loss: 0.00960164237767458\n",
      "Epoch [2076/3000], Loss: 0.009601029567420483\n",
      "Epoch [2077/3000], Loss: 0.009600426070392132\n",
      "Epoch [2078/3000], Loss: 0.009599834680557251\n",
      "Epoch [2079/3000], Loss: 0.009599254466593266\n",
      "Epoch [2080/3000], Loss: 0.009598689153790474\n",
      "Epoch [2081/3000], Loss: 0.009598138742148876\n",
      "Epoch [2082/3000], Loss: 0.009597604162991047\n",
      "Epoch [2083/3000], Loss: 0.009597087278962135\n",
      "Epoch [2084/3000], Loss: 0.009596587158739567\n",
      "Epoch [2085/3000], Loss: 0.00959610566496849\n",
      "Epoch [2086/3000], Loss: 0.009595641866326332\n",
      "Epoch [2087/3000], Loss: 0.009595195762813091\n",
      "Epoch [2088/3000], Loss: 0.009594767354428768\n",
      "Epoch [2089/3000], Loss: 0.009594354778528214\n",
      "Epoch [2090/3000], Loss: 0.009593958035111427\n",
      "Epoch [2091/3000], Loss: 0.00959357526153326\n",
      "Epoch [2092/3000], Loss: 0.009593203663825989\n",
      "Epoch [2093/3000], Loss: 0.009592846035957336\n",
      "Epoch [2094/3000], Loss: 0.009592496789991856\n",
      "Epoch [2095/3000], Loss: 0.009592155925929546\n",
      "Epoch [2096/3000], Loss: 0.009591820649802685\n",
      "Epoch [2097/3000], Loss: 0.009591491892933846\n",
      "Epoch [2098/3000], Loss: 0.00959116779267788\n",
      "Epoch [2099/3000], Loss: 0.009590845555067062\n",
      "Epoch [2100/3000], Loss: 0.009590527042746544\n",
      "Epoch [2101/3000], Loss: 0.0095902094617486\n",
      "Epoch [2102/3000], Loss: 0.009589891880750656\n",
      "Epoch [2103/3000], Loss: 0.009589577093720436\n",
      "Epoch [2104/3000], Loss: 0.009589264169335365\n",
      "Epoch [2105/3000], Loss: 0.009588953107595444\n",
      "Epoch [2106/3000], Loss: 0.009588649496436119\n",
      "Epoch [2107/3000], Loss: 0.00958835706114769\n",
      "Epoch [2108/3000], Loss: 0.009588101878762245\n",
      "Epoch [2109/3000], Loss: 0.009587930515408516\n",
      "Epoch [2110/3000], Loss: 0.009587958455085754\n",
      "Epoch [2111/3000], Loss: 0.00958837941288948\n",
      "Epoch [2112/3000], Loss: 0.009589259512722492\n",
      "Epoch [2113/3000], Loss: 0.009589899331331253\n",
      "Epoch [2114/3000], Loss: 0.009588833898305893\n",
      "Epoch [2115/3000], Loss: 0.00958652887493372\n",
      "Epoch [2116/3000], Loss: 0.009585768915712833\n",
      "Epoch [2117/3000], Loss: 0.009586750529706478\n",
      "Epoch [2118/3000], Loss: 0.009586691856384277\n",
      "Epoch [2119/3000], Loss: 0.00958514679223299\n",
      "Epoch [2120/3000], Loss: 0.009584705345332623\n",
      "Epoch [2121/3000], Loss: 0.009585307911038399\n",
      "Epoch [2122/3000], Loss: 0.00958473701030016\n",
      "Epoch [2123/3000], Loss: 0.009583743289113045\n",
      "Epoch [2124/3000], Loss: 0.00958389975130558\n",
      "Epoch [2125/3000], Loss: 0.009583895094692707\n",
      "Epoch [2126/3000], Loss: 0.009583059698343277\n",
      "Epoch [2127/3000], Loss: 0.009582805447280407\n",
      "Epoch [2128/3000], Loss: 0.009582904167473316\n",
      "Epoch [2129/3000], Loss: 0.009582346305251122\n",
      "Epoch [2130/3000], Loss: 0.009581910446286201\n",
      "Epoch [2131/3000], Loss: 0.009581927210092545\n",
      "Epoch [2132/3000], Loss: 0.009581567719578743\n",
      "Epoch [2133/3000], Loss: 0.009581098333001137\n",
      "Epoch [2134/3000], Loss: 0.009581000544130802\n",
      "Epoch [2135/3000], Loss: 0.009580747224390507\n",
      "Epoch [2136/3000], Loss: 0.009580308571457863\n",
      "Epoch [2137/3000], Loss: 0.009580118581652641\n",
      "Epoch [2138/3000], Loss: 0.009579909965395927\n",
      "Epoch [2139/3000], Loss: 0.00957951694726944\n",
      "Epoch [2140/3000], Loss: 0.009579265490174294\n",
      "Epoch [2141/3000], Loss: 0.009579060599207878\n",
      "Epoch [2142/3000], Loss: 0.00957871600985527\n",
      "Epoch [2143/3000], Loss: 0.00957842543721199\n",
      "Epoch [2144/3000], Loss: 0.00957820937037468\n",
      "Epoch [2145/3000], Loss: 0.009577898308634758\n",
      "Epoch [2146/3000], Loss: 0.009577590972185135\n",
      "Epoch [2147/3000], Loss: 0.009577352553606033\n",
      "Epoch [2148/3000], Loss: 0.009577063843607903\n",
      "Epoch [2149/3000], Loss: 0.009576750919222832\n",
      "Epoch [2150/3000], Loss: 0.009576489217579365\n",
      "Epoch [2151/3000], Loss: 0.00957621168345213\n",
      "Epoch [2152/3000], Loss: 0.009575897827744484\n",
      "Epoch [2153/3000], Loss: 0.009575617499649525\n",
      "Epoch [2154/3000], Loss: 0.009575337171554565\n",
      "Epoch [2155/3000], Loss: 0.009575027972459793\n",
      "Epoch [2156/3000], Loss: 0.009574729949235916\n",
      "Epoch [2157/3000], Loss: 0.00957444403320551\n",
      "Epoch [2158/3000], Loss: 0.009574133902788162\n",
      "Epoch [2159/3000], Loss: 0.00957382284104824\n",
      "Epoch [2160/3000], Loss: 0.009573525749146938\n",
      "Epoch [2161/3000], Loss: 0.009573213756084442\n",
      "Epoch [2162/3000], Loss: 0.009572891518473625\n",
      "Epoch [2163/3000], Loss: 0.009572579525411129\n",
      "Epoch [2164/3000], Loss: 0.00957226101309061\n",
      "Epoch [2165/3000], Loss: 0.009571930393576622\n",
      "Epoch [2166/3000], Loss: 0.009571604430675507\n",
      "Epoch [2167/3000], Loss: 0.009571274742484093\n",
      "Epoch [2168/3000], Loss: 0.009570935741066933\n",
      "Epoch [2169/3000], Loss: 0.0095705920830369\n",
      "Epoch [2170/3000], Loss: 0.009570252150297165\n",
      "Epoch [2171/3000], Loss: 0.00956990011036396\n",
      "Epoch [2172/3000], Loss: 0.009569543413817883\n",
      "Epoch [2173/3000], Loss: 0.009569184854626656\n",
      "Epoch [2174/3000], Loss: 0.009568820707499981\n",
      "Epoch [2175/3000], Loss: 0.009568448178470135\n",
      "Epoch [2176/3000], Loss: 0.00956807192414999\n",
      "Epoch [2177/3000], Loss: 0.009567692875862122\n",
      "Epoch [2178/3000], Loss: 0.009567302651703358\n",
      "Epoch [2179/3000], Loss: 0.00956690777093172\n",
      "Epoch [2180/3000], Loss: 0.009566509164869785\n",
      "Epoch [2181/3000], Loss: 0.009566101245582104\n",
      "Epoch [2182/3000], Loss: 0.009565685875713825\n",
      "Epoch [2183/3000], Loss: 0.009565264917910099\n",
      "Epoch [2184/3000], Loss: 0.009564836509525776\n",
      "Epoch [2185/3000], Loss: 0.009564399719238281\n",
      "Epoch [2186/3000], Loss: 0.00956395547837019\n",
      "Epoch [2187/3000], Loss: 0.00956350564956665\n",
      "Epoch [2188/3000], Loss: 0.009563044644892216\n",
      "Epoch [2189/3000], Loss: 0.009562576189637184\n",
      "Epoch [2190/3000], Loss: 0.009562099352478981\n",
      "Epoch [2191/3000], Loss: 0.009561614133417606\n",
      "Epoch [2192/3000], Loss: 0.009561118669807911\n",
      "Epoch [2193/3000], Loss: 0.009560615755617619\n",
      "Epoch [2194/3000], Loss: 0.00956010352820158\n",
      "Epoch [2195/3000], Loss: 0.00955958105623722\n",
      "Epoch [2196/3000], Loss: 0.00955904833972454\n",
      "Epoch [2197/3000], Loss: 0.009558507241308689\n",
      "Epoch [2198/3000], Loss: 0.009557955898344517\n",
      "Epoch [2199/3000], Loss: 0.009557396173477173\n",
      "Epoch [2200/3000], Loss: 0.009556826204061508\n",
      "Epoch [2201/3000], Loss: 0.009556245990097523\n",
      "Epoch [2202/3000], Loss: 0.00955565832555294\n",
      "Epoch [2203/3000], Loss: 0.009555058553814888\n",
      "Epoch [2204/3000], Loss: 0.009554452262818813\n",
      "Epoch [2205/3000], Loss: 0.009553835727274418\n",
      "Epoch [2206/3000], Loss: 0.009553211741149426\n",
      "Epoch [2207/3000], Loss: 0.009552580304443836\n",
      "Epoch [2208/3000], Loss: 0.009551940485835075\n",
      "Epoch [2209/3000], Loss: 0.009551294147968292\n",
      "Epoch [2210/3000], Loss: 0.009550643153488636\n",
      "Epoch [2211/3000], Loss: 0.009549985639750957\n",
      "Epoch [2212/3000], Loss: 0.00954932440072298\n",
      "Epoch [2213/3000], Loss: 0.00954866036772728\n",
      "Epoch [2214/3000], Loss: 0.00954799447208643\n",
      "Epoch [2215/3000], Loss: 0.009547327645123005\n",
      "Epoch [2216/3000], Loss: 0.00954666081815958\n",
      "Epoch [2217/3000], Loss: 0.00954599678516388\n",
      "Epoch [2218/3000], Loss: 0.009545334614813328\n",
      "Epoch [2219/3000], Loss: 0.009544678032398224\n",
      "Epoch [2220/3000], Loss: 0.009544027969241142\n",
      "Epoch [2221/3000], Loss: 0.009543384425342083\n",
      "Epoch [2222/3000], Loss: 0.009542751125991344\n",
      "Epoch [2223/3000], Loss: 0.009542129002511501\n",
      "Epoch [2224/3000], Loss: 0.009541518986225128\n",
      "Epoch [2225/3000], Loss: 0.009540922939777374\n",
      "Epoch [2226/3000], Loss: 0.009540341794490814\n",
      "Epoch [2227/3000], Loss: 0.009539775550365448\n",
      "Epoch [2228/3000], Loss: 0.009539226070046425\n",
      "Epoch [2229/3000], Loss: 0.009538693353533745\n",
      "Epoch [2230/3000], Loss: 0.009538178332149982\n",
      "Epoch [2231/3000], Loss: 0.009537681937217712\n",
      "Epoch [2232/3000], Loss: 0.009537200443446636\n",
      "Epoch [2233/3000], Loss: 0.009536737576127052\n",
      "Epoch [2234/3000], Loss: 0.009536290541291237\n",
      "Epoch [2235/3000], Loss: 0.009535858407616615\n",
      "Epoch [2236/3000], Loss: 0.009535439312458038\n",
      "Epoch [2237/3000], Loss: 0.009535033255815506\n",
      "Epoch [2238/3000], Loss: 0.009534640237689018\n",
      "Epoch [2239/3000], Loss: 0.009534255601465702\n",
      "Epoch [2240/3000], Loss: 0.009533879347145557\n",
      "Epoch [2241/3000], Loss: 0.009533511474728584\n",
      "Epoch [2242/3000], Loss: 0.009533149190247059\n",
      "Epoch [2243/3000], Loss: 0.009532791562378407\n",
      "Epoch [2244/3000], Loss: 0.009532442316412926\n",
      "Epoch [2245/3000], Loss: 0.009532105177640915\n",
      "Epoch [2246/3000], Loss: 0.009531800635159016\n",
      "Epoch [2247/3000], Loss: 0.009531588293612003\n",
      "Epoch [2248/3000], Loss: 0.00953165628015995\n",
      "Epoch [2249/3000], Loss: 0.00953247956931591\n",
      "Epoch [2250/3000], Loss: 0.009534764103591442\n",
      "Epoch [2251/3000], Loss: 0.009537446312606335\n",
      "Epoch [2252/3000], Loss: 0.009535718709230423\n",
      "Epoch [2253/3000], Loss: 0.009530271403491497\n",
      "Epoch [2254/3000], Loss: 0.009530246257781982\n",
      "Epoch [2255/3000], Loss: 0.00953295174986124\n",
      "Epoch [2256/3000], Loss: 0.009530249983072281\n",
      "Epoch [2257/3000], Loss: 0.00952840130776167\n",
      "Epoch [2258/3000], Loss: 0.009530462324619293\n",
      "Epoch [2259/3000], Loss: 0.009528862312436104\n",
      "Epoch [2260/3000], Loss: 0.009527414105832577\n",
      "Epoch [2261/3000], Loss: 0.009528801776468754\n",
      "Epoch [2262/3000], Loss: 0.009527286514639854\n",
      "Epoch [2263/3000], Loss: 0.009526651352643967\n",
      "Epoch [2264/3000], Loss: 0.009527395479381084\n",
      "Epoch [2265/3000], Loss: 0.009525897912681103\n",
      "Epoch [2266/3000], Loss: 0.00952596589922905\n",
      "Epoch [2267/3000], Loss: 0.00952601246535778\n",
      "Epoch [2268/3000], Loss: 0.00952482782304287\n",
      "Epoch [2269/3000], Loss: 0.009525190107524395\n",
      "Epoch [2270/3000], Loss: 0.009524663910269737\n",
      "Epoch [2271/3000], Loss: 0.00952402874827385\n",
      "Epoch [2272/3000], Loss: 0.009524231776595116\n",
      "Epoch [2273/3000], Loss: 0.009523482993245125\n",
      "Epoch [2274/3000], Loss: 0.009523306973278522\n",
      "Epoch [2275/3000], Loss: 0.009523143991827965\n",
      "Epoch [2276/3000], Loss: 0.009522517211735249\n",
      "Epoch [2277/3000], Loss: 0.009522493928670883\n",
      "Epoch [2278/3000], Loss: 0.009522057138383389\n",
      "Epoch [2279/3000], Loss: 0.009521682746708393\n",
      "Epoch [2280/3000], Loss: 0.009521559812128544\n",
      "Epoch [2281/3000], Loss: 0.0095210587605834\n",
      "Epoch [2282/3000], Loss: 0.00952085293829441\n",
      "Epoch [2283/3000], Loss: 0.00952056422829628\n",
      "Epoch [2284/3000], Loss: 0.009520146995782852\n",
      "Epoch [2285/3000], Loss: 0.009519959799945354\n",
      "Epoch [2286/3000], Loss: 0.009519575163722038\n",
      "Epoch [2287/3000], Loss: 0.009519260376691818\n",
      "Epoch [2288/3000], Loss: 0.009519005194306374\n",
      "Epoch [2289/3000], Loss: 0.00951861310750246\n",
      "Epoch [2290/3000], Loss: 0.00951834861189127\n",
      "Epoch [2291/3000], Loss: 0.009518023580312729\n",
      "Epoch [2292/3000], Loss: 0.009517666883766651\n",
      "Epoch [2293/3000], Loss: 0.009517391212284565\n",
      "Epoch [2294/3000], Loss: 0.009517030790448189\n",
      "Epoch [2295/3000], Loss: 0.009516706690192223\n",
      "Epoch [2296/3000], Loss: 0.009516395628452301\n",
      "Epoch [2297/3000], Loss: 0.009516028687357903\n",
      "Epoch [2298/3000], Loss: 0.009515711106359959\n",
      "Epoch [2299/3000], Loss: 0.009515366517007351\n",
      "Epoch [2300/3000], Loss: 0.0095150051638484\n",
      "Epoch [2301/3000], Loss: 0.009514672681689262\n",
      "Epoch [2302/3000], Loss: 0.009514306671917439\n",
      "Epoch [2303/3000], Loss: 0.009513948112726212\n",
      "Epoch [2304/3000], Loss: 0.009513591416180134\n",
      "Epoch [2305/3000], Loss: 0.00951321143656969\n",
      "Epoch [2306/3000], Loss: 0.009512844495475292\n",
      "Epoch [2307/3000], Loss: 0.009512466378509998\n",
      "Epoch [2308/3000], Loss: 0.009512074291706085\n",
      "Epoch [2309/3000], Loss: 0.009511692449450493\n",
      "Epoch [2310/3000], Loss: 0.009511290118098259\n",
      "Epoch [2311/3000], Loss: 0.009510887786746025\n",
      "Epoch [2312/3000], Loss: 0.009510482661426067\n",
      "Epoch [2313/3000], Loss: 0.009510062634944916\n",
      "Epoch [2314/3000], Loss: 0.009509642608463764\n",
      "Epoch [2315/3000], Loss: 0.009509214200079441\n",
      "Epoch [2316/3000], Loss: 0.009508773684501648\n",
      "Epoch [2317/3000], Loss: 0.009508333168923855\n",
      "Epoch [2318/3000], Loss: 0.009507878683507442\n",
      "Epoch [2319/3000], Loss: 0.009507417678833008\n",
      "Epoch [2320/3000], Loss: 0.0095069520175457\n",
      "Epoch [2321/3000], Loss: 0.009506472386419773\n",
      "Epoch [2322/3000], Loss: 0.009505986236035824\n",
      "Epoch [2323/3000], Loss: 0.009505492635071278\n",
      "Epoch [2324/3000], Loss: 0.009504986926913261\n",
      "Epoch [2325/3000], Loss: 0.009504473768174648\n",
      "Epoch [2326/3000], Loss: 0.00950394943356514\n",
      "Epoch [2327/3000], Loss: 0.00950341485440731\n",
      "Epoch [2328/3000], Loss: 0.00950287189334631\n",
      "Epoch [2329/3000], Loss: 0.009502317756414413\n",
      "Epoch [2330/3000], Loss: 0.009501751512289047\n",
      "Epoch [2331/3000], Loss: 0.00950117688626051\n",
      "Epoch [2332/3000], Loss: 0.009500590153038502\n",
      "Epoch [2333/3000], Loss: 0.009499992243945599\n",
      "Epoch [2334/3000], Loss: 0.009499384090304375\n",
      "Epoch [2335/3000], Loss: 0.009498764760792255\n",
      "Epoch [2336/3000], Loss: 0.009498135186731815\n",
      "Epoch [2337/3000], Loss: 0.00949749443680048\n",
      "Epoch [2338/3000], Loss: 0.009496843442320824\n",
      "Epoch [2339/3000], Loss: 0.009496183134615421\n",
      "Epoch [2340/3000], Loss: 0.009495510719716549\n",
      "Epoch [2341/3000], Loss: 0.009494829922914505\n",
      "Epoch [2342/3000], Loss: 0.009494139812886715\n",
      "Epoch [2343/3000], Loss: 0.009493440389633179\n",
      "Epoch [2344/3000], Loss: 0.009492733515799046\n",
      "Epoch [2345/3000], Loss: 0.00949202012270689\n",
      "Epoch [2346/3000], Loss: 0.009491300210356712\n",
      "Epoch [2347/3000], Loss: 0.009490573778748512\n",
      "Epoch [2348/3000], Loss: 0.009489845484495163\n",
      "Epoch [2349/3000], Loss: 0.009489113464951515\n",
      "Epoch [2350/3000], Loss: 0.009488380514085293\n",
      "Epoch [2351/3000], Loss: 0.009487648494541645\n",
      "Epoch [2352/3000], Loss: 0.009486918337643147\n",
      "Epoch [2353/3000], Loss: 0.009486190974712372\n",
      "Epoch [2354/3000], Loss: 0.009485471062362194\n",
      "Epoch [2355/3000], Loss: 0.009484756737947464\n",
      "Epoch [2356/3000], Loss: 0.00948405358940363\n",
      "Epoch [2357/3000], Loss: 0.00948335975408554\n",
      "Epoch [2358/3000], Loss: 0.009482678957283497\n",
      "Epoch [2359/3000], Loss: 0.009482013992965221\n",
      "Epoch [2360/3000], Loss: 0.009481365792453289\n",
      "Epoch [2361/3000], Loss: 0.0094807343557477\n",
      "Epoch [2362/3000], Loss: 0.009480123408138752\n",
      "Epoch [2363/3000], Loss: 0.009479530155658722\n",
      "Epoch [2364/3000], Loss: 0.009478959254920483\n",
      "Epoch [2365/3000], Loss: 0.00947840791195631\n",
      "Epoch [2366/3000], Loss: 0.009477878920733929\n",
      "Epoch [2367/3000], Loss: 0.009477369487285614\n",
      "Epoch [2368/3000], Loss: 0.009476880542933941\n",
      "Epoch [2369/3000], Loss: 0.00947640836238861\n",
      "Epoch [2370/3000], Loss: 0.009475954808294773\n",
      "Epoch [2371/3000], Loss: 0.009475517086684704\n",
      "Epoch [2372/3000], Loss: 0.009475093334913254\n",
      "Epoch [2373/3000], Loss: 0.009474681690335274\n",
      "Epoch [2374/3000], Loss: 0.009474281221628189\n",
      "Epoch [2375/3000], Loss: 0.00947389006614685\n",
      "Epoch [2376/3000], Loss: 0.009473511949181557\n",
      "Epoch [2377/3000], Loss: 0.0094731654971838\n",
      "Epoch [2378/3000], Loss: 0.009472929872572422\n",
      "Epoch [2379/3000], Loss: 0.00947312917560339\n",
      "Epoch [2380/3000], Loss: 0.009474939666688442\n",
      "Epoch [2381/3000], Loss: 0.009480861946940422\n",
      "Epoch [2382/3000], Loss: 0.009488429874181747\n",
      "Epoch [2383/3000], Loss: 0.009481114335358143\n",
      "Epoch [2384/3000], Loss: 0.009470570832490921\n",
      "Epoch [2385/3000], Loss: 0.009478225372731686\n",
      "Epoch [2386/3000], Loss: 0.009476573206484318\n",
      "Epoch [2387/3000], Loss: 0.00946962833404541\n",
      "Epoch [2388/3000], Loss: 0.009475717321038246\n",
      "Epoch [2389/3000], Loss: 0.009470962919294834\n",
      "Epoch [2390/3000], Loss: 0.009470228105783463\n",
      "Epoch [2391/3000], Loss: 0.009472498670220375\n",
      "Epoch [2392/3000], Loss: 0.009467821568250656\n",
      "Epoch [2393/3000], Loss: 0.009470969438552856\n",
      "Epoch [2394/3000], Loss: 0.009467963129281998\n",
      "Epoch [2395/3000], Loss: 0.00946840737015009\n",
      "Epoch [2396/3000], Loss: 0.009468263946473598\n",
      "Epoch [2397/3000], Loss: 0.009466523304581642\n",
      "Epoch [2398/3000], Loss: 0.00946781225502491\n",
      "Epoch [2399/3000], Loss: 0.009465551935136318\n",
      "Epoch [2400/3000], Loss: 0.009466770105063915\n",
      "Epoch [2401/3000], Loss: 0.009465091861784458\n",
      "Epoch [2402/3000], Loss: 0.009465518407523632\n",
      "Epoch [2403/3000], Loss: 0.009464738890528679\n",
      "Epoch [2404/3000], Loss: 0.009464353322982788\n",
      "Epoch [2405/3000], Loss: 0.009464285336434841\n",
      "Epoch [2406/3000], Loss: 0.009463395923376083\n",
      "Epoch [2407/3000], Loss: 0.009463663212954998\n",
      "Epoch [2408/3000], Loss: 0.009462636895477772\n",
      "Epoch [2409/3000], Loss: 0.009462898597121239\n",
      "Epoch [2410/3000], Loss: 0.009462011978030205\n",
      "Epoch [2411/3000], Loss: 0.00946206133812666\n",
      "Epoch [2412/3000], Loss: 0.009461442939937115\n",
      "Epoch [2413/3000], Loss: 0.009461220353841782\n",
      "Epoch [2414/3000], Loss: 0.009460863657295704\n",
      "Epoch [2415/3000], Loss: 0.009460417553782463\n",
      "Epoch [2416/3000], Loss: 0.009460236877202988\n",
      "Epoch [2417/3000], Loss: 0.009459675289690495\n",
      "Epoch [2418/3000], Loss: 0.00945955328643322\n",
      "Epoch [2419/3000], Loss: 0.009458983317017555\n",
      "Epoch [2420/3000], Loss: 0.009458821266889572\n",
      "Epoch [2421/3000], Loss: 0.009458322077989578\n",
      "Epoch [2422/3000], Loss: 0.00945806410163641\n",
      "Epoch [2423/3000], Loss: 0.009457663632929325\n",
      "Epoch [2424/3000], Loss: 0.009457303211092949\n",
      "Epoch [2425/3000], Loss: 0.009456987492740154\n",
      "Epoch [2426/3000], Loss: 0.009456552565097809\n",
      "Epoch [2427/3000], Loss: 0.009456275030970573\n",
      "Epoch [2428/3000], Loss: 0.009455817751586437\n",
      "Epoch [2429/3000], Loss: 0.009455529041588306\n",
      "Epoch [2430/3000], Loss: 0.009455089457333088\n",
      "Epoch [2431/3000], Loss: 0.00945475697517395\n",
      "Epoch [2432/3000], Loss: 0.009454354643821716\n",
      "Epoch [2433/3000], Loss: 0.009453969076275826\n",
      "Epoch [2434/3000], Loss: 0.009453600272536278\n",
      "Epoch [2435/3000], Loss: 0.009453177452087402\n",
      "Epoch [2436/3000], Loss: 0.009452816098928452\n",
      "Epoch [2437/3000], Loss: 0.009452379308640957\n",
      "Epoch [2438/3000], Loss: 0.009452004916965961\n",
      "Epoch [2439/3000], Loss: 0.009451569989323616\n",
      "Epoch [2440/3000], Loss: 0.009451166726648808\n",
      "Epoch [2441/3000], Loss: 0.009450742043554783\n",
      "Epoch [2442/3000], Loss: 0.009450312703847885\n",
      "Epoch [2443/3000], Loss: 0.009449890814721584\n",
      "Epoch [2444/3000], Loss: 0.009449440985918045\n",
      "Epoch [2445/3000], Loss: 0.009449012577533722\n",
      "Epoch [2446/3000], Loss: 0.009448549710214138\n",
      "Epoch [2447/3000], Loss: 0.009448107331991196\n",
      "Epoch [2448/3000], Loss: 0.009447639808058739\n",
      "Epoch [2449/3000], Loss: 0.009447176940739155\n",
      "Epoch [2450/3000], Loss: 0.0094467056915164\n",
      "Epoch [2451/3000], Loss: 0.009446223266422749\n",
      "Epoch [2452/3000], Loss: 0.009445744566619396\n",
      "Epoch [2453/3000], Loss: 0.009445248171687126\n",
      "Epoch [2454/3000], Loss: 0.009444755502045155\n",
      "Epoch [2455/3000], Loss: 0.009444249793887138\n",
      "Epoch [2456/3000], Loss: 0.009443743154406548\n",
      "Epoch [2457/3000], Loss: 0.00944322720170021\n",
      "Epoch [2458/3000], Loss: 0.009442704729735851\n",
      "Epoch [2459/3000], Loss: 0.009442177601158619\n",
      "Epoch [2460/3000], Loss: 0.00944164302200079\n",
      "Epoch [2461/3000], Loss: 0.009441106580197811\n",
      "Epoch [2462/3000], Loss: 0.009440558962523937\n",
      "Epoch [2463/3000], Loss: 0.009440012276172638\n",
      "Epoch [2464/3000], Loss: 0.009439457207918167\n",
      "Epoch [2465/3000], Loss: 0.009438898414373398\n",
      "Epoch [2466/3000], Loss: 0.00943833589553833\n",
      "Epoch [2467/3000], Loss: 0.00943776685744524\n",
      "Epoch [2468/3000], Loss: 0.00943719781935215\n",
      "Epoch [2469/3000], Loss: 0.009436623193323612\n",
      "Epoch [2470/3000], Loss: 0.009436049498617649\n",
      "Epoch [2471/3000], Loss: 0.009435471147298813\n",
      "Epoch [2472/3000], Loss: 0.009434892795979977\n",
      "Epoch [2473/3000], Loss: 0.009434313513338566\n",
      "Epoch [2474/3000], Loss: 0.00943373329937458\n",
      "Epoch [2475/3000], Loss: 0.009433156810700893\n",
      "Epoch [2476/3000], Loss: 0.009432578459382057\n",
      "Epoch [2477/3000], Loss: 0.009432004764676094\n",
      "Epoch [2478/3000], Loss: 0.009431433863937855\n",
      "Epoch [2479/3000], Loss: 0.009430867619812489\n",
      "Epoch [2480/3000], Loss: 0.009430306032299995\n",
      "Epoch [2481/3000], Loss: 0.009429749101400375\n",
      "Epoch [2482/3000], Loss: 0.009429199621081352\n",
      "Epoch [2483/3000], Loss: 0.009428657591342926\n",
      "Epoch [2484/3000], Loss: 0.009428123943507671\n",
      "Epoch [2485/3000], Loss: 0.009427597746253014\n",
      "Epoch [2486/3000], Loss: 0.009427081793546677\n",
      "Epoch [2487/3000], Loss: 0.00942657608538866\n",
      "Epoch [2488/3000], Loss: 0.00942607969045639\n",
      "Epoch [2489/3000], Loss: 0.009425594471395016\n",
      "Epoch [2490/3000], Loss: 0.009425120428204536\n",
      "Epoch [2491/3000], Loss: 0.009424657560884953\n",
      "Epoch [2492/3000], Loss: 0.009424205869436264\n",
      "Epoch [2493/3000], Loss: 0.009423764422535896\n",
      "Epoch [2494/3000], Loss: 0.009423334151506424\n",
      "Epoch [2495/3000], Loss: 0.009422913193702698\n",
      "Epoch [2496/3000], Loss: 0.009422502480447292\n",
      "Epoch [2497/3000], Loss: 0.009422102011740208\n",
      "Epoch [2498/3000], Loss: 0.009421709924936295\n",
      "Epoch [2499/3000], Loss: 0.009421326220035553\n",
      "Epoch [2500/3000], Loss: 0.009420948103070259\n",
      "Epoch [2501/3000], Loss: 0.009420578368008137\n",
      "Epoch [2502/3000], Loss: 0.009420213289558887\n",
      "Epoch [2503/3000], Loss: 0.009419853799045086\n",
      "Epoch [2504/3000], Loss: 0.009419498033821583\n",
      "Epoch [2505/3000], Loss: 0.009419146925210953\n",
      "Epoch [2506/3000], Loss: 0.009418796747922897\n",
      "Epoch [2507/3000], Loss: 0.00941845029592514\n",
      "Epoch [2508/3000], Loss: 0.009418104775249958\n",
      "Epoch [2509/3000], Loss: 0.00941776018589735\n",
      "Epoch [2510/3000], Loss: 0.009417417459189892\n",
      "Epoch [2511/3000], Loss: 0.009417073801159859\n",
      "Epoch [2512/3000], Loss: 0.009416732005774975\n",
      "Epoch [2513/3000], Loss: 0.009416388347744942\n",
      "Epoch [2514/3000], Loss: 0.009416046552360058\n",
      "Epoch [2515/3000], Loss: 0.0094157038256526\n",
      "Epoch [2516/3000], Loss: 0.009415359236299992\n",
      "Epoch [2517/3000], Loss: 0.009415017440915108\n",
      "Epoch [2518/3000], Loss: 0.0094146728515625\n",
      "Epoch [2519/3000], Loss: 0.009414328262209892\n",
      "Epoch [2520/3000], Loss: 0.009413983672857285\n",
      "Epoch [2521/3000], Loss: 0.009413638152182102\n",
      "Epoch [2522/3000], Loss: 0.009413291700184345\n",
      "Epoch [2523/3000], Loss: 0.009412946179509163\n",
      "Epoch [2524/3000], Loss: 0.009412599727511406\n",
      "Epoch [2525/3000], Loss: 0.0094122514128685\n",
      "Epoch [2526/3000], Loss: 0.009411903098225594\n",
      "Epoch [2527/3000], Loss: 0.009411552920937538\n",
      "Epoch [2528/3000], Loss: 0.009411203674972057\n",
      "Epoch [2529/3000], Loss: 0.009410851635038853\n",
      "Epoch [2530/3000], Loss: 0.009410498663783073\n",
      "Epoch [2531/3000], Loss: 0.00941014476120472\n",
      "Epoch [2532/3000], Loss: 0.009409788995981216\n",
      "Epoch [2533/3000], Loss: 0.00940943043678999\n",
      "Epoch [2534/3000], Loss: 0.009409070014953613\n",
      "Epoch [2535/3000], Loss: 0.009408708661794662\n",
      "Epoch [2536/3000], Loss: 0.009408344514667988\n",
      "Epoch [2537/3000], Loss: 0.009407978504896164\n",
      "Epoch [2538/3000], Loss: 0.009407608769834042\n",
      "Epoch [2539/3000], Loss: 0.009407236240804195\n",
      "Epoch [2540/3000], Loss: 0.009406860917806625\n",
      "Epoch [2541/3000], Loss: 0.009406480938196182\n",
      "Epoch [2542/3000], Loss: 0.00940609909594059\n",
      "Epoch [2543/3000], Loss: 0.009405714459717274\n",
      "Epoch [2544/3000], Loss: 0.009405323304235935\n",
      "Epoch [2545/3000], Loss: 0.009404930286109447\n",
      "Epoch [2546/3000], Loss: 0.009404532611370087\n",
      "Epoch [2547/3000], Loss: 0.009404131211340427\n",
      "Epoch [2548/3000], Loss: 0.009403723292052746\n",
      "Epoch [2549/3000], Loss: 0.00940331257879734\n",
      "Epoch [2550/3000], Loss: 0.009402896277606487\n",
      "Epoch [2551/3000], Loss: 0.00940247718244791\n",
      "Epoch [2552/3000], Loss: 0.009402049705386162\n",
      "Epoch [2553/3000], Loss: 0.00940161943435669\n",
      "Epoch [2554/3000], Loss: 0.009401182644069195\n",
      "Epoch [2555/3000], Loss: 0.009400741197168827\n",
      "Epoch [2556/3000], Loss: 0.009400294162333012\n",
      "Epoch [2557/3000], Loss: 0.009399841539561749\n",
      "Epoch [2558/3000], Loss: 0.009399382397532463\n",
      "Epoch [2559/3000], Loss: 0.00939891766756773\n",
      "Epoch [2560/3000], Loss: 0.00939844734966755\n",
      "Epoch [2561/3000], Loss: 0.00939797144383192\n",
      "Epoch [2562/3000], Loss: 0.009397488087415695\n",
      "Epoch [2563/3000], Loss: 0.009397001005709171\n",
      "Epoch [2564/3000], Loss: 0.009396505542099476\n",
      "Epoch [2565/3000], Loss: 0.009396005421876907\n",
      "Epoch [2566/3000], Loss: 0.009395498782396317\n",
      "Epoch [2567/3000], Loss: 0.009394986554980278\n",
      "Epoch [2568/3000], Loss: 0.009394467808306217\n",
      "Epoch [2569/3000], Loss: 0.009393944405019283\n",
      "Epoch [2570/3000], Loss: 0.009393415413796902\n",
      "Epoch [2571/3000], Loss: 0.009392880834639072\n",
      "Epoch [2572/3000], Loss: 0.00939234159886837\n",
      "Epoch [2573/3000], Loss: 0.00939179677516222\n",
      "Epoch [2574/3000], Loss: 0.009391247294843197\n",
      "Epoch [2575/3000], Loss: 0.009390692226588726\n",
      "Epoch [2576/3000], Loss: 0.009390134364366531\n",
      "Epoch [2577/3000], Loss: 0.009389573708176613\n",
      "Epoch [2578/3000], Loss: 0.00938901025801897\n",
      "Epoch [2579/3000], Loss: 0.009388444013893604\n",
      "Epoch [2580/3000], Loss: 0.009387874975800514\n",
      "Epoch [2581/3000], Loss: 0.009387306869029999\n",
      "Epoch [2582/3000], Loss: 0.00938673596829176\n",
      "Epoch [2583/3000], Loss: 0.009386167861521244\n",
      "Epoch [2584/3000], Loss: 0.009385598823428154\n",
      "Epoch [2585/3000], Loss: 0.009385033510625362\n",
      "Epoch [2586/3000], Loss: 0.009384469129145145\n",
      "Epoch [2587/3000], Loss: 0.009383909404277802\n",
      "Epoch [2588/3000], Loss: 0.009383355267345905\n",
      "Epoch [2589/3000], Loss: 0.009382805787026882\n",
      "Epoch [2590/3000], Loss: 0.009382261894643307\n",
      "Epoch [2591/3000], Loss: 0.009381727315485477\n",
      "Epoch [2592/3000], Loss: 0.009381200186908245\n",
      "Epoch [2593/3000], Loss: 0.009380681440234184\n",
      "Epoch [2594/3000], Loss: 0.009380173869431019\n",
      "Epoch [2595/3000], Loss: 0.009379676543176174\n",
      "Epoch [2596/3000], Loss: 0.0093791913241148\n",
      "Epoch [2597/3000], Loss: 0.00937871728092432\n",
      "Epoch [2598/3000], Loss: 0.00937825720757246\n",
      "Epoch [2599/3000], Loss: 0.009377808310091496\n",
      "Epoch [2600/3000], Loss: 0.0093773752450943\n",
      "Epoch [2601/3000], Loss: 0.009376953355967999\n",
      "Epoch [2602/3000], Loss: 0.009376544505357742\n",
      "Epoch [2603/3000], Loss: 0.009376149624586105\n",
      "Epoch [2604/3000], Loss: 0.009375768713653088\n",
      "Epoch [2605/3000], Loss: 0.009375398978590965\n",
      "Epoch [2606/3000], Loss: 0.009375040419399738\n",
      "Epoch [2607/3000], Loss: 0.009374693036079407\n",
      "Epoch [2608/3000], Loss: 0.009374357759952545\n",
      "Epoch [2609/3000], Loss: 0.00937403179705143\n",
      "Epoch [2610/3000], Loss: 0.009373714216053486\n",
      "Epoch [2611/3000], Loss: 0.009373405948281288\n",
      "Epoch [2612/3000], Loss: 0.009373104199767113\n",
      "Epoch [2613/3000], Loss: 0.009372808039188385\n",
      "Epoch [2614/3000], Loss: 0.009372517466545105\n",
      "Epoch [2615/3000], Loss: 0.009372232481837273\n",
      "Epoch [2616/3000], Loss: 0.009371952153742313\n",
      "Epoch [2617/3000], Loss: 0.009371676482260227\n",
      "Epoch [2618/3000], Loss: 0.009371411055326462\n",
      "Epoch [2619/3000], Loss: 0.009371170774102211\n",
      "Epoch [2620/3000], Loss: 0.00937102735042572\n",
      "Epoch [2621/3000], Loss: 0.009371253661811352\n",
      "Epoch [2622/3000], Loss: 0.009372832253575325\n",
      "Epoch [2623/3000], Loss: 0.009378154762089252\n",
      "Epoch [2624/3000], Loss: 0.009386558085680008\n",
      "Epoch [2625/3000], Loss: 0.009383145719766617\n",
      "Epoch [2626/3000], Loss: 0.00937007088214159\n",
      "Epoch [2627/3000], Loss: 0.009374978020787239\n",
      "Epoch [2628/3000], Loss: 0.009378168731927872\n",
      "Epoch [2629/3000], Loss: 0.00936893466860056\n",
      "Epoch [2630/3000], Loss: 0.009373732842504978\n",
      "Epoch [2631/3000], Loss: 0.009372836910188198\n",
      "Epoch [2632/3000], Loss: 0.009368243627250195\n",
      "Epoch [2633/3000], Loss: 0.009372912347316742\n",
      "Epoch [2634/3000], Loss: 0.009368308819830418\n",
      "Epoch [2635/3000], Loss: 0.009369591251015663\n",
      "Epoch [2636/3000], Loss: 0.009369625709950924\n",
      "Epoch [2637/3000], Loss: 0.009367150254547596\n",
      "Epoch [2638/3000], Loss: 0.009369459003210068\n",
      "Epoch [2639/3000], Loss: 0.009366507641971111\n",
      "Epoch [2640/3000], Loss: 0.009368060156702995\n",
      "Epoch [2641/3000], Loss: 0.009366709738969803\n",
      "Epoch [2642/3000], Loss: 0.009366508573293686\n",
      "Epoch [2643/3000], Loss: 0.00936677772551775\n",
      "Epoch [2644/3000], Loss: 0.009365453384816647\n",
      "Epoch [2645/3000], Loss: 0.009366379119455814\n",
      "Epoch [2646/3000], Loss: 0.009364938363432884\n",
      "Epoch [2647/3000], Loss: 0.00936561543494463\n",
      "Epoch [2648/3000], Loss: 0.009364718571305275\n",
      "Epoch [2649/3000], Loss: 0.009364757686853409\n",
      "Epoch [2650/3000], Loss: 0.009364519268274307\n",
      "Epoch [2651/3000], Loss: 0.00936402939260006\n",
      "Epoch [2652/3000], Loss: 0.009364183060824871\n",
      "Epoch [2653/3000], Loss: 0.009363495744764805\n",
      "Epoch [2654/3000], Loss: 0.00936370063573122\n",
      "Epoch [2655/3000], Loss: 0.009363106451928616\n",
      "Epoch [2656/3000], Loss: 0.009363152086734772\n",
      "Epoch [2657/3000], Loss: 0.009362772107124329\n",
      "Epoch [2658/3000], Loss: 0.009362604469060898\n",
      "Epoch [2659/3000], Loss: 0.009362434037029743\n",
      "Epoch [2660/3000], Loss: 0.00936210434883833\n",
      "Epoch [2661/3000], Loss: 0.009362049400806427\n",
      "Epoch [2662/3000], Loss: 0.00936167687177658\n",
      "Epoch [2663/3000], Loss: 0.009361614473164082\n",
      "Epoch [2664/3000], Loss: 0.009361299686133862\n",
      "Epoch [2665/3000], Loss: 0.009361160919070244\n",
      "Epoch [2666/3000], Loss: 0.009360933676362038\n",
      "Epoch [2667/3000], Loss: 0.009360721334815025\n",
      "Epoch [2668/3000], Loss: 0.009360557422041893\n",
      "Epoch [2669/3000], Loss: 0.009360307827591896\n",
      "Epoch [2670/3000], Loss: 0.009360161609947681\n",
      "Epoch [2671/3000], Loss: 0.009359914809465408\n",
      "Epoch [2672/3000], Loss: 0.00935975369066\n",
      "Epoch [2673/3000], Loss: 0.009359534829854965\n",
      "Epoch [2674/3000], Loss: 0.009359344840049744\n",
      "Epoch [2675/3000], Loss: 0.009359153918921947\n",
      "Epoch [2676/3000], Loss: 0.009358944371342659\n",
      "Epoch [2677/3000], Loss: 0.009358766488730907\n",
      "Epoch [2678/3000], Loss: 0.009358552284538746\n",
      "Epoch [2679/3000], Loss: 0.00935837160795927\n",
      "Epoch [2680/3000], Loss: 0.00935816578567028\n",
      "Epoch [2681/3000], Loss: 0.00935797393321991\n",
      "Epoch [2682/3000], Loss: 0.009357779286801815\n",
      "Epoch [2683/3000], Loss: 0.009357579983770847\n",
      "Epoch [2684/3000], Loss: 0.009357388131320477\n",
      "Epoch [2685/3000], Loss: 0.009357186034321785\n",
      "Epoch [2686/3000], Loss: 0.009356995113193989\n",
      "Epoch [2687/3000], Loss: 0.009356792084872723\n",
      "Epoch [2688/3000], Loss: 0.009356598369777203\n",
      "Epoch [2689/3000], Loss: 0.00935639999806881\n",
      "Epoch [2690/3000], Loss: 0.009356200695037842\n",
      "Epoch [2691/3000], Loss: 0.009356003254652023\n",
      "Epoch [2692/3000], Loss: 0.009355800226330757\n",
      "Epoch [2693/3000], Loss: 0.009355602785944939\n",
      "Epoch [2694/3000], Loss: 0.009355401620268822\n",
      "Epoch [2695/3000], Loss: 0.009355200454592705\n",
      "Epoch [2696/3000], Loss: 0.009354996494948864\n",
      "Epoch [2697/3000], Loss: 0.009354793466627598\n",
      "Epoch [2698/3000], Loss: 0.009354591369628906\n",
      "Epoch [2699/3000], Loss: 0.009354385547339916\n",
      "Epoch [2700/3000], Loss: 0.009354181587696075\n",
      "Epoch [2701/3000], Loss: 0.009353973902761936\n",
      "Epoch [2702/3000], Loss: 0.009353766217827797\n",
      "Epoch [2703/3000], Loss: 0.009353558532893658\n",
      "Epoch [2704/3000], Loss: 0.009353348053991795\n",
      "Epoch [2705/3000], Loss: 0.009353138506412506\n",
      "Epoch [2706/3000], Loss: 0.009352927096188068\n",
      "Epoch [2707/3000], Loss: 0.009352713823318481\n",
      "Epoch [2708/3000], Loss: 0.009352498687803745\n",
      "Epoch [2709/3000], Loss: 0.009352283552289009\n",
      "Epoch [2710/3000], Loss: 0.009352067485451698\n",
      "Epoch [2711/3000], Loss: 0.009351848624646664\n",
      "Epoch [2712/3000], Loss: 0.009351629763841629\n",
      "Epoch [2713/3000], Loss: 0.00935140810906887\n",
      "Epoch [2714/3000], Loss: 0.009351186454296112\n",
      "Epoch [2715/3000], Loss: 0.00935096200555563\n",
      "Epoch [2716/3000], Loss: 0.009350736625492573\n",
      "Epoch [2717/3000], Loss: 0.009350509382784367\n",
      "Epoch [2718/3000], Loss: 0.009350279346108437\n",
      "Epoch [2719/3000], Loss: 0.009350048378109932\n",
      "Epoch [2720/3000], Loss: 0.009349815547466278\n",
      "Epoch [2721/3000], Loss: 0.00934958178550005\n",
      "Epoch [2722/3000], Loss: 0.009349343366920948\n",
      "Epoch [2723/3000], Loss: 0.009349105879664421\n",
      "Epoch [2724/3000], Loss: 0.009348864667117596\n",
      "Epoch [2725/3000], Loss: 0.009348620660603046\n",
      "Epoch [2726/3000], Loss: 0.009348375722765923\n",
      "Epoch [2727/3000], Loss: 0.0093481270596385\n",
      "Epoch [2728/3000], Loss: 0.009347879327833652\n",
      "Epoch [2729/3000], Loss: 0.009347626008093357\n",
      "Epoch [2730/3000], Loss: 0.009347370825707912\n",
      "Epoch [2731/3000], Loss: 0.009347112849354744\n",
      "Epoch [2732/3000], Loss: 0.009346852079033852\n",
      "Epoch [2733/3000], Loss: 0.00934658944606781\n",
      "Epoch [2734/3000], Loss: 0.009346324019134045\n",
      "Epoch [2735/3000], Loss: 0.009346053935587406\n",
      "Epoch [2736/3000], Loss: 0.009345781989395618\n",
      "Epoch [2737/3000], Loss: 0.009345507249236107\n",
      "Epoch [2738/3000], Loss: 0.009345228783786297\n",
      "Epoch [2739/3000], Loss: 0.009344946593046188\n",
      "Epoch [2740/3000], Loss: 0.00934466253966093\n",
      "Epoch [2741/3000], Loss: 0.0093443738296628\n",
      "Epoch [2742/3000], Loss: 0.009344082325696945\n",
      "Epoch [2743/3000], Loss: 0.009343787096440792\n",
      "Epoch [2744/3000], Loss: 0.009343487210571766\n",
      "Epoch [2745/3000], Loss: 0.009343183599412441\n",
      "Epoch [2746/3000], Loss: 0.009342877194285393\n",
      "Epoch [2747/3000], Loss: 0.009342567063868046\n",
      "Epoch [2748/3000], Loss: 0.009342252276837826\n",
      "Epoch [2749/3000], Loss: 0.009341932833194733\n",
      "Epoch [2750/3000], Loss: 0.009341609664261341\n",
      "Epoch [2751/3000], Loss: 0.009341282770037651\n",
      "Epoch [2752/3000], Loss: 0.009340950287878513\n",
      "Epoch [2753/3000], Loss: 0.009340614080429077\n",
      "Epoch [2754/3000], Loss: 0.009340274147689342\n",
      "Epoch [2755/3000], Loss: 0.00933992862701416\n",
      "Epoch [2756/3000], Loss: 0.009339578449726105\n",
      "Epoch [2757/3000], Loss: 0.009339223615825176\n",
      "Epoch [2758/3000], Loss: 0.00933886505663395\n",
      "Epoch [2759/3000], Loss: 0.0093384999781847\n",
      "Epoch [2760/3000], Loss: 0.009338132105767727\n",
      "Epoch [2761/3000], Loss: 0.009337757714092731\n",
      "Epoch [2762/3000], Loss: 0.009337378665804863\n",
      "Epoch [2763/3000], Loss: 0.009336995892226696\n",
      "Epoch [2764/3000], Loss: 0.009336607530713081\n",
      "Epoch [2765/3000], Loss: 0.009336214512586594\n",
      "Epoch [2766/3000], Loss: 0.009335816837847233\n",
      "Epoch [2767/3000], Loss: 0.009335414506494999\n",
      "Epoch [2768/3000], Loss: 0.009335007518529892\n",
      "Epoch [2769/3000], Loss: 0.009334596805274487\n",
      "Epoch [2770/3000], Loss: 0.009334179572761059\n",
      "Epoch [2771/3000], Loss: 0.009333758614957333\n",
      "Epoch [2772/3000], Loss: 0.009333333931863308\n",
      "Epoch [2773/3000], Loss: 0.009332905523478985\n",
      "Epoch [2774/3000], Loss: 0.009332473389804363\n",
      "Epoch [2775/3000], Loss: 0.009332037530839443\n",
      "Epoch [2776/3000], Loss: 0.009331597946584225\n",
      "Epoch [2777/3000], Loss: 0.009331154637038708\n",
      "Epoch [2778/3000], Loss: 0.00933071132749319\n",
      "Epoch [2779/3000], Loss: 0.009330262430012226\n",
      "Epoch [2780/3000], Loss: 0.009329813532531261\n",
      "Epoch [2781/3000], Loss: 0.009329361841082573\n",
      "Epoch [2782/3000], Loss: 0.009328910149633884\n",
      "Epoch [2783/3000], Loss: 0.009328456595540047\n",
      "Epoch [2784/3000], Loss: 0.009328003041446209\n",
      "Epoch [2785/3000], Loss: 0.009327550418674946\n",
      "Epoch [2786/3000], Loss: 0.009327099658548832\n",
      "Epoch [2787/3000], Loss: 0.009326647967100143\n",
      "Epoch [2788/3000], Loss: 0.009326199069619179\n",
      "Epoch [2789/3000], Loss: 0.009325752966105938\n",
      "Epoch [2790/3000], Loss: 0.00932531151920557\n",
      "Epoch [2791/3000], Loss: 0.009324872866272926\n",
      "Epoch [2792/3000], Loss: 0.009324438869953156\n",
      "Epoch [2793/3000], Loss: 0.009324011392891407\n",
      "Epoch [2794/3000], Loss: 0.009323586709797382\n",
      "Epoch [2795/3000], Loss: 0.009323171339929104\n",
      "Epoch [2796/3000], Loss: 0.009322762489318848\n",
      "Epoch [2797/3000], Loss: 0.009322360157966614\n",
      "Epoch [2798/3000], Loss: 0.009321967139840126\n",
      "Epoch [2799/3000], Loss: 0.009321581572294235\n",
      "Epoch [2800/3000], Loss: 0.009321206249296665\n",
      "Epoch [2801/3000], Loss: 0.009320839308202267\n",
      "Epoch [2802/3000], Loss: 0.009320481680333614\n",
      "Epoch [2803/3000], Loss: 0.009320135228335857\n",
      "Epoch [2804/3000], Loss: 0.009319797158241272\n",
      "Epoch [2805/3000], Loss: 0.009319471195340157\n",
      "Epoch [2806/3000], Loss: 0.009319152683019638\n",
      "Epoch [2807/3000], Loss: 0.009318845346570015\n",
      "Epoch [2808/3000], Loss: 0.009318547323346138\n",
      "Epoch [2809/3000], Loss: 0.009318256750702858\n",
      "Epoch [2810/3000], Loss: 0.009317977353930473\n",
      "Epoch [2811/3000], Loss: 0.009317705407738686\n",
      "Epoch [2812/3000], Loss: 0.00931744184345007\n",
      "Epoch [2813/3000], Loss: 0.00931718572974205\n",
      "Epoch [2814/3000], Loss: 0.009316936135292053\n",
      "Epoch [2815/3000], Loss: 0.009316693060100079\n",
      "Epoch [2816/3000], Loss: 0.009316455572843552\n",
      "Epoch [2817/3000], Loss: 0.009316223673522472\n",
      "Epoch [2818/3000], Loss: 0.009315996430814266\n",
      "Epoch [2819/3000], Loss: 0.009315773844718933\n",
      "Epoch [2820/3000], Loss: 0.009315554052591324\n",
      "Epoch [2821/3000], Loss: 0.009315337985754013\n",
      "Epoch [2822/3000], Loss: 0.009315124712884426\n",
      "Epoch [2823/3000], Loss: 0.009314914233982563\n",
      "Epoch [2824/3000], Loss: 0.009314704686403275\n",
      "Epoch [2825/3000], Loss: 0.009314497001469135\n",
      "Epoch [2826/3000], Loss: 0.009314293041825294\n",
      "Epoch [2827/3000], Loss: 0.009314089082181454\n",
      "Epoch [2828/3000], Loss: 0.009313886053860188\n",
      "Epoch [2829/3000], Loss: 0.00931368488818407\n",
      "Epoch [2830/3000], Loss: 0.009313484653830528\n",
      "Epoch [2831/3000], Loss: 0.009313286282122135\n",
      "Epoch [2832/3000], Loss: 0.009313087910413742\n",
      "Epoch [2833/3000], Loss: 0.009312891401350498\n",
      "Epoch [2834/3000], Loss: 0.009312696754932404\n",
      "Epoch [2835/3000], Loss: 0.009312502108514309\n",
      "Epoch [2836/3000], Loss: 0.009312310256063938\n",
      "Epoch [2837/3000], Loss: 0.009312116540968418\n",
      "Epoch [2838/3000], Loss: 0.009311925619840622\n",
      "Epoch [2839/3000], Loss: 0.009311736561357975\n",
      "Epoch [2840/3000], Loss: 0.009311549365520477\n",
      "Epoch [2841/3000], Loss: 0.00931136216968298\n",
      "Epoch [2842/3000], Loss: 0.009311176836490631\n",
      "Epoch [2843/3000], Loss: 0.009310991503298283\n",
      "Epoch [2844/3000], Loss: 0.009310808032751083\n",
      "Epoch [2845/3000], Loss: 0.009310625493526459\n",
      "Epoch [2846/3000], Loss: 0.009310444816946983\n",
      "Epoch [2847/3000], Loss: 0.009310264140367508\n",
      "Epoch [2848/3000], Loss: 0.009310087189078331\n",
      "Epoch [2849/3000], Loss: 0.009309908375144005\n",
      "Epoch [2850/3000], Loss: 0.009309732355177402\n",
      "Epoch [2851/3000], Loss: 0.0093095563352108\n",
      "Epoch [2852/3000], Loss: 0.009309382177889347\n",
      "Epoch [2853/3000], Loss: 0.009309208020567894\n",
      "Epoch [2854/3000], Loss: 0.009309034794569016\n",
      "Epoch [2855/3000], Loss: 0.009308861568570137\n",
      "Epoch [2856/3000], Loss: 0.009308690205216408\n",
      "Epoch [2857/3000], Loss: 0.009308518841862679\n",
      "Epoch [2858/3000], Loss: 0.009308349341154099\n",
      "Epoch [2859/3000], Loss: 0.009308178909122944\n",
      "Epoch [2860/3000], Loss: 0.00930800847709179\n",
      "Epoch [2861/3000], Loss: 0.009307840839028358\n",
      "Epoch [2862/3000], Loss: 0.009307672269642353\n",
      "Epoch [2863/3000], Loss: 0.009307504631578922\n",
      "Epoch [2864/3000], Loss: 0.009307336993515491\n",
      "Epoch [2865/3000], Loss: 0.009307170286774635\n",
      "Epoch [2866/3000], Loss: 0.009307002648711205\n",
      "Epoch [2867/3000], Loss: 0.009306835941970348\n",
      "Epoch [2868/3000], Loss: 0.009306671097874641\n",
      "Epoch [2869/3000], Loss: 0.00930650345981121\n",
      "Epoch [2870/3000], Loss: 0.009306338615715504\n",
      "Epoch [2871/3000], Loss: 0.009306173771619797\n",
      "Epoch [2872/3000], Loss: 0.009306007996201515\n",
      "Epoch [2873/3000], Loss: 0.009305842220783234\n",
      "Epoch [2874/3000], Loss: 0.009305678308010101\n",
      "Epoch [2875/3000], Loss: 0.00930551253259182\n",
      "Epoch [2876/3000], Loss: 0.009305348619818687\n",
      "Epoch [2877/3000], Loss: 0.009305182844400406\n",
      "Epoch [2878/3000], Loss: 0.009305018000304699\n",
      "Epoch [2879/3000], Loss: 0.009304854087531567\n",
      "Epoch [2880/3000], Loss: 0.00930468924343586\n",
      "Epoch [2881/3000], Loss: 0.009304524399340153\n",
      "Epoch [2882/3000], Loss: 0.009304359555244446\n",
      "Epoch [2883/3000], Loss: 0.009304194711148739\n",
      "Epoch [2884/3000], Loss: 0.009304029867053032\n",
      "Epoch [2885/3000], Loss: 0.00930386409163475\n",
      "Epoch [2886/3000], Loss: 0.009303699247539043\n",
      "Epoch [2887/3000], Loss: 0.009303534403443336\n",
      "Epoch [2888/3000], Loss: 0.00930336955934763\n",
      "Epoch [2889/3000], Loss: 0.009303202852606773\n",
      "Epoch [2890/3000], Loss: 0.009303038008511066\n",
      "Epoch [2891/3000], Loss: 0.009302872233092785\n",
      "Epoch [2892/3000], Loss: 0.009302704595029354\n",
      "Epoch [2893/3000], Loss: 0.009302538819611073\n",
      "Epoch [2894/3000], Loss: 0.009302371181547642\n",
      "Epoch [2895/3000], Loss: 0.009302203543484211\n",
      "Epoch [2896/3000], Loss: 0.009302036836743355\n",
      "Epoch [2897/3000], Loss: 0.009301869198679924\n",
      "Epoch [2898/3000], Loss: 0.009301700629293919\n",
      "Epoch [2899/3000], Loss: 0.009301531128585339\n",
      "Epoch [2900/3000], Loss: 0.009301361627876759\n",
      "Epoch [2901/3000], Loss: 0.009301193058490753\n",
      "Epoch [2902/3000], Loss: 0.009301022626459599\n",
      "Epoch [2903/3000], Loss: 0.00930085126310587\n",
      "Epoch [2904/3000], Loss: 0.00930068176239729\n",
      "Epoch [2905/3000], Loss: 0.00930050853639841\n",
      "Epoch [2906/3000], Loss: 0.009300336241722107\n",
      "Epoch [2907/3000], Loss: 0.009300163015723228\n",
      "Epoch [2908/3000], Loss: 0.009299988858401775\n",
      "Epoch [2909/3000], Loss: 0.009299814701080322\n",
      "Epoch [2910/3000], Loss: 0.00929964054375887\n",
      "Epoch [2911/3000], Loss: 0.009299464523792267\n",
      "Epoch [2912/3000], Loss: 0.009299288503825665\n",
      "Epoch [2913/3000], Loss: 0.009299111552536488\n",
      "Epoch [2914/3000], Loss: 0.009298933669924736\n",
      "Epoch [2915/3000], Loss: 0.009298756718635559\n",
      "Epoch [2916/3000], Loss: 0.009298579767346382\n",
      "Epoch [2917/3000], Loss: 0.009298402816057205\n",
      "Epoch [2918/3000], Loss: 0.009298227727413177\n",
      "Epoch [2919/3000], Loss: 0.009298056364059448\n",
      "Epoch [2920/3000], Loss: 0.00929789338260889\n",
      "Epoch [2921/3000], Loss: 0.009297743439674377\n",
      "Epoch [2922/3000], Loss: 0.00929761491715908\n",
      "Epoch [2923/3000], Loss: 0.009297524578869343\n",
      "Epoch [2924/3000], Loss: 0.009297490119934082\n",
      "Epoch [2925/3000], Loss: 0.009297523647546768\n",
      "Epoch [2926/3000], Loss: 0.00929761677980423\n",
      "Epoch [2927/3000], Loss: 0.00929769966751337\n",
      "Epoch [2928/3000], Loss: 0.009297618642449379\n",
      "Epoch [2929/3000], Loss: 0.009297231212258339\n",
      "Epoch [2930/3000], Loss: 0.009296567179262638\n",
      "Epoch [2931/3000], Loss: 0.009295910596847534\n",
      "Epoch [2932/3000], Loss: 0.009295557625591755\n",
      "Epoch [2933/3000], Loss: 0.009295535273849964\n",
      "Epoch [2934/3000], Loss: 0.009295607917010784\n",
      "Epoch [2935/3000], Loss: 0.009295493364334106\n",
      "Epoch [2936/3000], Loss: 0.009295114316046238\n",
      "Epoch [2937/3000], Loss: 0.009294649586081505\n",
      "Epoch [2938/3000], Loss: 0.009294342249631882\n",
      "Epoch [2939/3000], Loss: 0.00929423701018095\n",
      "Epoch [2940/3000], Loss: 0.009294173680245876\n",
      "Epoch [2941/3000], Loss: 0.009293977171182632\n",
      "Epoch [2942/3000], Loss: 0.009293637238442898\n",
      "Epoch [2943/3000], Loss: 0.009293296374380589\n",
      "Epoch [2944/3000], Loss: 0.009293071925640106\n",
      "Epoch [2945/3000], Loss: 0.009292935021221638\n",
      "Epoch [2946/3000], Loss: 0.009292769245803356\n",
      "Epoch [2947/3000], Loss: 0.00929250754415989\n",
      "Epoch [2948/3000], Loss: 0.009292196482419968\n",
      "Epoch [2949/3000], Loss: 0.00929192453622818\n",
      "Epoch [2950/3000], Loss: 0.009291722439229488\n",
      "Epoch [2951/3000], Loss: 0.00929153524339199\n",
      "Epoch [2952/3000], Loss: 0.009291301481425762\n",
      "Epoch [2953/3000], Loss: 0.009291020222008228\n",
      "Epoch [2954/3000], Loss: 0.009290734305977821\n",
      "Epoch [2955/3000], Loss: 0.009290482848882675\n",
      "Epoch [2956/3000], Loss: 0.009290259331464767\n",
      "Epoch [2957/3000], Loss: 0.00929002370685339\n",
      "Epoch [2958/3000], Loss: 0.0092897554859519\n",
      "Epoch [2959/3000], Loss: 0.00928946677595377\n",
      "Epoch [2960/3000], Loss: 0.009289185516536236\n",
      "Epoch [2961/3000], Loss: 0.009288922883570194\n",
      "Epoch [2962/3000], Loss: 0.009288666769862175\n",
      "Epoch [2963/3000], Loss: 0.009288394823670387\n",
      "Epoch [2964/3000], Loss: 0.009288102388381958\n",
      "Epoch [2965/3000], Loss: 0.009287802502512932\n",
      "Epoch [2966/3000], Loss: 0.009287508204579353\n",
      "Epoch [2967/3000], Loss: 0.009287218563258648\n",
      "Epoch [2968/3000], Loss: 0.009286927059292793\n",
      "Epoch [2969/3000], Loss: 0.009286624379456043\n",
      "Epoch [2970/3000], Loss: 0.0092863067984581\n",
      "Epoch [2971/3000], Loss: 0.009285985492169857\n",
      "Epoch [2972/3000], Loss: 0.00928566511720419\n",
      "Epoch [2973/3000], Loss: 0.009285342879593372\n",
      "Epoch [2974/3000], Loss: 0.009285015054047108\n",
      "Epoch [2975/3000], Loss: 0.009284677915275097\n",
      "Epoch [2976/3000], Loss: 0.009284332394599915\n",
      "Epoch [2977/3000], Loss: 0.009283979423344135\n",
      "Epoch [2978/3000], Loss: 0.009283623658120632\n",
      "Epoch [2979/3000], Loss: 0.009283263236284256\n",
      "Epoch [2980/3000], Loss: 0.009282897226512432\n",
      "Epoch [2981/3000], Loss: 0.009282521903514862\n",
      "Epoch [2982/3000], Loss: 0.00928214006125927\n",
      "Epoch [2983/3000], Loss: 0.009281748905777931\n",
      "Epoch [2984/3000], Loss: 0.00928135123103857\n",
      "Epoch [2985/3000], Loss: 0.009280948899686337\n",
      "Epoch [2986/3000], Loss: 0.009280539117753506\n",
      "Epoch [2987/3000], Loss: 0.009280120022594929\n",
      "Epoch [2988/3000], Loss: 0.00927969254553318\n",
      "Epoch [2989/3000], Loss: 0.009279257617890835\n",
      "Epoch [2990/3000], Loss: 0.009278813377022743\n",
      "Epoch [2991/3000], Loss: 0.00927836261689663\n",
      "Epoch [2992/3000], Loss: 0.009277904406189919\n",
      "Epoch [2993/3000], Loss: 0.009277437813580036\n",
      "Epoch [2994/3000], Loss: 0.009276961907744408\n",
      "Epoch [2995/3000], Loss: 0.009276477620005608\n",
      "Epoch [2996/3000], Loss: 0.009275984019041061\n",
      "Epoch [2997/3000], Loss: 0.009275482036173344\n",
      "Epoch [2998/3000], Loss: 0.009274973534047604\n",
      "Epoch [2999/3000], Loss: 0.009274455718696117\n",
      "Epoch [3000/3000], Loss: 0.009273930452764034\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "sYejrSyd2jyl",
    "ExecuteTime": {
     "end_time": "2023-09-02T08:23:54.415909100Z",
     "start_time": "2023-09-02T08:23:48.040310Z"
    }
   },
   "outputs": [],
   "source": [
    "import statistics\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score_list_base = []\n",
    "acc_list_base = []\n",
    "\n",
    "f1_score_list = []\n",
    "acc_list = []\n",
    "for i in range(10):\n",
    "    print(\"*****\")\n",
    "    print(f'epoch : {i + 1} / 10')\n",
    "    # model = GATv2(dataset.num_features, dataset.num_classes, embedding_dimension=32).to(device)\n",
    "    model = GATv2(dataset.num_features, dataset.num_classes, embedding_dimension=8).to(device)\n",
    "    # model = GCN(128, dataset.num_classes).to(device)\n",
    "    acc_base, preds_base, labels_base = train_GNN_mode_and_save(model, data, 800, train_mask=new_train_mask,\n",
    "                                                                test_mask=new_test_mask)\n",
    "    preds_base = preds_base.cpu().numpy()\n",
    "    labels_base = labels_base.cpu().numpy()\n",
    "    f1_score_base = f1_score(labels_base, preds_base, average='macro')\n",
    "\n",
    "    f1_score_list_base.append(f1_score_base)\n",
    "    acc_list_base.append(acc_base)\n",
    "\n",
    "    acc, preds, labels = finalizer_DNN(train_mask=new_train_mask, epochs=4000, test_mask=new_test_mask,\n",
    "                                       repeat_threshold=0.90,\n",
    "                                       file_name_enc_emb=\"encoder_emb32_cora.pt\")\n",
    "\n",
    "    preds = preds.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    print(\"base model\")\n",
    "    print(acc_base)\n",
    "    print(f1_score_base)\n",
    "\n",
    "    print(\"OurModel\")\n",
    "    print(acc)\n",
    "    print(f1_score(labels, preds, average='macro'))\n",
    "    f1_score_list.append(f1_score(labels, preds, average='macro'))\n",
    "    acc_list.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"base model\")\n",
    "print_details(acc_list_base, f1_score_list_base)\n",
    "print(\"Our model\")\n",
    "print_details(acc_list, f1_score_list)"
   ],
   "metadata": {
    "id": "pdU2Jik3W5HM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "try 64"
   ],
   "metadata": {
    "id": "3uO3E_WEQWkO",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "GNN_model = GATv2(dataset.num_features, dataset.num_classes, embedding_dimension=8).to(device)\n",
    "run_model(GNN_model , Gnn_epochs=900 ,enc_address=\"/content/encoder_emb64_cora.pt\" ,reply_threshold=0.95 , head_epochs= 1400 , validation_mask=new_valdation_mask , early_stop=0.88 )"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "04pJOxwi5oW1",
    "outputId": "380f627b-157f-4cc2-b2fb-8355835277f6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "*****\n",
      "epoch : 1 / 10\n",
      "epoch : 50 = >  Accuracy: 0.3210\n",
      "epoch : 100 = >  Accuracy: 0.4330\n",
      "epoch : 150 = >  Accuracy: 0.4059\n",
      "epoch : 200 = >  Accuracy: 0.3862\n",
      "epoch : 250 = >  Accuracy: 0.3764\n",
      "epoch : 300 = >  Accuracy: 0.3825\n",
      "epoch : 350 = >  Accuracy: 0.4440\n",
      "epoch : 400 = >  Accuracy: 0.5547\n",
      "epoch : 450 = >  Accuracy: 0.6261\n",
      "epoch : 500 = >  Accuracy: 0.7011\n",
      "epoch : 550 = >  Accuracy: 0.7552\n",
      "epoch : 600 = >  Accuracy: 0.8020\n",
      "epoch : 650 = >  Accuracy: 0.8290\n",
      "epoch : 700 = >  Accuracy: 0.8450\n",
      "epoch : 750 = >  Accuracy: 0.8598\n",
      "epoch : 800 = >  Accuracy: 0.8684\n",
      "epoch : 850 = >  Accuracy: 0.8684\n",
      "epoch : 900 = >  Accuracy: 0.8708\n",
      "Final accuracy: 0.8708\n",
      "epoch 0 ,Accuracy: 0.1697 , train acc : 0.17429837518463812\n",
      "epoch 50 ,Accuracy: 0.7688 , train acc : 0.7754800590841949\n",
      "epoch 100 ,Accuracy: 0.7847 , train acc : 0.792466765140325\n",
      "epoch 150 ,Accuracy: 0.7798 , train acc : 0.8028064992614475\n",
      "epoch 200 ,Accuracy: 0.7774 , train acc : 0.7976366322008862\n",
      "epoch 250 ,Accuracy: 0.7823 , train acc : 0.8072378138847858\n",
      "epoch 300 ,Accuracy: 0.7884 , train acc : 0.810192023633678\n",
      "epoch 350 ,Accuracy: 0.8327 , train acc : 0.8641063515509602\n",
      "epoch 400 ,Accuracy: 0.8364 , train acc : 0.8714918759231906\n",
      "epoch 450 ,Accuracy: 0.8401 , train acc : 0.8722304283604135\n",
      "epoch 500 ,Accuracy: 0.8339 , train acc : 0.8729689807976366\n",
      "epoch 550 ,Accuracy: 0.8315 , train acc : 0.878138847858198\n",
      "epoch 600 ,Accuracy: 0.8339 , train acc : 0.8751846381093058\n",
      "epoch 650 ,Accuracy: 0.8352 , train acc : 0.8729689807976366\n",
      "epoch 700 ,Accuracy: 0.8241 , train acc : 0.8759231905465288\n",
      "epoch 750 ,Accuracy: 0.8278 , train acc : 0.8818316100443131\n",
      "epoch 800 ,Accuracy: 0.8315 , train acc : 0.8833087149187593\n",
      "epoch 850 ,Accuracy: 0.8339 , train acc : 0.8833087149187593\n",
      "epoch 900 ,Accuracy: 0.8290 , train acc : 0.8840472673559823\n",
      "epoch 950 ,Accuracy: 0.8327 , train acc : 0.8892171344165436\n",
      "epoch 1000 ,Accuracy: 0.8315 , train acc : 0.895864106351551\n",
      "epoch 1050 ,Accuracy: 0.8339 , train acc : 0.8936484490398818\n",
      "epoch 1100 ,Accuracy: 0.8389 , train acc : 0.8936484490398818\n",
      "epoch 1150 ,Accuracy: 0.8327 , train acc : 0.8951255539143279\n",
      "epoch 1200 ,Accuracy: 0.8290 , train acc : 0.8906942392909897\n",
      "epoch 1250 ,Accuracy: 0.8352 , train acc : 0.896602658788774\n",
      "epoch 1300 ,Accuracy: 0.8352 , train acc : 0.9017725258493353\n",
      "epoch 1350 ,Accuracy: 0.8278 , train acc : 0.9010339734121122\n",
      "epoch 0 ,Accuracy: 0.2854 , train acc : 0.2946824224519941\n",
      "epoch 50 ,Accuracy: 0.5498 , train acc : 0.5568685376661743\n",
      "epoch 100 ,Accuracy: 0.6076 , train acc : 0.603397341211226\n",
      "epoch 150 ,Accuracy: 0.6482 , train acc : 0.6462333825701625\n",
      "epoch 200 ,Accuracy: 0.6630 , train acc : 0.6558345642540621\n",
      "epoch 250 ,Accuracy: 0.6642 , train acc : 0.6587887740029542\n",
      "epoch 300 ,Accuracy: 0.6544 , train acc : 0.6573116691285081\n",
      "epoch 350 ,Accuracy: 0.7085 , train acc : 0.7223042836041359\n",
      "epoch 400 ,Accuracy: 0.7085 , train acc : 0.7259970457902511\n",
      "epoch 450 ,Accuracy: 0.7085 , train acc : 0.7245199409158051\n",
      "epoch 500 ,Accuracy: 0.7109 , train acc : 0.7311669128508124\n",
      "epoch 550 ,Accuracy: 0.7540 , train acc : 0.7843426883308715\n",
      "epoch 600 ,Accuracy: 0.7589 , train acc : 0.7902511078286558\n",
      "epoch 650 ,Accuracy: 0.7589 , train acc : 0.7917282127031019\n",
      "epoch 700 ,Accuracy: 0.7601 , train acc : 0.7887740029542097\n",
      "epoch 750 ,Accuracy: 0.7577 , train acc : 0.793205317577548\n",
      "epoch 800 ,Accuracy: 0.7515 , train acc : 0.7991137370753324\n",
      "epoch 850 ,Accuracy: 0.7515 , train acc : 0.7961595273264401\n",
      "epoch 900 ,Accuracy: 0.7565 , train acc : 0.7954209748892171\n",
      "epoch 950 ,Accuracy: 0.7589 , train acc : 0.7976366322008862\n",
      "epoch 1000 ,Accuracy: 0.7577 , train acc : 0.8028064992614475\n",
      "epoch 1050 ,Accuracy: 0.7491 , train acc : 0.8057607090103397\n",
      "epoch 1100 ,Accuracy: 0.7614 , train acc : 0.8079763663220089\n",
      "epoch 1150 ,Accuracy: 0.7552 , train acc : 0.8028064992614475\n",
      "epoch 1200 ,Accuracy: 0.7528 , train acc : 0.8116691285081241\n",
      "epoch 1250 ,Accuracy: 0.7651 , train acc : 0.8079763663220089\n",
      "epoch 1300 ,Accuracy: 0.7577 , train acc : 0.810192023633678\n",
      "epoch 1350 ,Accuracy: 0.7552 , train acc : 0.8072378138847858\n",
      "epoch 0 ,Accuracy: 0.1021 , train acc : 0.09010339734121123\n",
      "epoch 50 ,Accuracy: 0.3333 , train acc : 0.3759231905465288\n",
      "epoch 100 ,Accuracy: 0.3346 , train acc : 0.38183161004431315\n",
      "epoch 150 ,Accuracy: 0.3948 , train acc : 0.44977843426883307\n",
      "epoch 200 ,Accuracy: 0.6630 , train acc : 0.7274741506646972\n",
      "epoch 250 ,Accuracy: 0.7122 , train acc : 0.7850812407680945\n",
      "epoch 300 ,Accuracy: 0.7060 , train acc : 0.7895125553914328\n",
      "epoch 350 ,Accuracy: 0.7196 , train acc : 0.7872968980797637\n",
      "epoch 400 ,Accuracy: 0.7085 , train acc : 0.7939438700147711\n",
      "epoch 450 ,Accuracy: 0.7220 , train acc : 0.7983751846381093\n",
      "epoch 500 ,Accuracy: 0.7171 , train acc : 0.8005908419497785\n",
      "epoch 550 ,Accuracy: 0.7269 , train acc : 0.7968980797636632\n",
      "epoch 600 ,Accuracy: 0.7171 , train acc : 0.7976366322008862\n",
      "epoch 650 ,Accuracy: 0.7122 , train acc : 0.7983751846381093\n",
      "epoch 700 ,Accuracy: 0.7159 , train acc : 0.8028064992614475\n",
      "epoch 750 ,Accuracy: 0.7196 , train acc : 0.8050221565731167\n",
      "epoch 800 ,Accuracy: 0.7171 , train acc : 0.8094534711964549\n",
      "epoch 850 ,Accuracy: 0.7208 , train acc : 0.8042836041358936\n",
      "epoch 900 ,Accuracy: 0.7171 , train acc : 0.8079763663220089\n",
      "epoch 950 ,Accuracy: 0.7196 , train acc : 0.8116691285081241\n",
      "epoch 1000 ,Accuracy: 0.7220 , train acc : 0.8116691285081241\n",
      "epoch 1050 ,Accuracy: 0.7159 , train acc : 0.8131462333825702\n",
      "epoch 1100 ,Accuracy: 0.7171 , train acc : 0.8138847858197932\n",
      "epoch 1150 ,Accuracy: 0.7159 , train acc : 0.8146233382570163\n",
      "epoch 1200 ,Accuracy: 0.7060 , train acc : 0.8168389955686853\n",
      "epoch 1250 ,Accuracy: 0.7134 , train acc : 0.8227474150664698\n",
      "epoch 1300 ,Accuracy: 0.7159 , train acc : 0.8190546528803545\n",
      "epoch 1350 ,Accuracy: 0.7159 , train acc : 0.8212703101920237\n",
      "epoch 0 ,Accuracy: 0.3223 , train acc : 0.310192023633678\n",
      "epoch 50 ,Accuracy: 0.6814 , train acc : 0.6986706056129985\n",
      "epoch 100 ,Accuracy: 0.7909 , train acc : 0.8094534711964549\n",
      "epoch 150 ,Accuracy: 0.8290 , train acc : 0.8567208271787297\n",
      "epoch 200 ,Accuracy: 0.8216 , train acc : 0.8559822747415067\n",
      "epoch 250 ,Accuracy: 0.8192 , train acc : 0.8626292466765141\n",
      "epoch 300 ,Accuracy: 0.8266 , train acc : 0.8633677991137371\n",
      "epoch 350 ,Accuracy: 0.8253 , train acc : 0.8633677991137371\n",
      "epoch 400 ,Accuracy: 0.8315 , train acc : 0.8670605612998523\n",
      "epoch 450 ,Accuracy: 0.8241 , train acc : 0.8692762186115214\n",
      "epoch 500 ,Accuracy: 0.8241 , train acc : 0.8685376661742984\n",
      "epoch 550 ,Accuracy: 0.8253 , train acc : 0.8670605612998523\n",
      "epoch 600 ,Accuracy: 0.8278 , train acc : 0.8766617429837519\n",
      "epoch 650 ,Accuracy: 0.8266 , train acc : 0.8722304283604135\n",
      "epoch 700 ,Accuracy: 0.8253 , train acc : 0.878138847858198\n",
      "epoch 750 ,Accuracy: 0.8745 , train acc : 0.9409158050221565\n",
      "epoch 800 ,Accuracy: 0.8770 , train acc : 0.947562776957164\n",
      "epoch 850 ,Accuracy: 0.8745 , train acc : 0.9460856720827179\n",
      "epoch 900 ,Accuracy: 0.8733 , train acc : 0.9519940915805022\n",
      "epoch 950 ,Accuracy: 0.8745 , train acc : 0.947562776957164\n",
      "epoch 1000 ,Accuracy: 0.8807 , train acc : 0.9601181683899557\n",
      "epoch 1050 ,Accuracy: 0.8795 , train acc : 0.9579025110782866\n",
      "epoch 1100 ,Accuracy: 0.8795 , train acc : 0.9549483013293943\n",
      "epoch 1150 ,Accuracy: 0.8770 , train acc : 0.9579025110782866\n",
      "epoch 1200 ,Accuracy: 0.8831 , train acc : 0.9593796159527327\n",
      "epoch 1250 ,Accuracy: 0.8782 , train acc : 0.9645494830132939\n",
      "epoch 1300 ,Accuracy: 0.8831 , train acc : 0.9675036927621861\n",
      "epoch 1350 ,Accuracy: 0.8831 , train acc : 0.96602658788774\n",
      "Accuracy: 0.8745\n",
      "base model\n",
      "0.8708487084870848\n",
      "0.8484922326167839\n",
      "OurModel\n",
      "0.8745387453874539\n",
      "0.8571012559500651\n",
      "*****\n",
      "epoch : 2 / 10\n",
      "epoch : 50 = >  Accuracy: 0.3247\n",
      "epoch : 100 = >  Accuracy: 0.4305\n",
      "epoch : 150 = >  Accuracy: 0.4034\n",
      "epoch : 200 = >  Accuracy: 0.3801\n",
      "epoch : 250 = >  Accuracy: 0.3665\n",
      "epoch : 300 = >  Accuracy: 0.3764\n",
      "epoch : 350 = >  Accuracy: 0.4526\n",
      "epoch : 400 = >  Accuracy: 0.5572\n",
      "epoch : 450 = >  Accuracy: 0.6298\n",
      "epoch : 500 = >  Accuracy: 0.7023\n",
      "epoch : 550 = >  Accuracy: 0.7565\n",
      "epoch : 600 = >  Accuracy: 0.8032\n",
      "epoch : 650 = >  Accuracy: 0.8303\n",
      "epoch : 700 = >  Accuracy: 0.8462\n",
      "epoch : 750 = >  Accuracy: 0.8598\n",
      "epoch : 800 = >  Accuracy: 0.8659\n",
      "epoch : 850 = >  Accuracy: 0.8684\n",
      "epoch : 900 = >  Accuracy: 0.8696\n",
      "Final accuracy: 0.8696\n",
      "epoch 0 ,Accuracy: 0.3087 , train acc : 0.30576070901033975\n",
      "epoch 50 ,Accuracy: 0.5830 , train acc : 0.6536189069423929\n",
      "epoch 100 ,Accuracy: 0.7097 , train acc : 0.7806499261447563\n",
      "epoch 150 ,Accuracy: 0.7196 , train acc : 0.7828655834564254\n",
      "epoch 200 ,Accuracy: 0.7208 , train acc : 0.7887740029542097\n",
      "epoch 250 ,Accuracy: 0.7171 , train acc : 0.7909896602658789\n",
      "epoch 300 ,Accuracy: 0.7146 , train acc : 0.7850812407680945\n",
      "epoch 350 ,Accuracy: 0.7159 , train acc : 0.793205317577548\n",
      "epoch 400 ,Accuracy: 0.7208 , train acc : 0.794682422451994\n",
      "epoch 450 ,Accuracy: 0.7146 , train acc : 0.7976366322008862\n",
      "epoch 500 ,Accuracy: 0.8782 , train acc : 0.9276218611521418\n",
      "epoch 550 ,Accuracy: 0.8782 , train acc : 0.9335302806499262\n",
      "epoch 600 ,Accuracy: 0.8967 , train acc : 0.9327917282127031\n",
      "epoch 650 ,Accuracy: 0.8856 , train acc : 0.9394387001477105\n",
      "epoch 700 ,Accuracy: 0.8795 , train acc : 0.9379615952732644\n",
      "epoch 750 ,Accuracy: 0.8856 , train acc : 0.9409158050221565\n",
      "epoch 800 ,Accuracy: 0.8770 , train acc : 0.9401772525849336\n",
      "epoch 850 ,Accuracy: 0.8819 , train acc : 0.9446085672082718\n",
      "epoch 900 ,Accuracy: 0.8844 , train acc : 0.948301329394387\n",
      "epoch 950 ,Accuracy: 0.8795 , train acc : 0.94903988183161\n",
      "epoch 1000 ,Accuracy: 0.8856 , train acc : 0.9505169867060561\n",
      "epoch 1050 ,Accuracy: 0.8831 , train acc : 0.9519940915805022\n",
      "epoch 1100 ,Accuracy: 0.8893 , train acc : 0.9527326440177253\n",
      "epoch 1150 ,Accuracy: 0.8782 , train acc : 0.9519940915805022\n",
      "epoch 1200 ,Accuracy: 0.8819 , train acc : 0.9549483013293943\n",
      "epoch 1250 ,Accuracy: 0.8708 , train acc : 0.9564254062038404\n",
      "epoch 1300 ,Accuracy: 0.8708 , train acc : 0.9623338257016248\n",
      "epoch 1350 ,Accuracy: 0.8782 , train acc : 0.9615952732644018\n",
      "Accuracy: 0.8795\n",
      "base model\n",
      "0.8696186961869619\n",
      "0.8471841265516293\n",
      "OurModel\n",
      "0.8794587945879458\n",
      "0.8647054304483993\n",
      "*****\n",
      "epoch : 3 / 10\n",
      "epoch : 50 = >  Accuracy: 0.3272\n",
      "epoch : 100 = >  Accuracy: 0.4317\n",
      "epoch : 150 = >  Accuracy: 0.4059\n",
      "epoch : 200 = >  Accuracy: 0.3887\n",
      "epoch : 250 = >  Accuracy: 0.3776\n",
      "epoch : 300 = >  Accuracy: 0.3825\n",
      "epoch : 350 = >  Accuracy: 0.4416\n",
      "epoch : 400 = >  Accuracy: 0.5535\n",
      "epoch : 450 = >  Accuracy: 0.6236\n",
      "epoch : 500 = >  Accuracy: 0.7023\n",
      "epoch : 550 = >  Accuracy: 0.7577\n",
      "epoch : 600 = >  Accuracy: 0.8020\n",
      "epoch : 650 = >  Accuracy: 0.8315\n",
      "epoch : 700 = >  Accuracy: 0.8475\n",
      "epoch : 750 = >  Accuracy: 0.8610\n",
      "epoch : 800 = >  Accuracy: 0.8684\n",
      "epoch : 850 = >  Accuracy: 0.8684\n",
      "epoch : 900 = >  Accuracy: 0.8721\n",
      "Final accuracy: 0.8721\n",
      "epoch 0 ,Accuracy: 0.3296 , train acc : 0.3175775480059084\n",
      "epoch 50 ,Accuracy: 0.7097 , train acc : 0.7090103397341211\n",
      "epoch 100 ,Accuracy: 0.7122 , train acc : 0.7163958641063516\n",
      "epoch 150 ,Accuracy: 0.7122 , train acc : 0.7163958641063516\n",
      "epoch 200 ,Accuracy: 0.7109 , train acc : 0.7223042836041359\n",
      "epoch 250 ,Accuracy: 0.7134 , train acc : 0.7200886262924667\n",
      "epoch 300 ,Accuracy: 0.7073 , train acc : 0.725258493353028\n",
      "epoch 350 ,Accuracy: 0.7134 , train acc : 0.7274741506646972\n",
      "epoch 400 ,Accuracy: 0.8339 , train acc : 0.8648449039881831\n",
      "epoch 450 ,Accuracy: 0.8758 , train acc : 0.9268833087149188\n",
      "epoch 500 ,Accuracy: 0.8721 , train acc : 0.9327917282127031\n",
      "epoch 550 ,Accuracy: 0.8844 , train acc : 0.9372230428360414\n",
      "epoch 600 ,Accuracy: 0.8770 , train acc : 0.9364844903988183\n",
      "epoch 650 ,Accuracy: 0.8819 , train acc : 0.9387001477104875\n",
      "epoch 700 ,Accuracy: 0.8782 , train acc : 0.9423929098966026\n",
      "epoch 750 ,Accuracy: 0.8844 , train acc : 0.9423929098966026\n",
      "epoch 800 ,Accuracy: 0.8844 , train acc : 0.946824224519941\n",
      "epoch 850 ,Accuracy: 0.8795 , train acc : 0.9453471196454948\n",
      "epoch 900 ,Accuracy: 0.8819 , train acc : 0.9446085672082718\n",
      "epoch 950 ,Accuracy: 0.8856 , train acc : 0.94903988183161\n",
      "epoch 1000 ,Accuracy: 0.8819 , train acc : 0.9519940915805022\n",
      "epoch 1050 ,Accuracy: 0.8819 , train acc : 0.9549483013293943\n",
      "epoch 1100 ,Accuracy: 0.8807 , train acc : 0.9549483013293943\n",
      "epoch 1150 ,Accuracy: 0.8795 , train acc : 0.9608567208271788\n",
      "epoch 1200 ,Accuracy: 0.8881 , train acc : 0.9527326440177253\n",
      "epoch 1250 ,Accuracy: 0.8782 , train acc : 0.9667651403249631\n",
      "epoch 1300 ,Accuracy: 0.8831 , train acc : 0.9623338257016248\n",
      "epoch 1350 ,Accuracy: 0.8844 , train acc : 0.9689807976366323\n",
      "Accuracy: 0.8782\n",
      "base model\n",
      "0.8720787207872078\n",
      "0.8487560580774399\n",
      "OurModel\n",
      "0.8782287822878229\n",
      "0.866233010222029\n",
      "*****\n",
      "epoch : 4 / 10\n",
      "epoch : 50 = >  Accuracy: 0.3296\n",
      "epoch : 100 = >  Accuracy: 0.4293\n",
      "epoch : 150 = >  Accuracy: 0.4034\n",
      "epoch : 200 = >  Accuracy: 0.3801\n",
      "epoch : 250 = >  Accuracy: 0.3702\n",
      "epoch : 300 = >  Accuracy: 0.3776\n",
      "epoch : 350 = >  Accuracy: 0.4440\n",
      "epoch : 400 = >  Accuracy: 0.5560\n",
      "epoch : 450 = >  Accuracy: 0.6285\n",
      "epoch : 500 = >  Accuracy: 0.6986\n",
      "epoch : 550 = >  Accuracy: 0.7540\n",
      "epoch : 600 = >  Accuracy: 0.8007\n",
      "epoch : 650 = >  Accuracy: 0.8290\n",
      "epoch : 700 = >  Accuracy: 0.8450\n",
      "epoch : 750 = >  Accuracy: 0.8598\n",
      "epoch : 800 = >  Accuracy: 0.8684\n",
      "epoch : 850 = >  Accuracy: 0.8684\n",
      "epoch : 900 = >  Accuracy: 0.8721\n",
      "Final accuracy: 0.8721\n",
      "epoch 0 ,Accuracy: 0.1611 , train acc : 0.19202363367799113\n",
      "epoch 50 ,Accuracy: 0.7060 , train acc : 0.6846381093057607\n",
      "epoch 100 ,Accuracy: 0.7761 , train acc : 0.793205317577548\n",
      "epoch 150 ,Accuracy: 0.8180 , train acc : 0.8545051698670606\n",
      "epoch 200 ,Accuracy: 0.8364 , train acc : 0.8574593796159528\n",
      "epoch 250 ,Accuracy: 0.8426 , train acc : 0.8663220088626292\n",
      "epoch 300 ,Accuracy: 0.8339 , train acc : 0.8633677991137371\n",
      "epoch 350 ,Accuracy: 0.8758 , train acc : 0.9290989660265879\n",
      "epoch 400 ,Accuracy: 0.8856 , train acc : 0.9268833087149188\n",
      "epoch 450 ,Accuracy: 0.8795 , train acc : 0.9409158050221565\n",
      "epoch 500 ,Accuracy: 0.8819 , train acc : 0.9335302806499262\n",
      "epoch 550 ,Accuracy: 0.8782 , train acc : 0.930576070901034\n",
      "epoch 600 ,Accuracy: 0.8745 , train acc : 0.9387001477104875\n",
      "epoch 650 ,Accuracy: 0.8758 , train acc : 0.9379615952732644\n",
      "epoch 700 ,Accuracy: 0.8795 , train acc : 0.9394387001477105\n",
      "epoch 750 ,Accuracy: 0.8868 , train acc : 0.9409158050221565\n",
      "epoch 800 ,Accuracy: 0.8782 , train acc : 0.9460856720827179\n",
      "epoch 850 ,Accuracy: 0.8807 , train acc : 0.9460856720827179\n",
      "epoch 900 ,Accuracy: 0.8856 , train acc : 0.946824224519941\n",
      "epoch 950 ,Accuracy: 0.8758 , train acc : 0.9512555391432792\n",
      "epoch 1000 ,Accuracy: 0.8721 , train acc : 0.9534711964549483\n",
      "epoch 1050 ,Accuracy: 0.8807 , train acc : 0.948301329394387\n",
      "epoch 1100 ,Accuracy: 0.8831 , train acc : 0.9534711964549483\n",
      "epoch 1150 ,Accuracy: 0.8856 , train acc : 0.9556868537666174\n",
      "epoch 1200 ,Accuracy: 0.8795 , train acc : 0.9601181683899557\n",
      "epoch 1250 ,Accuracy: 0.8782 , train acc : 0.9556868537666174\n",
      "epoch 1300 ,Accuracy: 0.8758 , train acc : 0.9564254062038404\n",
      "epoch 1350 ,Accuracy: 0.8782 , train acc : 0.9630723781388478\n",
      "Accuracy: 0.8721\n",
      "base model\n",
      "0.8720787207872078\n",
      "0.8504650464007\n",
      "OurModel\n",
      "0.8720787207872078\n",
      "0.8495462636768278\n",
      "*****\n",
      "epoch : 5 / 10\n",
      "epoch : 50 = >  Accuracy: 0.3260\n",
      "epoch : 100 = >  Accuracy: 0.4293\n",
      "epoch : 150 = >  Accuracy: 0.4034\n",
      "epoch : 200 = >  Accuracy: 0.3788\n",
      "epoch : 250 = >  Accuracy: 0.3616\n",
      "epoch : 300 = >  Accuracy: 0.3752\n",
      "epoch : 350 = >  Accuracy: 0.4490\n",
      "epoch : 400 = >  Accuracy: 0.5560\n",
      "epoch : 450 = >  Accuracy: 0.6298\n",
      "epoch : 500 = >  Accuracy: 0.7023\n",
      "epoch : 550 = >  Accuracy: 0.7552\n",
      "epoch : 600 = >  Accuracy: 0.8032\n",
      "epoch : 650 = >  Accuracy: 0.8290\n",
      "epoch : 700 = >  Accuracy: 0.8450\n",
      "epoch : 750 = >  Accuracy: 0.8598\n",
      "epoch : 800 = >  Accuracy: 0.8696\n",
      "epoch : 850 = >  Accuracy: 0.8708\n",
      "epoch : 900 = >  Accuracy: 0.8696\n",
      "Final accuracy: 0.8696\n",
      "epoch 0 ,Accuracy: 0.2153 , train acc : 0.22304283604135894\n",
      "epoch 50 ,Accuracy: 0.6839 , train acc : 0.6787296898079763\n",
      "epoch 100 ,Accuracy: 0.7109 , train acc : 0.6957163958641064\n",
      "epoch 150 ,Accuracy: 0.7159 , train acc : 0.6927621861152142\n",
      "epoch 200 ,Accuracy: 0.7134 , train acc : 0.6920236336779911\n",
      "epoch 250 ,Accuracy: 0.7097 , train acc : 0.6979320531757754\n",
      "epoch 300 ,Accuracy: 0.7183 , train acc : 0.7016248153618907\n",
      "epoch 350 ,Accuracy: 0.7196 , train acc : 0.7001477104874446\n",
      "epoch 400 ,Accuracy: 0.7159 , train acc : 0.7031019202363368\n",
      "epoch 450 ,Accuracy: 0.7085 , train acc : 0.7031019202363368\n",
      "epoch 500 ,Accuracy: 0.7109 , train acc : 0.7023633677991138\n",
      "epoch 550 ,Accuracy: 0.7073 , train acc : 0.6986706056129985\n",
      "epoch 600 ,Accuracy: 0.7097 , train acc : 0.7023633677991138\n",
      "epoch 650 ,Accuracy: 0.7134 , train acc : 0.705317577548006\n",
      "epoch 700 ,Accuracy: 0.7060 , train acc : 0.7031019202363368\n",
      "epoch 750 ,Accuracy: 0.7097 , train acc : 0.705317577548006\n",
      "epoch 800 ,Accuracy: 0.7122 , train acc : 0.7060561299852289\n",
      "epoch 850 ,Accuracy: 0.7109 , train acc : 0.7023633677991138\n",
      "epoch 900 ,Accuracy: 0.7122 , train acc : 0.7060561299852289\n",
      "epoch 950 ,Accuracy: 0.7036 , train acc : 0.7060561299852289\n",
      "epoch 1000 ,Accuracy: 0.7085 , train acc : 0.7082717872968981\n",
      "epoch 1050 ,Accuracy: 0.7134 , train acc : 0.705317577548006\n",
      "epoch 1100 ,Accuracy: 0.7085 , train acc : 0.7119645494830132\n",
      "epoch 1150 ,Accuracy: 0.7097 , train acc : 0.706794682422452\n",
      "epoch 1200 ,Accuracy: 0.7109 , train acc : 0.7097488921713442\n",
      "epoch 1250 ,Accuracy: 0.7097 , train acc : 0.7112259970457903\n",
      "epoch 1300 ,Accuracy: 0.7060 , train acc : 0.7112259970457903\n",
      "epoch 1350 ,Accuracy: 0.7097 , train acc : 0.7090103397341211\n",
      "epoch 0 ,Accuracy: 0.1513 , train acc : 0.1757754800590842\n",
      "epoch 50 ,Accuracy: 0.6839 , train acc : 0.6964549483013294\n",
      "epoch 100 ,Accuracy: 0.8118 , train acc : 0.8545051698670606\n",
      "epoch 150 ,Accuracy: 0.8659 , train acc : 0.9106351550960118\n",
      "epoch 200 ,Accuracy: 0.8758 , train acc : 0.9150664697193501\n",
      "epoch 250 ,Accuracy: 0.8782 , train acc : 0.9180206794682423\n",
      "epoch 300 ,Accuracy: 0.8758 , train acc : 0.9239290989660266\n",
      "epoch 350 ,Accuracy: 0.8831 , train acc : 0.9246676514032496\n",
      "epoch 400 ,Accuracy: 0.8807 , train acc : 0.930576070901034\n",
      "epoch 450 ,Accuracy: 0.8893 , train acc : 0.9320531757754801\n",
      "epoch 500 ,Accuracy: 0.8782 , train acc : 0.9261447562776958\n",
      "epoch 550 ,Accuracy: 0.8905 , train acc : 0.9335302806499262\n",
      "epoch 600 ,Accuracy: 0.8758 , train acc : 0.9350073855243722\n",
      "epoch 650 ,Accuracy: 0.8844 , train acc : 0.9364844903988183\n",
      "epoch 700 ,Accuracy: 0.8856 , train acc : 0.9379615952732644\n",
      "epoch 750 ,Accuracy: 0.8819 , train acc : 0.9387001477104875\n",
      "epoch 800 ,Accuracy: 0.8856 , train acc : 0.9453471196454948\n",
      "epoch 850 ,Accuracy: 0.8819 , train acc : 0.9401772525849336\n",
      "epoch 900 ,Accuracy: 0.8807 , train acc : 0.94903988183161\n",
      "epoch 950 ,Accuracy: 0.8770 , train acc : 0.9527326440177253\n",
      "epoch 1000 ,Accuracy: 0.8807 , train acc : 0.9497784342688331\n",
      "epoch 1050 ,Accuracy: 0.8844 , train acc : 0.9519940915805022\n",
      "epoch 1100 ,Accuracy: 0.8807 , train acc : 0.9534711964549483\n",
      "epoch 1150 ,Accuracy: 0.8745 , train acc : 0.9556868537666174\n",
      "epoch 1200 ,Accuracy: 0.8770 , train acc : 0.9586410635155096\n",
      "epoch 1250 ,Accuracy: 0.8807 , train acc : 0.9623338257016248\n",
      "epoch 1300 ,Accuracy: 0.8831 , train acc : 0.9601181683899557\n",
      "epoch 1350 ,Accuracy: 0.8782 , train acc : 0.9586410635155096\n",
      "Accuracy: 0.8782\n",
      "base model\n",
      "0.8696186961869619\n",
      "0.8471841265516293\n",
      "OurModel\n",
      "0.8782287822878229\n",
      "0.8633165063654019\n",
      "*****\n",
      "epoch : 6 / 10\n",
      "epoch : 50 = >  Accuracy: 0.3198\n",
      "epoch : 100 = >  Accuracy: 0.4280\n",
      "epoch : 150 = >  Accuracy: 0.4059\n",
      "epoch : 200 = >  Accuracy: 0.3862\n",
      "epoch : 250 = >  Accuracy: 0.3764\n",
      "epoch : 300 = >  Accuracy: 0.3825\n",
      "epoch : 350 = >  Accuracy: 0.4403\n",
      "epoch : 400 = >  Accuracy: 0.5535\n",
      "epoch : 450 = >  Accuracy: 0.6236\n",
      "epoch : 500 = >  Accuracy: 0.6986\n",
      "epoch : 550 = >  Accuracy: 0.7552\n",
      "epoch : 600 = >  Accuracy: 0.8007\n",
      "epoch : 650 = >  Accuracy: 0.8303\n",
      "epoch : 700 = >  Accuracy: 0.8487\n",
      "epoch : 750 = >  Accuracy: 0.8622\n",
      "epoch : 800 = >  Accuracy: 0.8684\n",
      "epoch : 850 = >  Accuracy: 0.8696\n",
      "epoch : 900 = >  Accuracy: 0.8721\n",
      "Final accuracy: 0.8721\n",
      "epoch 0 ,Accuracy: 0.3063 , train acc : 0.3072378138847858\n",
      "epoch 50 ,Accuracy: 0.7368 , train acc : 0.7769571639586411\n",
      "epoch 100 ,Accuracy: 0.7749 , train acc : 0.793205317577548\n",
      "epoch 150 ,Accuracy: 0.7823 , train acc : 0.7998522895125554\n",
      "epoch 200 ,Accuracy: 0.8364 , train acc : 0.8596750369276218\n",
      "epoch 250 ,Accuracy: 0.8327 , train acc : 0.8670605612998523\n",
      "epoch 300 ,Accuracy: 0.8278 , train acc : 0.8729689807976366\n",
      "epoch 350 ,Accuracy: 0.8352 , train acc : 0.8722304283604135\n",
      "epoch 400 ,Accuracy: 0.8339 , train acc : 0.8692762186115214\n",
      "epoch 450 ,Accuracy: 0.8376 , train acc : 0.8744460856720827\n",
      "epoch 500 ,Accuracy: 0.8389 , train acc : 0.8722304283604135\n",
      "epoch 550 ,Accuracy: 0.8339 , train acc : 0.8744460856720827\n",
      "epoch 600 ,Accuracy: 0.8339 , train acc : 0.8810930576070901\n",
      "epoch 650 ,Accuracy: 0.8327 , train acc : 0.8825701624815362\n",
      "epoch 700 ,Accuracy: 0.8327 , train acc : 0.8788774002954209\n",
      "epoch 750 ,Accuracy: 0.8303 , train acc : 0.8833087149187593\n",
      "epoch 800 ,Accuracy: 0.8241 , train acc : 0.8833087149187593\n",
      "epoch 850 ,Accuracy: 0.8290 , train acc : 0.8810930576070901\n",
      "epoch 900 ,Accuracy: 0.8339 , train acc : 0.880354505169867\n",
      "epoch 950 ,Accuracy: 0.8253 , train acc : 0.8818316100443131\n",
      "epoch 1000 ,Accuracy: 0.8339 , train acc : 0.8870014771048744\n",
      "epoch 1050 ,Accuracy: 0.8290 , train acc : 0.8906942392909897\n",
      "epoch 1100 ,Accuracy: 0.8303 , train acc : 0.8929098966026587\n",
      "epoch 1150 ,Accuracy: 0.8266 , train acc : 0.8951255539143279\n",
      "epoch 1200 ,Accuracy: 0.8339 , train acc : 0.8914327917282127\n",
      "epoch 1250 ,Accuracy: 0.8339 , train acc : 0.8980797636632201\n",
      "epoch 1300 ,Accuracy: 0.8241 , train acc : 0.8973412112259971\n",
      "epoch 1350 ,Accuracy: 0.8315 , train acc : 0.8980797636632201\n",
      "epoch 0 ,Accuracy: 0.3100 , train acc : 0.310930576070901\n",
      "epoch 50 ,Accuracy: 0.7159 , train acc : 0.7555391432791728\n",
      "epoch 100 ,Accuracy: 0.7454 , train acc : 0.7769571639586411\n",
      "epoch 150 ,Accuracy: 0.7675 , train acc : 0.8079763663220089\n",
      "epoch 200 ,Accuracy: 0.8352 , train acc : 0.8581979320531757\n",
      "epoch 250 ,Accuracy: 0.8216 , train acc : 0.8581979320531757\n",
      "epoch 300 ,Accuracy: 0.8253 , train acc : 0.8596750369276218\n",
      "epoch 350 ,Accuracy: 0.8327 , train acc : 0.8648449039881831\n",
      "epoch 400 ,Accuracy: 0.8278 , train acc : 0.8581979320531757\n",
      "epoch 450 ,Accuracy: 0.8303 , train acc : 0.8692762186115214\n",
      "epoch 500 ,Accuracy: 0.8229 , train acc : 0.8663220088626292\n",
      "epoch 550 ,Accuracy: 0.8266 , train acc : 0.8663220088626292\n",
      "epoch 600 ,Accuracy: 0.8266 , train acc : 0.8714918759231906\n",
      "epoch 650 ,Accuracy: 0.8229 , train acc : 0.8729689807976366\n",
      "epoch 700 ,Accuracy: 0.8229 , train acc : 0.8729689807976366\n",
      "epoch 750 ,Accuracy: 0.8253 , train acc : 0.8737075332348597\n",
      "epoch 800 ,Accuracy: 0.8204 , train acc : 0.8759231905465288\n",
      "epoch 850 ,Accuracy: 0.8721 , train acc : 0.9372230428360414\n",
      "epoch 900 ,Accuracy: 0.8795 , train acc : 0.9438700147710487\n",
      "epoch 950 ,Accuracy: 0.8733 , train acc : 0.9416543574593796\n",
      "epoch 1000 ,Accuracy: 0.8733 , train acc : 0.946824224519941\n",
      "epoch 1050 ,Accuracy: 0.8819 , train acc : 0.9512555391432792\n",
      "epoch 1100 ,Accuracy: 0.8745 , train acc : 0.9519940915805022\n",
      "epoch 1150 ,Accuracy: 0.8782 , train acc : 0.9556868537666174\n",
      "epoch 1200 ,Accuracy: 0.8708 , train acc : 0.9564254062038404\n",
      "epoch 1250 ,Accuracy: 0.8770 , train acc : 0.9579025110782866\n",
      "epoch 1300 ,Accuracy: 0.8721 , train acc : 0.9571639586410635\n",
      "epoch 1350 ,Accuracy: 0.8721 , train acc : 0.9630723781388478\n",
      "Accuracy: 0.8684\n",
      "base model\n",
      "0.8720787207872078\n",
      "0.8499079057854706\n",
      "OurModel\n",
      "0.8683886838868389\n",
      "0.8492787643787194\n",
      "*****\n",
      "epoch : 7 / 10\n",
      "epoch : 50 = >  Accuracy: 0.3210\n",
      "epoch : 100 = >  Accuracy: 0.4330\n",
      "epoch : 150 = >  Accuracy: 0.4059\n",
      "epoch : 200 = >  Accuracy: 0.3887\n",
      "epoch : 250 = >  Accuracy: 0.3764\n",
      "epoch : 300 = >  Accuracy: 0.3825\n",
      "epoch : 350 = >  Accuracy: 0.4453\n",
      "epoch : 400 = >  Accuracy: 0.5547\n",
      "epoch : 450 = >  Accuracy: 0.6261\n",
      "epoch : 500 = >  Accuracy: 0.7048\n",
      "epoch : 550 = >  Accuracy: 0.7601\n",
      "epoch : 600 = >  Accuracy: 0.8057\n",
      "epoch : 650 = >  Accuracy: 0.8352\n",
      "epoch : 700 = >  Accuracy: 0.8487\n",
      "epoch : 750 = >  Accuracy: 0.8622\n",
      "epoch : 800 = >  Accuracy: 0.8696\n",
      "epoch : 850 = >  Accuracy: 0.8672\n",
      "epoch : 900 = >  Accuracy: 0.8721\n",
      "Final accuracy: 0.8721\n",
      "epoch 0 ,Accuracy: 0.3161 , train acc : 0.30797636632200887\n",
      "epoch 50 ,Accuracy: 0.8106 , train acc : 0.8220088626292467\n",
      "epoch 100 ,Accuracy: 0.8229 , train acc : 0.8530280649926145\n",
      "epoch 150 ,Accuracy: 0.8782 , train acc : 0.9076809453471196\n",
      "epoch 200 ,Accuracy: 0.8782 , train acc : 0.9135893648449039\n",
      "epoch 250 ,Accuracy: 0.8795 , train acc : 0.9209748892171344\n",
      "epoch 300 ,Accuracy: 0.8782 , train acc : 0.9224519940915805\n",
      "epoch 350 ,Accuracy: 0.8831 , train acc : 0.9246676514032496\n",
      "epoch 400 ,Accuracy: 0.8770 , train acc : 0.9298375184638109\n",
      "epoch 450 ,Accuracy: 0.8696 , train acc : 0.9335302806499262\n",
      "epoch 500 ,Accuracy: 0.8844 , train acc : 0.9350073855243722\n",
      "epoch 550 ,Accuracy: 0.8819 , train acc : 0.9379615952732644\n",
      "epoch 600 ,Accuracy: 0.8758 , train acc : 0.9364844903988183\n",
      "epoch 650 ,Accuracy: 0.8684 , train acc : 0.947562776957164\n",
      "epoch 700 ,Accuracy: 0.8770 , train acc : 0.9416543574593796\n",
      "epoch 750 ,Accuracy: 0.8868 , train acc : 0.9438700147710487\n",
      "epoch 800 ,Accuracy: 0.8868 , train acc : 0.9446085672082718\n",
      "epoch 850 ,Accuracy: 0.8831 , train acc : 0.948301329394387\n",
      "epoch 900 ,Accuracy: 0.8758 , train acc : 0.9519940915805022\n",
      "epoch 950 ,Accuracy: 0.8819 , train acc : 0.9497784342688331\n",
      "epoch 1000 ,Accuracy: 0.8758 , train acc : 0.9542097488921714\n",
      "epoch 1050 ,Accuracy: 0.8831 , train acc : 0.9564254062038404\n",
      "epoch 1100 ,Accuracy: 0.8881 , train acc : 0.9542097488921714\n",
      "epoch 1150 ,Accuracy: 0.8721 , train acc : 0.9601181683899557\n",
      "epoch 1200 ,Accuracy: 0.8819 , train acc : 0.9579025110782866\n",
      "epoch 1250 ,Accuracy: 0.8696 , train acc : 0.9593796159527327\n",
      "epoch 1300 ,Accuracy: 0.8770 , train acc : 0.9667651403249631\n",
      "epoch 1350 ,Accuracy: 0.8684 , train acc : 0.96602658788774\n",
      "Accuracy: 0.8782\n",
      "base model\n",
      "0.8720787207872078\n",
      "0.8487560580774399\n",
      "OurModel\n",
      "0.8782287822878229\n",
      "0.862233206722319\n",
      "*****\n",
      "epoch : 8 / 10\n",
      "epoch : 50 = >  Accuracy: 0.3186\n",
      "epoch : 100 = >  Accuracy: 0.4330\n",
      "epoch : 150 = >  Accuracy: 0.4059\n",
      "epoch : 200 = >  Accuracy: 0.3862\n",
      "epoch : 250 = >  Accuracy: 0.3764\n",
      "epoch : 300 = >  Accuracy: 0.3825\n",
      "epoch : 350 = >  Accuracy: 0.4440\n",
      "epoch : 400 = >  Accuracy: 0.5572\n",
      "epoch : 450 = >  Accuracy: 0.6273\n",
      "epoch : 500 = >  Accuracy: 0.6999\n",
      "epoch : 550 = >  Accuracy: 0.7577\n",
      "epoch : 600 = >  Accuracy: 0.8032\n",
      "epoch : 650 = >  Accuracy: 0.8278\n",
      "epoch : 700 = >  Accuracy: 0.8462\n",
      "epoch : 750 = >  Accuracy: 0.8598\n",
      "epoch : 800 = >  Accuracy: 0.8672\n",
      "epoch : 850 = >  Accuracy: 0.8684\n",
      "epoch : 900 = >  Accuracy: 0.8696\n",
      "Final accuracy: 0.8696\n",
      "epoch 0 ,Accuracy: 0.2103 , train acc : 0.18980797636632202\n",
      "epoch 50 ,Accuracy: 0.8057 , train acc : 0.8264401772525849\n",
      "epoch 100 ,Accuracy: 0.8143 , train acc : 0.8493353028064993\n",
      "epoch 150 ,Accuracy: 0.8278 , train acc : 0.8537666174298375\n",
      "epoch 200 ,Accuracy: 0.8216 , train acc : 0.8581979320531757\n",
      "epoch 250 ,Accuracy: 0.8229 , train acc : 0.8581979320531757\n",
      "epoch 300 ,Accuracy: 0.8290 , train acc : 0.8633677991137371\n",
      "epoch 350 ,Accuracy: 0.8315 , train acc : 0.8714918759231906\n",
      "epoch 400 ,Accuracy: 0.8807 , train acc : 0.9335302806499262\n",
      "epoch 450 ,Accuracy: 0.8856 , train acc : 0.9298375184638109\n",
      "epoch 500 ,Accuracy: 0.8831 , train acc : 0.931314623338257\n",
      "epoch 550 ,Accuracy: 0.8684 , train acc : 0.9401772525849336\n",
      "epoch 600 ,Accuracy: 0.8856 , train acc : 0.9372230428360414\n",
      "epoch 650 ,Accuracy: 0.8770 , train acc : 0.9409158050221565\n",
      "epoch 700 ,Accuracy: 0.8758 , train acc : 0.9416543574593796\n",
      "epoch 750 ,Accuracy: 0.8807 , train acc : 0.9401772525849336\n",
      "epoch 800 ,Accuracy: 0.8782 , train acc : 0.9409158050221565\n",
      "epoch 850 ,Accuracy: 0.8770 , train acc : 0.9505169867060561\n",
      "epoch 900 ,Accuracy: 0.8708 , train acc : 0.946824224519941\n",
      "epoch 950 ,Accuracy: 0.8672 , train acc : 0.9512555391432792\n",
      "epoch 1000 ,Accuracy: 0.8770 , train acc : 0.9519940915805022\n",
      "epoch 1050 ,Accuracy: 0.8708 , train acc : 0.9534711964549483\n",
      "epoch 1100 ,Accuracy: 0.8758 , train acc : 0.9579025110782866\n",
      "epoch 1150 ,Accuracy: 0.8807 , train acc : 0.9601181683899557\n",
      "epoch 1200 ,Accuracy: 0.8868 , train acc : 0.9593796159527327\n",
      "epoch 1250 ,Accuracy: 0.8745 , train acc : 0.9593796159527327\n",
      "epoch 1300 ,Accuracy: 0.8770 , train acc : 0.9638109305760709\n",
      "epoch 1350 ,Accuracy: 0.8721 , train acc : 0.965288035450517\n",
      "Accuracy: 0.8733\n",
      "base model\n",
      "0.8696186961869619\n",
      "0.8471841265516293\n",
      "OurModel\n",
      "0.8733087330873309\n",
      "0.8549139927318308\n",
      "*****\n",
      "epoch : 9 / 10\n",
      "epoch : 50 = >  Accuracy: 0.3235\n",
      "epoch : 100 = >  Accuracy: 0.4305\n",
      "epoch : 150 = >  Accuracy: 0.4022\n",
      "epoch : 200 = >  Accuracy: 0.3788\n",
      "epoch : 250 = >  Accuracy: 0.3665\n",
      "epoch : 300 = >  Accuracy: 0.3776\n",
      "epoch : 350 = >  Accuracy: 0.4453\n",
      "epoch : 400 = >  Accuracy: 0.5547\n",
      "epoch : 450 = >  Accuracy: 0.6273\n",
      "epoch : 500 = >  Accuracy: 0.7011\n",
      "epoch : 550 = >  Accuracy: 0.7565\n",
      "epoch : 600 = >  Accuracy: 0.8020\n",
      "epoch : 650 = >  Accuracy: 0.8303\n",
      "epoch : 700 = >  Accuracy: 0.8438\n",
      "epoch : 750 = >  Accuracy: 0.8598\n",
      "epoch : 800 = >  Accuracy: 0.8684\n",
      "epoch : 850 = >  Accuracy: 0.8684\n",
      "epoch : 900 = >  Accuracy: 0.8708\n",
      "Final accuracy: 0.8708\n",
      "epoch 0 ,Accuracy: 0.3161 , train acc : 0.2983751846381093\n",
      "epoch 50 ,Accuracy: 0.7614 , train acc : 0.7806499261447563\n",
      "epoch 100 ,Accuracy: 0.8229 , train acc : 0.8382570162481536\n",
      "epoch 150 ,Accuracy: 0.8290 , train acc : 0.8574593796159528\n",
      "epoch 200 ,Accuracy: 0.8266 , train acc : 0.8552437223042836\n",
      "epoch 250 ,Accuracy: 0.8253 , train acc : 0.8574593796159528\n",
      "epoch 300 ,Accuracy: 0.8229 , train acc : 0.8641063515509602\n",
      "epoch 350 ,Accuracy: 0.8315 , train acc : 0.8655834564254062\n",
      "epoch 400 ,Accuracy: 0.8278 , train acc : 0.8641063515509602\n",
      "epoch 450 ,Accuracy: 0.8229 , train acc : 0.8641063515509602\n",
      "epoch 500 ,Accuracy: 0.8266 , train acc : 0.8685376661742984\n",
      "epoch 550 ,Accuracy: 0.8229 , train acc : 0.8700147710487445\n",
      "epoch 600 ,Accuracy: 0.8315 , train acc : 0.8744460856720827\n",
      "epoch 650 ,Accuracy: 0.8253 , train acc : 0.8737075332348597\n",
      "epoch 700 ,Accuracy: 0.8229 , train acc : 0.8744460856720827\n",
      "epoch 750 ,Accuracy: 0.8303 , train acc : 0.878138847858198\n",
      "epoch 800 ,Accuracy: 0.8253 , train acc : 0.8774002954209749\n",
      "epoch 850 ,Accuracy: 0.8192 , train acc : 0.878138847858198\n",
      "epoch 900 ,Accuracy: 0.8155 , train acc : 0.8744460856720827\n",
      "epoch 950 ,Accuracy: 0.8204 , train acc : 0.8788774002954209\n",
      "epoch 1000 ,Accuracy: 0.8204 , train acc : 0.8810930576070901\n",
      "epoch 1050 ,Accuracy: 0.8278 , train acc : 0.8847858197932054\n",
      "epoch 1100 ,Accuracy: 0.8229 , train acc : 0.8847858197932054\n",
      "epoch 1150 ,Accuracy: 0.8192 , train acc : 0.8818316100443131\n",
      "epoch 1200 ,Accuracy: 0.8229 , train acc : 0.8825701624815362\n",
      "epoch 1250 ,Accuracy: 0.8659 , train acc : 0.9527326440177253\n",
      "epoch 1300 ,Accuracy: 0.8733 , train acc : 0.9542097488921714\n",
      "epoch 1350 ,Accuracy: 0.8745 , train acc : 0.965288035450517\n",
      "Accuracy: 0.8721\n",
      "base model\n",
      "0.8708487084870848\n",
      "0.8491569403355455\n",
      "OurModel\n",
      "0.8720787207872078\n",
      "0.8528945662225181\n",
      "*****\n",
      "epoch : 10 / 10\n",
      "epoch : 50 = >  Accuracy: 0.3272\n",
      "epoch : 100 = >  Accuracy: 0.4305\n",
      "epoch : 150 = >  Accuracy: 0.4034\n",
      "epoch : 200 = >  Accuracy: 0.3813\n",
      "epoch : 250 = >  Accuracy: 0.3678\n",
      "epoch : 300 = >  Accuracy: 0.3776\n",
      "epoch : 350 = >  Accuracy: 0.4477\n",
      "epoch : 400 = >  Accuracy: 0.5572\n",
      "epoch : 450 = >  Accuracy: 0.6285\n",
      "epoch : 500 = >  Accuracy: 0.7036\n",
      "epoch : 550 = >  Accuracy: 0.7565\n",
      "epoch : 600 = >  Accuracy: 0.8032\n",
      "epoch : 650 = >  Accuracy: 0.8303\n",
      "epoch : 700 = >  Accuracy: 0.8450\n",
      "epoch : 750 = >  Accuracy: 0.8610\n",
      "epoch : 800 = >  Accuracy: 0.8696\n",
      "epoch : 850 = >  Accuracy: 0.8696\n",
      "epoch : 900 = >  Accuracy: 0.8721\n",
      "Final accuracy: 0.8721\n",
      "epoch 0 ,Accuracy: 0.2177 , train acc : 0.24224519940915806\n",
      "epoch 50 ,Accuracy: 0.7232 , train acc : 0.7621861152141802\n",
      "epoch 100 ,Accuracy: 0.7651 , train acc : 0.7902511078286558\n",
      "epoch 150 ,Accuracy: 0.7884 , train acc : 0.8131462333825702\n",
      "epoch 200 ,Accuracy: 0.8057 , train acc : 0.827178729689808\n",
      "epoch 250 ,Accuracy: 0.8081 , train acc : 0.8345642540620384\n",
      "epoch 300 ,Accuracy: 0.8020 , train acc : 0.843426883308715\n",
      "epoch 350 ,Accuracy: 0.8204 , train acc : 0.8463810930576071\n",
      "epoch 400 ,Accuracy: 0.8143 , train acc : 0.8493353028064993\n",
      "epoch 450 ,Accuracy: 0.8167 , train acc : 0.8559822747415067\n",
      "epoch 500 ,Accuracy: 0.8069 , train acc : 0.8581979320531757\n",
      "epoch 550 ,Accuracy: 0.8204 , train acc : 0.8648449039881831\n",
      "epoch 600 ,Accuracy: 0.8180 , train acc : 0.8633677991137371\n",
      "epoch 650 ,Accuracy: 0.8167 , train acc : 0.8670605612998523\n",
      "epoch 700 ,Accuracy: 0.8364 , train acc : 0.8751846381093058\n",
      "epoch 750 ,Accuracy: 0.8401 , train acc : 0.8825701624815362\n",
      "epoch 800 ,Accuracy: 0.8696 , train acc : 0.930576070901034\n",
      "epoch 850 ,Accuracy: 0.8795 , train acc : 0.9416543574593796\n",
      "epoch 900 ,Accuracy: 0.8856 , train acc : 0.9423929098966026\n",
      "epoch 950 ,Accuracy: 0.8807 , train acc : 0.9446085672082718\n",
      "epoch 1000 ,Accuracy: 0.8819 , train acc : 0.94903988183161\n",
      "epoch 1050 ,Accuracy: 0.8807 , train acc : 0.9497784342688331\n",
      "epoch 1100 ,Accuracy: 0.8770 , train acc : 0.9512555391432792\n",
      "epoch 1150 ,Accuracy: 0.8782 , train acc : 0.9579025110782866\n",
      "epoch 1200 ,Accuracy: 0.8721 , train acc : 0.9623338257016248\n",
      "epoch 1250 ,Accuracy: 0.8782 , train acc : 0.9615952732644018\n",
      "epoch 1300 ,Accuracy: 0.8770 , train acc : 0.9579025110782866\n",
      "epoch 1350 ,Accuracy: 0.8770 , train acc : 0.9623338257016248\n",
      "Accuracy: 0.8782\n",
      "base model\n",
      "0.8720787207872078\n",
      "0.8504650464007\n",
      "OurModel\n",
      "0.8782287822878229\n",
      "0.860204088703177\n",
      "+++***Fianal Result***+++\n",
      "base\n",
      "Accuracy avg = 0.8710947109471094\n",
      "Accuracy deviation = 0.00113030330070439\n",
      "F1 score(macro) avg = 0.8487551667348968\n",
      "F1 score(macro) deviation = 0.001282845968051955\n",
      "new model\n",
      "Accuracy avg = 0.8752767527675277\n",
      "Accuracy deviation = 0.0037217894933694786\n",
      "F1 score(macro) avg = 0.8580427085421287\n",
      "F1 score(macro) deviation = 0.006215505209498426\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "GNN_model = GAT(dataset.num_features, dataset.num_classes, embedding_dimension=8).to(device)\n",
    "run_model(GNN_model , Gnn_epochs=700 ,enc_address=\"/content/encoder_emb64_cora.pt\" ,reply_threshold=0.95 , head_epochs= 3000 , validation_mask=new_valdation_mask , early_stop=0.88 )"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BZWtlHvMA2xf",
    "outputId": "d6986cba-337b-41cc-8044-ebe4ff1f76b1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "*****\n",
      "epoch : 1 / 10\n",
      "epoch : 50 = >  Accuracy: 0.3911\n",
      "epoch : 100 = >  Accuracy: 0.3678\n",
      "epoch : 150 = >  Accuracy: 0.3555\n",
      "epoch : 200 = >  Accuracy: 0.3579\n",
      "epoch : 250 = >  Accuracy: 0.3702\n",
      "epoch : 300 = >  Accuracy: 0.4231\n",
      "epoch : 350 = >  Accuracy: 0.5117\n",
      "epoch : 400 = >  Accuracy: 0.5978\n",
      "epoch : 450 = >  Accuracy: 0.6519\n",
      "epoch : 500 = >  Accuracy: 0.7208\n",
      "epoch : 550 = >  Accuracy: 0.7847\n",
      "epoch : 600 = >  Accuracy: 0.8167\n",
      "epoch : 650 = >  Accuracy: 0.8364\n",
      "epoch : 700 = >  Accuracy: 0.8512\n",
      "Final accuracy: 0.8512\n",
      "epoch 0 ,Accuracy: 0.1255 , train acc : 0.15066469719350073\n",
      "epoch 50 ,Accuracy: 0.3346 , train acc : 0.3190546528803545\n",
      "epoch 100 ,Accuracy: 0.4194 , train acc : 0.39217134416543575\n",
      "epoch 150 ,Accuracy: 0.4096 , train acc : 0.4224519940915805\n",
      "epoch 200 ,Accuracy: 0.4736 , train acc : 0.5\n",
      "epoch 250 ,Accuracy: 0.5006 , train acc : 0.5110782865583456\n",
      "epoch 300 ,Accuracy: 0.5572 , train acc : 0.5745937961595273\n",
      "epoch 350 ,Accuracy: 0.5633 , train acc : 0.5790251107828656\n",
      "epoch 400 ,Accuracy: 0.5621 , train acc : 0.5834564254062038\n",
      "epoch 450 ,Accuracy: 0.5547 , train acc : 0.5841949778434269\n",
      "epoch 500 ,Accuracy: 0.5966 , train acc : 0.6358936484490398\n",
      "epoch 550 ,Accuracy: 0.6015 , train acc : 0.6418020679468243\n",
      "epoch 600 ,Accuracy: 0.6064 , train acc : 0.6403249630723782\n",
      "epoch 650 ,Accuracy: 0.6039 , train acc : 0.6432791728212703\n",
      "epoch 700 ,Accuracy: 0.5978 , train acc : 0.6440177252584933\n",
      "epoch 750 ,Accuracy: 0.6039 , train acc : 0.6506646971935007\n",
      "epoch 800 ,Accuracy: 0.6027 , train acc : 0.6499261447562777\n",
      "epoch 850 ,Accuracy: 0.6052 , train acc : 0.6499261447562777\n",
      "epoch 900 ,Accuracy: 0.6027 , train acc : 0.654357459379616\n",
      "epoch 950 ,Accuracy: 0.6064 , train acc : 0.6580502215657311\n",
      "epoch 1000 ,Accuracy: 0.6039 , train acc : 0.656573116691285\n",
      "epoch 1050 ,Accuracy: 0.6052 , train acc : 0.6595273264401772\n",
      "epoch 1100 ,Accuracy: 0.6027 , train acc : 0.6610044313146234\n",
      "epoch 1150 ,Accuracy: 0.6039 , train acc : 0.6587887740029542\n",
      "epoch 1200 ,Accuracy: 0.6052 , train acc : 0.6654357459379616\n",
      "epoch 1250 ,Accuracy: 0.5966 , train acc : 0.6661742983751846\n",
      "epoch 1300 ,Accuracy: 0.5978 , train acc : 0.6602658788774003\n",
      "epoch 1350 ,Accuracy: 0.5990 , train acc : 0.6676514032496307\n",
      "epoch 1400 ,Accuracy: 0.6039 , train acc : 0.6698670605612999\n",
      "epoch 1450 ,Accuracy: 0.6039 , train acc : 0.670605612998523\n",
      "epoch 1500 ,Accuracy: 0.5990 , train acc : 0.6661742983751846\n",
      "epoch 1550 ,Accuracy: 0.6039 , train acc : 0.6735598227474151\n",
      "epoch 1600 ,Accuracy: 0.6064 , train acc : 0.6742983751846381\n",
      "epoch 1650 ,Accuracy: 0.6076 , train acc : 0.672082717872969\n",
      "epoch 1700 ,Accuracy: 0.6089 , train acc : 0.672082717872969\n",
      "epoch 1750 ,Accuracy: 0.6064 , train acc : 0.6757754800590842\n",
      "epoch 1800 ,Accuracy: 0.5978 , train acc : 0.672821270310192\n",
      "epoch 1850 ,Accuracy: 0.6064 , train acc : 0.6757754800590842\n",
      "epoch 1900 ,Accuracy: 0.6015 , train acc : 0.6779911373707533\n",
      "epoch 1950 ,Accuracy: 0.6052 , train acc : 0.6742983751846381\n",
      "epoch 2000 ,Accuracy: 0.6052 , train acc : 0.6787296898079763\n",
      "epoch 2050 ,Accuracy: 0.5990 , train acc : 0.6772525849335302\n",
      "epoch 2100 ,Accuracy: 0.6027 , train acc : 0.6794682422451994\n",
      "epoch 2150 ,Accuracy: 0.6052 , train acc : 0.6802067946824224\n",
      "epoch 2200 ,Accuracy: 0.5953 , train acc : 0.6816838995568686\n",
      "epoch 2250 ,Accuracy: 0.6064 , train acc : 0.6787296898079763\n",
      "epoch 2300 ,Accuracy: 0.6076 , train acc : 0.6816838995568686\n",
      "epoch 2350 ,Accuracy: 0.6052 , train acc : 0.6816838995568686\n",
      "epoch 2400 ,Accuracy: 0.6015 , train acc : 0.6853766617429837\n",
      "epoch 2450 ,Accuracy: 0.8807 , train acc : 0.9734121122599705\n",
      "epoch 2500 ,Accuracy: 0.8745 , train acc : 0.9793205317577548\n",
      "epoch 2550 ,Accuracy: 0.8696 , train acc : 0.9785819793205317\n",
      "epoch 2600 ,Accuracy: 0.8745 , train acc : 0.9807976366322009\n",
      "epoch 2650 ,Accuracy: 0.8770 , train acc : 0.9778434268833087\n",
      "epoch 2700 ,Accuracy: 0.8721 , train acc : 0.982274741506647\n",
      "epoch 2750 ,Accuracy: 0.8819 , train acc : 0.9800590841949779\n",
      "epoch 2800 ,Accuracy: 0.8721 , train acc : 0.9800590841949779\n",
      "epoch 2850 ,Accuracy: 0.8708 , train acc : 0.9859675036927622\n",
      "epoch 2900 ,Accuracy: 0.8758 , train acc : 0.98301329394387\n",
      "epoch 2950 ,Accuracy: 0.8708 , train acc : 0.9859675036927622\n",
      "Accuracy: 0.8782\n",
      "base model\n",
      "0.8511685116851169\n",
      "0.8245549233369756\n",
      "OurModel\n",
      "0.8782287822878229\n",
      "0.8635414122151668\n",
      "*****\n",
      "epoch : 2 / 10\n",
      "epoch : 50 = >  Accuracy: 0.3899\n",
      "epoch : 100 = >  Accuracy: 0.3690\n",
      "epoch : 150 = >  Accuracy: 0.3555\n",
      "epoch : 200 = >  Accuracy: 0.3579\n",
      "epoch : 250 = >  Accuracy: 0.3715\n",
      "epoch : 300 = >  Accuracy: 0.4244\n",
      "epoch : 350 = >  Accuracy: 0.5141\n",
      "epoch : 400 = >  Accuracy: 0.5978\n",
      "epoch : 450 = >  Accuracy: 0.6544\n",
      "epoch : 500 = >  Accuracy: 0.7208\n",
      "epoch : 550 = >  Accuracy: 0.7835\n",
      "epoch : 600 = >  Accuracy: 0.8192\n",
      "epoch : 650 = >  Accuracy: 0.8364\n",
      "epoch : 700 = >  Accuracy: 0.8512\n",
      "Final accuracy: 0.8512\n",
      "epoch 0 ,Accuracy: 0.1316 , train acc : 0.12038404726735598\n",
      "epoch 50 ,Accuracy: 0.4440 , train acc : 0.4505169867060561\n",
      "epoch 100 ,Accuracy: 0.5068 , train acc : 0.5612998522895125\n",
      "epoch 150 ,Accuracy: 0.5363 , train acc : 0.5908419497784343\n",
      "epoch 200 ,Accuracy: 0.5498 , train acc : 0.6019202363367799\n",
      "epoch 250 ,Accuracy: 0.5523 , train acc : 0.6255539143279173\n",
      "epoch 300 ,Accuracy: 0.5855 , train acc : 0.6462333825701625\n",
      "epoch 350 ,Accuracy: 0.7294 , train acc : 0.7607090103397341\n",
      "epoch 400 ,Accuracy: 0.7528 , train acc : 0.8013293943870015\n",
      "epoch 450 ,Accuracy: 0.7626 , train acc : 0.8072378138847858\n",
      "epoch 500 ,Accuracy: 0.7737 , train acc : 0.8205317577548006\n",
      "epoch 550 ,Accuracy: 0.7811 , train acc : 0.8161004431314623\n",
      "epoch 600 ,Accuracy: 0.7884 , train acc : 0.8242245199409158\n",
      "epoch 650 ,Accuracy: 0.7835 , train acc : 0.8330871491875923\n",
      "epoch 700 ,Accuracy: 0.8155 , train acc : 0.861890694239291\n",
      "epoch 750 ,Accuracy: 0.8696 , train acc : 0.9202363367799113\n",
      "epoch 800 ,Accuracy: 0.8733 , train acc : 0.9261447562776958\n",
      "epoch 850 ,Accuracy: 0.8795 , train acc : 0.9298375184638109\n",
      "epoch 900 ,Accuracy: 0.8795 , train acc : 0.9298375184638109\n",
      "epoch 950 ,Accuracy: 0.8819 , train acc : 0.931314623338257\n",
      "epoch 1000 ,Accuracy: 0.8782 , train acc : 0.9342688330871491\n",
      "epoch 1050 ,Accuracy: 0.8733 , train acc : 0.9387001477104875\n",
      "epoch 1100 ,Accuracy: 0.8758 , train acc : 0.9416543574593796\n",
      "epoch 1150 ,Accuracy: 0.8721 , train acc : 0.9401772525849336\n",
      "epoch 1200 ,Accuracy: 0.8844 , train acc : 0.9401772525849336\n",
      "epoch 1250 ,Accuracy: 0.8721 , train acc : 0.9512555391432792\n",
      "epoch 1300 ,Accuracy: 0.8708 , train acc : 0.946824224519941\n",
      "epoch 1350 ,Accuracy: 0.8684 , train acc : 0.94903988183161\n",
      "epoch 1400 ,Accuracy: 0.8807 , train acc : 0.9497784342688331\n",
      "epoch 1450 ,Accuracy: 0.8770 , train acc : 0.9556868537666174\n",
      "epoch 1500 ,Accuracy: 0.8733 , train acc : 0.9564254062038404\n",
      "epoch 1550 ,Accuracy: 0.8745 , train acc : 0.9556868537666174\n",
      "epoch 1600 ,Accuracy: 0.8758 , train acc : 0.9645494830132939\n",
      "epoch 1650 ,Accuracy: 0.8733 , train acc : 0.9608567208271788\n",
      "epoch 1700 ,Accuracy: 0.8721 , train acc : 0.9667651403249631\n",
      "epoch 1750 ,Accuracy: 0.8856 , train acc : 0.9682422451994092\n",
      "epoch 1800 ,Accuracy: 0.8721 , train acc : 0.9638109305760709\n",
      "epoch 1850 ,Accuracy: 0.8745 , train acc : 0.9697193500738552\n",
      "epoch 1900 ,Accuracy: 0.8881 , train acc : 0.9667651403249631\n",
      "epoch 1950 ,Accuracy: 0.8659 , train acc : 0.9748892171344166\n",
      "epoch 2000 ,Accuracy: 0.8795 , train acc : 0.9697193500738552\n",
      "epoch 2050 ,Accuracy: 0.8758 , train acc : 0.9771048744460856\n",
      "epoch 2100 ,Accuracy: 0.8708 , train acc : 0.9763663220088626\n",
      "epoch 2150 ,Accuracy: 0.8782 , train acc : 0.9807976366322009\n",
      "epoch 2200 ,Accuracy: 0.8635 , train acc : 0.9778434268833087\n",
      "epoch 2250 ,Accuracy: 0.8635 , train acc : 0.9867060561299852\n",
      "epoch 2300 ,Accuracy: 0.8733 , train acc : 0.9785819793205317\n",
      "epoch 2350 ,Accuracy: 0.8733 , train acc : 0.9793205317577548\n",
      "epoch 2400 ,Accuracy: 0.8807 , train acc : 0.9889217134416544\n",
      "epoch 2450 ,Accuracy: 0.8721 , train acc : 0.9867060561299852\n",
      "epoch 2500 ,Accuracy: 0.8708 , train acc : 0.9859675036927622\n",
      "epoch 2550 ,Accuracy: 0.8745 , train acc : 0.9874446085672083\n",
      "epoch 2600 ,Accuracy: 0.8696 , train acc : 0.9859675036927622\n",
      "epoch 2650 ,Accuracy: 0.8659 , train acc : 0.9918759231905465\n",
      "epoch 2700 ,Accuracy: 0.8782 , train acc : 0.9881831610044313\n",
      "epoch 2750 ,Accuracy: 0.8733 , train acc : 0.9903988183161004\n",
      "epoch 2800 ,Accuracy: 0.8758 , train acc : 0.9896602658788775\n",
      "epoch 2850 ,Accuracy: 0.8659 , train acc : 0.9911373707533235\n",
      "epoch 2900 ,Accuracy: 0.8721 , train acc : 0.9881831610044313\n",
      "epoch 2950 ,Accuracy: 0.8708 , train acc : 0.9889217134416544\n",
      "Accuracy: 0.8758\n",
      "base model\n",
      "0.8511685116851169\n",
      "0.825252218193612\n",
      "OurModel\n",
      "0.8757687576875769\n",
      "0.8659621901611215\n",
      "*****\n",
      "epoch : 3 / 10\n",
      "epoch : 50 = >  Accuracy: 0.3887\n",
      "epoch : 100 = >  Accuracy: 0.3690\n",
      "epoch : 150 = >  Accuracy: 0.3567\n",
      "epoch : 200 = >  Accuracy: 0.3579\n",
      "epoch : 250 = >  Accuracy: 0.3702\n",
      "epoch : 300 = >  Accuracy: 0.4268\n",
      "epoch : 350 = >  Accuracy: 0.5141\n",
      "epoch : 400 = >  Accuracy: 0.5978\n",
      "epoch : 450 = >  Accuracy: 0.6519\n",
      "epoch : 500 = >  Accuracy: 0.7208\n",
      "epoch : 550 = >  Accuracy: 0.7884\n",
      "epoch : 600 = >  Accuracy: 0.8180\n",
      "epoch : 650 = >  Accuracy: 0.8364\n",
      "epoch : 700 = >  Accuracy: 0.8512\n",
      "Final accuracy: 0.8512\n",
      "epoch 0 ,Accuracy: 0.3173 , train acc : 0.3087149187592319\n",
      "epoch 50 ,Accuracy: 0.6986 , train acc : 0.7230428360413589\n",
      "epoch 100 ,Accuracy: 0.7282 , train acc : 0.7673559822747416\n",
      "epoch 150 ,Accuracy: 0.7589 , train acc : 0.7865583456425406\n",
      "epoch 200 ,Accuracy: 0.7614 , train acc : 0.8035450516986706\n",
      "epoch 250 ,Accuracy: 0.7786 , train acc : 0.8227474150664698\n",
      "epoch 300 ,Accuracy: 0.7897 , train acc : 0.8212703101920237\n",
      "epoch 350 ,Accuracy: 0.7774 , train acc : 0.8389955686853766\n",
      "epoch 400 ,Accuracy: 0.7946 , train acc : 0.8338257016248154\n",
      "epoch 450 ,Accuracy: 0.7970 , train acc : 0.8353028064992615\n",
      "epoch 500 ,Accuracy: 0.8057 , train acc : 0.8441654357459379\n",
      "epoch 550 ,Accuracy: 0.8007 , train acc : 0.843426883308715\n",
      "epoch 600 ,Accuracy: 0.7995 , train acc : 0.8508124076809453\n",
      "epoch 650 ,Accuracy: 0.8143 , train acc : 0.8604135893648449\n",
      "epoch 700 ,Accuracy: 0.8081 , train acc : 0.8530280649926145\n",
      "epoch 750 ,Accuracy: 0.8130 , train acc : 0.8589364844903988\n",
      "epoch 800 ,Accuracy: 0.8044 , train acc : 0.8567208271787297\n",
      "epoch 850 ,Accuracy: 0.8081 , train acc : 0.8626292466765141\n",
      "epoch 900 ,Accuracy: 0.8093 , train acc : 0.8648449039881831\n",
      "epoch 950 ,Accuracy: 0.8106 , train acc : 0.8700147710487445\n",
      "epoch 1000 ,Accuracy: 0.8155 , train acc : 0.8700147710487445\n",
      "epoch 1050 ,Accuracy: 0.8155 , train acc : 0.8729689807976366\n",
      "epoch 1100 ,Accuracy: 0.8180 , train acc : 0.8759231905465288\n",
      "epoch 1150 ,Accuracy: 0.8130 , train acc : 0.8729689807976366\n",
      "epoch 1200 ,Accuracy: 0.8081 , train acc : 0.879615952732644\n",
      "epoch 1250 ,Accuracy: 0.8167 , train acc : 0.879615952732644\n",
      "epoch 1300 ,Accuracy: 0.8180 , train acc : 0.8833087149187593\n",
      "epoch 1350 ,Accuracy: 0.8180 , train acc : 0.8847858197932054\n",
      "epoch 1400 ,Accuracy: 0.8143 , train acc : 0.8818316100443131\n",
      "epoch 1450 ,Accuracy: 0.8155 , train acc : 0.8862629246676514\n",
      "epoch 1500 ,Accuracy: 0.8167 , train acc : 0.8862629246676514\n",
      "epoch 1550 ,Accuracy: 0.8180 , train acc : 0.880354505169867\n",
      "epoch 1600 ,Accuracy: 0.8229 , train acc : 0.8877400295420975\n",
      "epoch 1650 ,Accuracy: 0.8204 , train acc : 0.8899556868537666\n",
      "epoch 1700 ,Accuracy: 0.8180 , train acc : 0.8906942392909897\n",
      "epoch 1750 ,Accuracy: 0.8130 , train acc : 0.8914327917282127\n",
      "epoch 1800 ,Accuracy: 0.8180 , train acc : 0.8943870014771049\n",
      "epoch 1850 ,Accuracy: 0.8216 , train acc : 0.8906942392909897\n",
      "epoch 1900 ,Accuracy: 0.8204 , train acc : 0.8921713441654358\n",
      "epoch 1950 ,Accuracy: 0.8167 , train acc : 0.8929098966026587\n",
      "epoch 2000 ,Accuracy: 0.8118 , train acc : 0.8973412112259971\n",
      "epoch 2050 ,Accuracy: 0.8253 , train acc : 0.8921713441654358\n",
      "epoch 2100 ,Accuracy: 0.8143 , train acc : 0.896602658788774\n",
      "epoch 2150 ,Accuracy: 0.8241 , train acc : 0.8973412112259971\n",
      "epoch 2200 ,Accuracy: 0.8241 , train acc : 0.896602658788774\n",
      "epoch 2250 ,Accuracy: 0.8216 , train acc : 0.9017725258493353\n",
      "epoch 2300 ,Accuracy: 0.8216 , train acc : 0.896602658788774\n",
      "epoch 2350 ,Accuracy: 0.8118 , train acc : 0.9002954209748892\n",
      "epoch 2400 ,Accuracy: 0.8155 , train acc : 0.896602658788774\n",
      "epoch 2450 ,Accuracy: 0.8216 , train acc : 0.8973412112259971\n",
      "epoch 2500 ,Accuracy: 0.8315 , train acc : 0.9017725258493353\n",
      "epoch 2550 ,Accuracy: 0.8290 , train acc : 0.9054652880354506\n",
      "epoch 2600 ,Accuracy: 0.8155 , train acc : 0.9002954209748892\n",
      "epoch 2650 ,Accuracy: 0.8130 , train acc : 0.9069423929098966\n",
      "epoch 2700 ,Accuracy: 0.8192 , train acc : 0.9039881831610044\n",
      "epoch 2750 ,Accuracy: 0.8180 , train acc : 0.9062038404726735\n",
      "epoch 2800 ,Accuracy: 0.8204 , train acc : 0.9062038404726735\n",
      "epoch 2850 ,Accuracy: 0.8278 , train acc : 0.9084194977843427\n",
      "epoch 2900 ,Accuracy: 0.8167 , train acc : 0.9069423929098966\n",
      "epoch 2950 ,Accuracy: 0.8290 , train acc : 0.9054652880354506\n",
      "epoch 0 ,Accuracy: 0.1390 , train acc : 0.15805022156573117\n",
      "epoch 50 ,Accuracy: 0.7073 , train acc : 0.7267355982274741\n",
      "epoch 100 ,Accuracy: 0.7884 , train acc : 0.8146233382570163\n",
      "epoch 150 ,Accuracy: 0.8130 , train acc : 0.8367799113737076\n",
      "epoch 200 ,Accuracy: 0.8241 , train acc : 0.8471196454948301\n",
      "epoch 250 ,Accuracy: 0.8708 , train acc : 0.9098966026587888\n",
      "epoch 300 ,Accuracy: 0.8782 , train acc : 0.914327917282127\n",
      "epoch 350 ,Accuracy: 0.8819 , train acc : 0.9172821270310192\n",
      "epoch 400 ,Accuracy: 0.8770 , train acc : 0.9224519940915805\n",
      "epoch 450 ,Accuracy: 0.8795 , train acc : 0.9194977843426884\n",
      "epoch 500 ,Accuracy: 0.8782 , train acc : 0.9268833087149188\n",
      "epoch 550 ,Accuracy: 0.8758 , train acc : 0.9217134416543574\n",
      "epoch 600 ,Accuracy: 0.8770 , train acc : 0.9261447562776958\n",
      "epoch 650 ,Accuracy: 0.8844 , train acc : 0.9320531757754801\n",
      "epoch 700 ,Accuracy: 0.8819 , train acc : 0.9327917282127031\n",
      "epoch 750 ,Accuracy: 0.8745 , train acc : 0.9379615952732644\n",
      "epoch 800 ,Accuracy: 0.8758 , train acc : 0.9342688330871491\n",
      "epoch 850 ,Accuracy: 0.8807 , train acc : 0.9409158050221565\n",
      "epoch 900 ,Accuracy: 0.8770 , train acc : 0.9387001477104875\n",
      "epoch 950 ,Accuracy: 0.8782 , train acc : 0.9431314623338257\n",
      "epoch 1000 ,Accuracy: 0.8819 , train acc : 0.9379615952732644\n",
      "epoch 1050 ,Accuracy: 0.8831 , train acc : 0.948301329394387\n",
      "epoch 1100 ,Accuracy: 0.8795 , train acc : 0.9453471196454948\n",
      "epoch 1150 ,Accuracy: 0.8819 , train acc : 0.9542097488921714\n",
      "epoch 1200 ,Accuracy: 0.8819 , train acc : 0.94903988183161\n",
      "epoch 1250 ,Accuracy: 0.8856 , train acc : 0.9534711964549483\n",
      "epoch 1300 ,Accuracy: 0.8844 , train acc : 0.9542097488921714\n",
      "epoch 1350 ,Accuracy: 0.8807 , train acc : 0.9593796159527327\n",
      "epoch 1400 ,Accuracy: 0.8795 , train acc : 0.9556868537666174\n",
      "epoch 1450 ,Accuracy: 0.8868 , train acc : 0.9615952732644018\n",
      "epoch 1500 ,Accuracy: 0.8758 , train acc : 0.9608567208271788\n",
      "epoch 1550 ,Accuracy: 0.8807 , train acc : 0.9645494830132939\n",
      "epoch 1600 ,Accuracy: 0.8831 , train acc : 0.9763663220088626\n",
      "epoch 1650 ,Accuracy: 0.8782 , train acc : 0.9689807976366323\n",
      "epoch 1700 ,Accuracy: 0.8770 , train acc : 0.9778434268833087\n",
      "epoch 1750 ,Accuracy: 0.8684 , train acc : 0.9793205317577548\n",
      "epoch 1800 ,Accuracy: 0.8795 , train acc : 0.9726735598227474\n",
      "epoch 1850 ,Accuracy: 0.8782 , train acc : 0.9719350073855244\n",
      "epoch 1900 ,Accuracy: 0.8733 , train acc : 0.9785819793205317\n",
      "epoch 1950 ,Accuracy: 0.8745 , train acc : 0.983751846381093\n",
      "epoch 2000 ,Accuracy: 0.8745 , train acc : 0.9844903988183161\n",
      "epoch 2050 ,Accuracy: 0.8770 , train acc : 0.9756277695716395\n",
      "epoch 2100 ,Accuracy: 0.8696 , train acc : 0.9785819793205317\n",
      "epoch 2150 ,Accuracy: 0.8795 , train acc : 0.9859675036927622\n",
      "epoch 2200 ,Accuracy: 0.8672 , train acc : 0.9881831610044313\n",
      "epoch 2250 ,Accuracy: 0.8696 , train acc : 0.9874446085672083\n",
      "epoch 2300 ,Accuracy: 0.8831 , train acc : 0.9926144756277696\n",
      "epoch 2350 ,Accuracy: 0.8770 , train acc : 0.9918759231905465\n",
      "epoch 2400 ,Accuracy: 0.8795 , train acc : 0.9844903988183161\n",
      "epoch 2450 ,Accuracy: 0.8745 , train acc : 0.9896602658788775\n",
      "epoch 2500 ,Accuracy: 0.8659 , train acc : 0.9926144756277696\n",
      "epoch 2550 ,Accuracy: 0.8782 , train acc : 0.9881831610044313\n",
      "epoch 2600 ,Accuracy: 0.8684 , train acc : 0.9903988183161004\n",
      "epoch 2650 ,Accuracy: 0.8721 , train acc : 0.9926144756277696\n",
      "epoch 2700 ,Accuracy: 0.8733 , train acc : 0.9889217134416544\n",
      "epoch 2750 ,Accuracy: 0.8831 , train acc : 0.9933530280649926\n",
      "epoch 2800 ,Accuracy: 0.8672 , train acc : 0.9926144756277696\n",
      "epoch 2850 ,Accuracy: 0.8733 , train acc : 0.9926144756277696\n",
      "epoch 2900 ,Accuracy: 0.8745 , train acc : 0.9963072378138847\n",
      "epoch 2950 ,Accuracy: 0.8770 , train acc : 0.9918759231905465\n",
      "Accuracy: 0.8708\n",
      "base model\n",
      "0.8511685116851169\n",
      "0.8245549233369756\n",
      "OurModel\n",
      "0.8708487084870848\n",
      "0.8540936367818774\n",
      "*****\n",
      "epoch : 4 / 10\n",
      "epoch : 50 = >  Accuracy: 0.3862\n",
      "epoch : 100 = >  Accuracy: 0.3653\n",
      "epoch : 150 = >  Accuracy: 0.3555\n",
      "epoch : 200 = >  Accuracy: 0.3579\n",
      "epoch : 250 = >  Accuracy: 0.3690\n",
      "epoch : 300 = >  Accuracy: 0.4231\n",
      "epoch : 350 = >  Accuracy: 0.5141\n",
      "epoch : 400 = >  Accuracy: 0.5966\n",
      "epoch : 450 = >  Accuracy: 0.6531\n",
      "epoch : 500 = >  Accuracy: 0.7208\n",
      "epoch : 550 = >  Accuracy: 0.7835\n",
      "epoch : 600 = >  Accuracy: 0.8180\n",
      "epoch : 650 = >  Accuracy: 0.8389\n",
      "epoch : 700 = >  Accuracy: 0.8512\n",
      "Final accuracy: 0.8512\n",
      "epoch 0 ,Accuracy: 0.3469 , train acc : 0.3500738552437223\n",
      "epoch 50 ,Accuracy: 0.7442 , train acc : 0.7666174298375185\n",
      "epoch 100 ,Accuracy: 0.8130 , train acc : 0.8249630723781388\n",
      "epoch 150 ,Accuracy: 0.8192 , train acc : 0.8463810930576071\n",
      "epoch 200 ,Accuracy: 0.8180 , train acc : 0.8485967503692762\n",
      "epoch 250 ,Accuracy: 0.8290 , train acc : 0.8552437223042836\n",
      "epoch 300 ,Accuracy: 0.8241 , train acc : 0.8581979320531757\n",
      "epoch 350 ,Accuracy: 0.8204 , train acc : 0.8581979320531757\n",
      "epoch 400 ,Accuracy: 0.8167 , train acc : 0.8596750369276218\n",
      "epoch 450 ,Accuracy: 0.8216 , train acc : 0.8611521418020679\n",
      "epoch 500 ,Accuracy: 0.8167 , train acc : 0.8677991137370753\n",
      "epoch 550 ,Accuracy: 0.8192 , train acc : 0.8655834564254062\n",
      "epoch 600 ,Accuracy: 0.8241 , train acc : 0.8648449039881831\n",
      "epoch 650 ,Accuracy: 0.8180 , train acc : 0.8641063515509602\n",
      "epoch 700 ,Accuracy: 0.8216 , train acc : 0.8714918759231906\n",
      "epoch 750 ,Accuracy: 0.8204 , train acc : 0.8692762186115214\n",
      "epoch 800 ,Accuracy: 0.8180 , train acc : 0.8729689807976366\n",
      "epoch 850 ,Accuracy: 0.8204 , train acc : 0.8700147710487445\n",
      "epoch 900 ,Accuracy: 0.8204 , train acc : 0.8751846381093058\n",
      "epoch 950 ,Accuracy: 0.8192 , train acc : 0.878138847858198\n",
      "epoch 1000 ,Accuracy: 0.8180 , train acc : 0.8774002954209749\n",
      "epoch 1050 ,Accuracy: 0.8278 , train acc : 0.8818316100443131\n",
      "epoch 1100 ,Accuracy: 0.8696 , train acc : 0.9438700147710487\n",
      "epoch 1150 ,Accuracy: 0.8758 , train acc : 0.9519940915805022\n",
      "epoch 1200 ,Accuracy: 0.8758 , train acc : 0.9534711964549483\n",
      "epoch 1250 ,Accuracy: 0.8868 , train acc : 0.9586410635155096\n",
      "epoch 1300 ,Accuracy: 0.8770 , train acc : 0.9534711964549483\n",
      "epoch 1350 ,Accuracy: 0.8856 , train acc : 0.9564254062038404\n",
      "epoch 1400 ,Accuracy: 0.8819 , train acc : 0.9586410635155096\n",
      "epoch 1450 ,Accuracy: 0.8745 , train acc : 0.9682422451994092\n",
      "epoch 1500 ,Accuracy: 0.8758 , train acc : 0.9675036927621861\n",
      "epoch 1550 ,Accuracy: 0.8708 , train acc : 0.9719350073855244\n",
      "epoch 1600 ,Accuracy: 0.8696 , train acc : 0.9748892171344166\n",
      "epoch 1650 ,Accuracy: 0.8696 , train acc : 0.9726735598227474\n",
      "epoch 1700 ,Accuracy: 0.8721 , train acc : 0.9697193500738552\n",
      "epoch 1750 ,Accuracy: 0.8708 , train acc : 0.9793205317577548\n",
      "epoch 1800 ,Accuracy: 0.8770 , train acc : 0.9793205317577548\n",
      "epoch 1850 ,Accuracy: 0.8721 , train acc : 0.9741506646971935\n",
      "epoch 1900 ,Accuracy: 0.8770 , train acc : 0.983751846381093\n",
      "epoch 1950 ,Accuracy: 0.8844 , train acc : 0.9807976366322009\n",
      "epoch 2000 ,Accuracy: 0.8770 , train acc : 0.9852289512555391\n",
      "epoch 2050 ,Accuracy: 0.8721 , train acc : 0.9844903988183161\n",
      "epoch 2100 ,Accuracy: 0.8868 , train acc : 0.9859675036927622\n",
      "epoch 2150 ,Accuracy: 0.8696 , train acc : 0.9859675036927622\n",
      "epoch 2200 ,Accuracy: 0.8782 , train acc : 0.9889217134416544\n",
      "epoch 2250 ,Accuracy: 0.8795 , train acc : 0.9874446085672083\n",
      "epoch 2300 ,Accuracy: 0.8721 , train acc : 0.9903988183161004\n",
      "epoch 2350 ,Accuracy: 0.8721 , train acc : 0.9896602658788775\n",
      "epoch 2400 ,Accuracy: 0.8745 , train acc : 0.9918759231905465\n",
      "epoch 2450 ,Accuracy: 0.8684 , train acc : 0.9940915805022157\n",
      "epoch 2500 ,Accuracy: 0.8819 , train acc : 0.9911373707533235\n",
      "epoch 2550 ,Accuracy: 0.8758 , train acc : 0.9918759231905465\n",
      "epoch 2600 ,Accuracy: 0.8819 , train acc : 0.9948301329394387\n",
      "epoch 2650 ,Accuracy: 0.8696 , train acc : 0.9963072378138847\n",
      "epoch 2700 ,Accuracy: 0.8708 , train acc : 0.9918759231905465\n",
      "epoch 2750 ,Accuracy: 0.8782 , train acc : 0.9926144756277696\n",
      "epoch 2800 ,Accuracy: 0.8758 , train acc : 0.9926144756277696\n",
      "epoch 2850 ,Accuracy: 0.8745 , train acc : 0.9970457902511078\n",
      "epoch 2900 ,Accuracy: 0.8795 , train acc : 0.9963072378138847\n",
      "epoch 2950 ,Accuracy: 0.8733 , train acc : 0.9948301329394387\n",
      "Accuracy: 0.8807\n",
      "base model\n",
      "0.8511685116851169\n",
      "0.825252218193612\n",
      "OurModel\n",
      "0.8806888068880688\n",
      "0.8652687771084198\n",
      "*****\n",
      "epoch : 5 / 10\n",
      "epoch : 50 = >  Accuracy: 0.3875\n",
      "epoch : 100 = >  Accuracy: 0.3678\n",
      "epoch : 150 = >  Accuracy: 0.3567\n",
      "epoch : 200 = >  Accuracy: 0.3579\n",
      "epoch : 250 = >  Accuracy: 0.3715\n",
      "epoch : 300 = >  Accuracy: 0.4256\n",
      "epoch : 350 = >  Accuracy: 0.5166\n",
      "epoch : 400 = >  Accuracy: 0.5990\n",
      "epoch : 450 = >  Accuracy: 0.6531\n",
      "epoch : 500 = >  Accuracy: 0.7208\n",
      "epoch : 550 = >  Accuracy: 0.7860\n",
      "epoch : 600 = >  Accuracy: 0.8180\n",
      "epoch : 650 = >  Accuracy: 0.8364\n",
      "epoch : 700 = >  Accuracy: 0.8512\n",
      "Final accuracy: 0.8512\n",
      "epoch 0 ,Accuracy: 0.2214 , train acc : 0.2223042836041359\n",
      "epoch 50 ,Accuracy: 0.4342 , train acc : 0.4098966026587888\n",
      "epoch 100 ,Accuracy: 0.7183 , train acc : 0.7112259970457903\n",
      "epoch 150 ,Accuracy: 0.7626 , train acc : 0.7459379615952733\n",
      "epoch 200 ,Accuracy: 0.7663 , train acc : 0.7525849335302807\n",
      "epoch 250 ,Accuracy: 0.8229 , train acc : 0.8441654357459379\n",
      "epoch 300 ,Accuracy: 0.8180 , train acc : 0.8500738552437223\n",
      "epoch 350 ,Accuracy: 0.8241 , train acc : 0.8530280649926145\n",
      "epoch 400 ,Accuracy: 0.8721 , train acc : 0.9150664697193501\n",
      "epoch 450 ,Accuracy: 0.8721 , train acc : 0.9187592319054653\n",
      "epoch 500 ,Accuracy: 0.8708 , train acc : 0.9231905465288035\n",
      "epoch 550 ,Accuracy: 0.8782 , train acc : 0.9268833087149188\n",
      "epoch 600 ,Accuracy: 0.8831 , train acc : 0.9246676514032496\n",
      "epoch 650 ,Accuracy: 0.8782 , train acc : 0.9231905465288035\n",
      "epoch 700 ,Accuracy: 0.8831 , train acc : 0.931314623338257\n",
      "epoch 750 ,Accuracy: 0.8856 , train acc : 0.9364844903988183\n",
      "epoch 800 ,Accuracy: 0.8819 , train acc : 0.9327917282127031\n",
      "epoch 850 ,Accuracy: 0.8782 , train acc : 0.9379615952732644\n",
      "epoch 900 ,Accuracy: 0.8807 , train acc : 0.9401772525849336\n",
      "epoch 950 ,Accuracy: 0.8708 , train acc : 0.9438700147710487\n",
      "epoch 1000 ,Accuracy: 0.8733 , train acc : 0.9460856720827179\n",
      "epoch 1050 ,Accuracy: 0.8844 , train acc : 0.9431314623338257\n",
      "epoch 1100 ,Accuracy: 0.8844 , train acc : 0.9438700147710487\n",
      "epoch 1150 ,Accuracy: 0.8868 , train acc : 0.947562776957164\n",
      "epoch 1200 ,Accuracy: 0.8770 , train acc : 0.9542097488921714\n",
      "epoch 1250 ,Accuracy: 0.8844 , train acc : 0.9564254062038404\n",
      "epoch 1300 ,Accuracy: 0.8770 , train acc : 0.96602658788774\n",
      "epoch 1350 ,Accuracy: 0.8745 , train acc : 0.9638109305760709\n",
      "epoch 1400 ,Accuracy: 0.8807 , train acc : 0.9615952732644018\n",
      "epoch 1450 ,Accuracy: 0.8770 , train acc : 0.9615952732644018\n",
      "epoch 1500 ,Accuracy: 0.8807 , train acc : 0.9615952732644018\n",
      "epoch 1550 ,Accuracy: 0.8795 , train acc : 0.9689807976366323\n",
      "epoch 1600 ,Accuracy: 0.8856 , train acc : 0.9675036927621861\n",
      "epoch 1650 ,Accuracy: 0.8758 , train acc : 0.9697193500738552\n",
      "epoch 1700 ,Accuracy: 0.8782 , train acc : 0.9675036927621861\n",
      "epoch 1750 ,Accuracy: 0.8795 , train acc : 0.9771048744460856\n",
      "epoch 1800 ,Accuracy: 0.8893 , train acc : 0.9778434268833087\n",
      "epoch 1850 ,Accuracy: 0.8856 , train acc : 0.9778434268833087\n",
      "epoch 1900 ,Accuracy: 0.8758 , train acc : 0.981536189069424\n",
      "epoch 1950 ,Accuracy: 0.8696 , train acc : 0.9793205317577548\n",
      "epoch 2000 ,Accuracy: 0.8721 , train acc : 0.98301329394387\n",
      "epoch 2050 ,Accuracy: 0.8770 , train acc : 0.9852289512555391\n",
      "epoch 2100 ,Accuracy: 0.8770 , train acc : 0.983751846381093\n",
      "epoch 2150 ,Accuracy: 0.8807 , train acc : 0.9867060561299852\n",
      "epoch 2200 ,Accuracy: 0.8819 , train acc : 0.9874446085672083\n",
      "epoch 2250 ,Accuracy: 0.8819 , train acc : 0.9896602658788775\n",
      "epoch 2300 ,Accuracy: 0.8770 , train acc : 0.9867060561299852\n",
      "epoch 2350 ,Accuracy: 0.8708 , train acc : 0.9896602658788775\n",
      "epoch 2400 ,Accuracy: 0.8819 , train acc : 0.9933530280649926\n",
      "epoch 2450 ,Accuracy: 0.8782 , train acc : 0.9881831610044313\n",
      "epoch 2500 ,Accuracy: 0.8770 , train acc : 0.9911373707533235\n",
      "epoch 2550 ,Accuracy: 0.8782 , train acc : 0.9896602658788775\n",
      "epoch 2600 ,Accuracy: 0.8733 , train acc : 0.9918759231905465\n",
      "epoch 2650 ,Accuracy: 0.8819 , train acc : 0.9911373707533235\n",
      "epoch 2700 ,Accuracy: 0.8807 , train acc : 0.9933530280649926\n",
      "epoch 2750 ,Accuracy: 0.8696 , train acc : 0.9940915805022157\n",
      "epoch 2800 ,Accuracy: 0.8708 , train acc : 0.9933530280649926\n",
      "epoch 2850 ,Accuracy: 0.8733 , train acc : 0.9940915805022157\n",
      "epoch 2900 ,Accuracy: 0.8758 , train acc : 0.9933530280649926\n",
      "epoch 2950 ,Accuracy: 0.8782 , train acc : 0.9963072378138847\n",
      "Accuracy: 0.8844\n",
      "base model\n",
      "0.8511685116851169\n",
      "0.8245549233369756\n",
      "OurModel\n",
      "0.8843788437884379\n",
      "0.8715342469909796\n",
      "*****\n",
      "epoch : 6 / 10\n",
      "epoch : 50 = >  Accuracy: 0.3887\n",
      "epoch : 100 = >  Accuracy: 0.3665\n",
      "epoch : 150 = >  Accuracy: 0.3567\n",
      "epoch : 200 = >  Accuracy: 0.3579\n",
      "epoch : 250 = >  Accuracy: 0.3715\n",
      "epoch : 300 = >  Accuracy: 0.4244\n",
      "epoch : 350 = >  Accuracy: 0.5141\n",
      "epoch : 400 = >  Accuracy: 0.5978\n",
      "epoch : 450 = >  Accuracy: 0.6519\n",
      "epoch : 500 = >  Accuracy: 0.7208\n",
      "epoch : 550 = >  Accuracy: 0.7872\n",
      "epoch : 600 = >  Accuracy: 0.8192\n",
      "epoch : 650 = >  Accuracy: 0.8376\n",
      "epoch : 700 = >  Accuracy: 0.8512\n",
      "Final accuracy: 0.8512\n",
      "epoch 0 ,Accuracy: 0.3186 , train acc : 0.31388478581979323\n",
      "epoch 50 ,Accuracy: 0.6052 , train acc : 0.5886262924667651\n",
      "epoch 100 ,Accuracy: 0.6950 , train acc : 0.6905465288035451\n",
      "epoch 150 ,Accuracy: 0.6974 , train acc : 0.6986706056129985\n",
      "epoch 200 ,Accuracy: 0.6999 , train acc : 0.706794682422452\n",
      "epoch 250 ,Accuracy: 0.6999 , train acc : 0.7104874446085672\n",
      "epoch 300 ,Accuracy: 0.7060 , train acc : 0.7141802067946824\n",
      "epoch 350 ,Accuracy: 0.8106 , train acc : 0.8485967503692762\n",
      "epoch 400 ,Accuracy: 0.8192 , train acc : 0.8559822747415067\n",
      "epoch 450 ,Accuracy: 0.8180 , train acc : 0.8559822747415067\n",
      "epoch 500 ,Accuracy: 0.8155 , train acc : 0.8589364844903988\n",
      "epoch 550 ,Accuracy: 0.8180 , train acc : 0.8648449039881831\n",
      "epoch 600 ,Accuracy: 0.8253 , train acc : 0.8692762186115214\n",
      "epoch 650 ,Accuracy: 0.8167 , train acc : 0.8648449039881831\n",
      "epoch 700 ,Accuracy: 0.8241 , train acc : 0.8663220088626292\n",
      "epoch 750 ,Accuracy: 0.8118 , train acc : 0.8692762186115214\n",
      "epoch 800 ,Accuracy: 0.8143 , train acc : 0.8685376661742984\n",
      "epoch 850 ,Accuracy: 0.8278 , train acc : 0.878138847858198\n",
      "epoch 900 ,Accuracy: 0.8204 , train acc : 0.8751846381093058\n",
      "epoch 950 ,Accuracy: 0.8180 , train acc : 0.8744460856720827\n",
      "epoch 1000 ,Accuracy: 0.8118 , train acc : 0.8818316100443131\n",
      "epoch 1050 ,Accuracy: 0.8229 , train acc : 0.8751846381093058\n",
      "epoch 1100 ,Accuracy: 0.8216 , train acc : 0.880354505169867\n",
      "epoch 1150 ,Accuracy: 0.8204 , train acc : 0.8833087149187593\n",
      "epoch 1200 ,Accuracy: 0.8216 , train acc : 0.8862629246676514\n",
      "epoch 1250 ,Accuracy: 0.8192 , train acc : 0.8833087149187593\n",
      "epoch 1300 ,Accuracy: 0.8130 , train acc : 0.8818316100443131\n",
      "epoch 1350 ,Accuracy: 0.8204 , train acc : 0.8951255539143279\n",
      "epoch 1400 ,Accuracy: 0.8253 , train acc : 0.8906942392909897\n",
      "epoch 1450 ,Accuracy: 0.8167 , train acc : 0.8921713441654358\n",
      "epoch 1500 ,Accuracy: 0.8241 , train acc : 0.8914327917282127\n",
      "epoch 1550 ,Accuracy: 0.8216 , train acc : 0.8921713441654358\n",
      "epoch 1600 ,Accuracy: 0.8180 , train acc : 0.8995568685376661\n",
      "epoch 1650 ,Accuracy: 0.8241 , train acc : 0.9062038404726735\n",
      "epoch 1700 ,Accuracy: 0.8229 , train acc : 0.9039881831610044\n",
      "epoch 1750 ,Accuracy: 0.8106 , train acc : 0.9069423929098966\n",
      "epoch 1800 ,Accuracy: 0.8241 , train acc : 0.9054652880354506\n",
      "epoch 1850 ,Accuracy: 0.8241 , train acc : 0.9062038404726735\n",
      "epoch 1900 ,Accuracy: 0.8130 , train acc : 0.9076809453471196\n",
      "epoch 1950 ,Accuracy: 0.8192 , train acc : 0.9076809453471196\n",
      "epoch 2000 ,Accuracy: 0.8192 , train acc : 0.9069423929098966\n",
      "epoch 2050 ,Accuracy: 0.8106 , train acc : 0.9121122599704579\n",
      "epoch 2100 ,Accuracy: 0.8155 , train acc : 0.9076809453471196\n",
      "epoch 2150 ,Accuracy: 0.8192 , train acc : 0.9106351550960118\n",
      "epoch 2200 ,Accuracy: 0.8118 , train acc : 0.9135893648449039\n",
      "epoch 2250 ,Accuracy: 0.8143 , train acc : 0.914327917282127\n",
      "epoch 2300 ,Accuracy: 0.8130 , train acc : 0.9121122599704579\n",
      "epoch 2350 ,Accuracy: 0.8192 , train acc : 0.9135893648449039\n",
      "epoch 2400 ,Accuracy: 0.8192 , train acc : 0.9135893648449039\n",
      "epoch 2450 ,Accuracy: 0.8155 , train acc : 0.9158050221565731\n",
      "epoch 2500 ,Accuracy: 0.8167 , train acc : 0.914327917282127\n",
      "epoch 2550 ,Accuracy: 0.8180 , train acc : 0.9172821270310192\n",
      "epoch 2600 ,Accuracy: 0.8118 , train acc : 0.9165435745937962\n",
      "epoch 2650 ,Accuracy: 0.8130 , train acc : 0.9187592319054653\n",
      "epoch 2700 ,Accuracy: 0.8167 , train acc : 0.9172821270310192\n",
      "epoch 2750 ,Accuracy: 0.8143 , train acc : 0.9150664697193501\n",
      "epoch 2800 ,Accuracy: 0.8216 , train acc : 0.9165435745937962\n",
      "epoch 2850 ,Accuracy: 0.8155 , train acc : 0.9180206794682423\n",
      "epoch 2900 ,Accuracy: 0.8118 , train acc : 0.9172821270310192\n",
      "epoch 2950 ,Accuracy: 0.8204 , train acc : 0.9180206794682423\n",
      "epoch 0 ,Accuracy: 0.1611 , train acc : 0.16691285081240767\n",
      "epoch 50 ,Accuracy: 0.4982 , train acc : 0.5553914327917282\n",
      "epoch 100 ,Accuracy: 0.5375 , train acc : 0.5915805022156573\n",
      "epoch 150 ,Accuracy: 0.5560 , train acc : 0.6056129985228951\n",
      "epoch 200 ,Accuracy: 0.5572 , train acc : 0.620384047267356\n",
      "epoch 250 ,Accuracy: 0.5658 , train acc : 0.620384047267356\n",
      "epoch 300 ,Accuracy: 0.6101 , train acc : 0.6536189069423929\n",
      "epoch 350 ,Accuracy: 0.6162 , train acc : 0.6558345642540621\n",
      "epoch 400 ,Accuracy: 0.6101 , train acc : 0.6610044313146234\n",
      "epoch 450 ,Accuracy: 0.6125 , train acc : 0.6624815361890695\n",
      "epoch 500 ,Accuracy: 0.6089 , train acc : 0.6669128508124077\n",
      "epoch 550 ,Accuracy: 0.7614 , train acc : 0.7968980797636632\n",
      "epoch 600 ,Accuracy: 0.7700 , train acc : 0.8042836041358936\n",
      "epoch 650 ,Accuracy: 0.7737 , train acc : 0.8028064992614475\n",
      "epoch 700 ,Accuracy: 0.7675 , train acc : 0.8035450516986706\n",
      "epoch 750 ,Accuracy: 0.7700 , train acc : 0.8057607090103397\n",
      "epoch 800 ,Accuracy: 0.7724 , train acc : 0.808714918759232\n",
      "epoch 850 ,Accuracy: 0.7663 , train acc : 0.8064992614475628\n",
      "epoch 900 ,Accuracy: 0.8216 , train acc : 0.8685376661742984\n",
      "epoch 950 ,Accuracy: 0.8364 , train acc : 0.8759231905465288\n",
      "epoch 1000 ,Accuracy: 0.8708 , train acc : 0.9335302806499262\n",
      "epoch 1050 ,Accuracy: 0.8758 , train acc : 0.9342688330871491\n",
      "epoch 1100 ,Accuracy: 0.8684 , train acc : 0.9379615952732644\n",
      "epoch 1150 ,Accuracy: 0.8745 , train acc : 0.9394387001477105\n",
      "epoch 1200 ,Accuracy: 0.8807 , train acc : 0.9453471196454948\n",
      "epoch 1250 ,Accuracy: 0.8745 , train acc : 0.94903988183161\n",
      "epoch 1300 ,Accuracy: 0.8807 , train acc : 0.9453471196454948\n",
      "epoch 1350 ,Accuracy: 0.8782 , train acc : 0.9512555391432792\n",
      "epoch 1400 ,Accuracy: 0.8758 , train acc : 0.9534711964549483\n",
      "epoch 1450 ,Accuracy: 0.8770 , train acc : 0.9579025110782866\n",
      "epoch 1500 ,Accuracy: 0.8844 , train acc : 0.9579025110782866\n",
      "epoch 1550 ,Accuracy: 0.8733 , train acc : 0.9556868537666174\n",
      "epoch 1600 ,Accuracy: 0.8795 , train acc : 0.9630723781388478\n",
      "epoch 1650 ,Accuracy: 0.8782 , train acc : 0.96602658788774\n",
      "epoch 1700 ,Accuracy: 0.8708 , train acc : 0.9689807976366323\n",
      "epoch 1750 ,Accuracy: 0.8807 , train acc : 0.96602658788774\n",
      "epoch 1800 ,Accuracy: 0.8708 , train acc : 0.9667651403249631\n",
      "epoch 1850 ,Accuracy: 0.8782 , train acc : 0.9697193500738552\n",
      "epoch 1900 ,Accuracy: 0.8733 , train acc : 0.9763663220088626\n",
      "epoch 1950 ,Accuracy: 0.8708 , train acc : 0.9785819793205317\n",
      "epoch 2000 ,Accuracy: 0.8770 , train acc : 0.9741506646971935\n",
      "epoch 2050 ,Accuracy: 0.8782 , train acc : 0.9807976366322009\n",
      "epoch 2100 ,Accuracy: 0.8831 , train acc : 0.981536189069424\n",
      "epoch 2150 ,Accuracy: 0.8758 , train acc : 0.9807976366322009\n",
      "epoch 2200 ,Accuracy: 0.8856 , train acc : 0.9874446085672083\n",
      "epoch 2250 ,Accuracy: 0.8795 , train acc : 0.9844903988183161\n",
      "epoch 2300 ,Accuracy: 0.8782 , train acc : 0.9852289512555391\n",
      "epoch 2350 ,Accuracy: 0.8758 , train acc : 0.9844903988183161\n",
      "epoch 2400 ,Accuracy: 0.8770 , train acc : 0.9852289512555391\n",
      "epoch 2450 ,Accuracy: 0.8721 , train acc : 0.9867060561299852\n",
      "epoch 2500 ,Accuracy: 0.8733 , train acc : 0.9874446085672083\n",
      "epoch 2550 ,Accuracy: 0.8733 , train acc : 0.9903988183161004\n",
      "epoch 2600 ,Accuracy: 0.8795 , train acc : 0.9940915805022157\n",
      "epoch 2650 ,Accuracy: 0.8745 , train acc : 0.9918759231905465\n",
      "epoch 2700 ,Accuracy: 0.8782 , train acc : 0.9933530280649926\n",
      "epoch 2750 ,Accuracy: 0.8807 , train acc : 0.9896602658788775\n",
      "epoch 2800 ,Accuracy: 0.8672 , train acc : 0.9948301329394387\n",
      "epoch 2850 ,Accuracy: 0.8745 , train acc : 0.9955686853766618\n",
      "epoch 2900 ,Accuracy: 0.8721 , train acc : 0.9955686853766618\n",
      "epoch 2950 ,Accuracy: 0.8721 , train acc : 0.9918759231905465\n",
      "Accuracy: 0.8733\n",
      "base model\n",
      "0.8511685116851169\n",
      "0.8245549233369756\n",
      "OurModel\n",
      "0.8733087330873309\n",
      "0.8591002473713331\n",
      "*****\n",
      "epoch : 7 / 10\n",
      "epoch : 50 = >  Accuracy: 0.3875\n",
      "epoch : 100 = >  Accuracy: 0.3641\n",
      "epoch : 150 = >  Accuracy: 0.3567\n",
      "epoch : 200 = >  Accuracy: 0.3579\n",
      "epoch : 250 = >  Accuracy: 0.3727\n",
      "epoch : 300 = >  Accuracy: 0.4244\n",
      "epoch : 350 = >  Accuracy: 0.5166\n",
      "epoch : 400 = >  Accuracy: 0.5978\n",
      "epoch : 450 = >  Accuracy: 0.6531\n",
      "epoch : 500 = >  Accuracy: 0.7220\n",
      "epoch : 550 = >  Accuracy: 0.7860\n",
      "epoch : 600 = >  Accuracy: 0.8192\n",
      "epoch : 650 = >  Accuracy: 0.8376\n",
      "epoch : 700 = >  Accuracy: 0.8512\n",
      "Final accuracy: 0.8512\n",
      "epoch 0 ,Accuracy: 0.1759 , train acc : 0.18463810930576072\n",
      "epoch 50 ,Accuracy: 0.5461 , train acc : 0.5915805022156573\n",
      "epoch 100 ,Accuracy: 0.6261 , train acc : 0.6875923190546529\n",
      "epoch 150 ,Accuracy: 0.8118 , train acc : 0.8308714918759232\n",
      "epoch 200 ,Accuracy: 0.8155 , train acc : 0.8537666174298375\n",
      "epoch 250 ,Accuracy: 0.8241 , train acc : 0.8463810930576071\n",
      "epoch 300 ,Accuracy: 0.8216 , train acc : 0.8485967503692762\n",
      "epoch 350 ,Accuracy: 0.8241 , train acc : 0.8552437223042836\n",
      "epoch 400 ,Accuracy: 0.8155 , train acc : 0.8611521418020679\n",
      "epoch 450 ,Accuracy: 0.8130 , train acc : 0.8559822747415067\n",
      "epoch 500 ,Accuracy: 0.8241 , train acc : 0.8589364844903988\n",
      "epoch 550 ,Accuracy: 0.8204 , train acc : 0.8655834564254062\n",
      "epoch 600 ,Accuracy: 0.8241 , train acc : 0.8626292466765141\n",
      "epoch 650 ,Accuracy: 0.8204 , train acc : 0.8589364844903988\n",
      "epoch 700 ,Accuracy: 0.8241 , train acc : 0.8700147710487445\n",
      "epoch 750 ,Accuracy: 0.8155 , train acc : 0.8663220088626292\n",
      "epoch 800 ,Accuracy: 0.8167 , train acc : 0.8707533234859675\n",
      "epoch 850 ,Accuracy: 0.8180 , train acc : 0.8700147710487445\n",
      "epoch 900 ,Accuracy: 0.8180 , train acc : 0.8677991137370753\n",
      "epoch 950 ,Accuracy: 0.8266 , train acc : 0.8729689807976366\n",
      "epoch 1000 ,Accuracy: 0.8266 , train acc : 0.8722304283604135\n",
      "epoch 1050 ,Accuracy: 0.8180 , train acc : 0.8729689807976366\n",
      "epoch 1100 ,Accuracy: 0.8204 , train acc : 0.8810930576070901\n",
      "epoch 1150 ,Accuracy: 0.8204 , train acc : 0.8766617429837519\n",
      "epoch 1200 ,Accuracy: 0.8216 , train acc : 0.8810930576070901\n",
      "epoch 1250 ,Accuracy: 0.8216 , train acc : 0.8833087149187593\n",
      "epoch 1300 ,Accuracy: 0.8180 , train acc : 0.8855243722304283\n",
      "epoch 1350 ,Accuracy: 0.8733 , train acc : 0.9534711964549483\n",
      "epoch 1400 ,Accuracy: 0.8795 , train acc : 0.9512555391432792\n",
      "epoch 1450 ,Accuracy: 0.8745 , train acc : 0.9505169867060561\n",
      "epoch 1500 ,Accuracy: 0.8770 , train acc : 0.9519940915805022\n",
      "epoch 1550 ,Accuracy: 0.8659 , train acc : 0.9608567208271788\n",
      "epoch 1600 ,Accuracy: 0.8807 , train acc : 0.9638109305760709\n",
      "epoch 1650 ,Accuracy: 0.8708 , train acc : 0.9630723781388478\n",
      "epoch 1700 ,Accuracy: 0.8733 , train acc : 0.9630723781388478\n",
      "epoch 1750 ,Accuracy: 0.8733 , train acc : 0.965288035450517\n",
      "epoch 1800 ,Accuracy: 0.8782 , train acc : 0.9697193500738552\n",
      "epoch 1850 ,Accuracy: 0.8758 , train acc : 0.9704579025110783\n",
      "epoch 1900 ,Accuracy: 0.8831 , train acc : 0.9741506646971935\n",
      "epoch 1950 ,Accuracy: 0.8782 , train acc : 0.9704579025110783\n",
      "epoch 2000 ,Accuracy: 0.8795 , train acc : 0.9734121122599705\n",
      "epoch 2050 ,Accuracy: 0.8696 , train acc : 0.9807976366322009\n",
      "epoch 2100 ,Accuracy: 0.8795 , train acc : 0.9785819793205317\n",
      "epoch 2150 ,Accuracy: 0.8659 , train acc : 0.9807976366322009\n",
      "epoch 2200 ,Accuracy: 0.8708 , train acc : 0.9852289512555391\n",
      "epoch 2250 ,Accuracy: 0.8733 , train acc : 0.9807976366322009\n",
      "epoch 2300 ,Accuracy: 0.8733 , train acc : 0.9852289512555391\n",
      "epoch 2350 ,Accuracy: 0.8745 , train acc : 0.9800590841949779\n",
      "epoch 2400 ,Accuracy: 0.8745 , train acc : 0.9844903988183161\n",
      "epoch 2450 ,Accuracy: 0.8721 , train acc : 0.9844903988183161\n",
      "epoch 2500 ,Accuracy: 0.8745 , train acc : 0.9859675036927622\n",
      "epoch 2550 ,Accuracy: 0.8708 , train acc : 0.98301329394387\n",
      "epoch 2600 ,Accuracy: 0.8708 , train acc : 0.983751846381093\n",
      "epoch 2650 ,Accuracy: 0.8684 , train acc : 0.9881831610044313\n",
      "epoch 2700 ,Accuracy: 0.8659 , train acc : 0.9889217134416544\n",
      "epoch 2750 ,Accuracy: 0.8708 , train acc : 0.9903988183161004\n",
      "epoch 2800 ,Accuracy: 0.8696 , train acc : 0.9903988183161004\n",
      "epoch 2850 ,Accuracy: 0.8635 , train acc : 0.9896602658788775\n",
      "epoch 2900 ,Accuracy: 0.8659 , train acc : 0.9918759231905465\n",
      "epoch 2950 ,Accuracy: 0.8635 , train acc : 0.9911373707533235\n",
      "Accuracy: 0.8708\n",
      "base model\n",
      "0.8511685116851169\n",
      "0.8245549233369756\n",
      "OurModel\n",
      "0.8708487084870848\n",
      "0.855333421433277\n",
      "*****\n",
      "epoch : 8 / 10\n",
      "epoch : 50 = >  Accuracy: 0.3875\n",
      "epoch : 100 = >  Accuracy: 0.3641\n",
      "epoch : 150 = >  Accuracy: 0.3567\n",
      "epoch : 200 = >  Accuracy: 0.3579\n",
      "epoch : 250 = >  Accuracy: 0.3727\n",
      "epoch : 300 = >  Accuracy: 0.4268\n",
      "epoch : 350 = >  Accuracy: 0.5141\n",
      "epoch : 400 = >  Accuracy: 0.5978\n",
      "epoch : 450 = >  Accuracy: 0.6531\n",
      "epoch : 500 = >  Accuracy: 0.7232\n",
      "epoch : 550 = >  Accuracy: 0.7835\n",
      "epoch : 600 = >  Accuracy: 0.8204\n",
      "epoch : 650 = >  Accuracy: 0.8376\n",
      "epoch : 700 = >  Accuracy: 0.8512\n",
      "Final accuracy: 0.8512\n",
      "epoch 0 ,Accuracy: 0.3260 , train acc : 0.3050221565731167\n",
      "epoch 50 ,Accuracy: 0.6027 , train acc : 0.604135893648449\n",
      "epoch 100 ,Accuracy: 0.6445 , train acc : 0.6432791728212703\n",
      "epoch 150 ,Accuracy: 0.7675 , train acc : 0.793205317577548\n",
      "epoch 200 ,Accuracy: 0.8241 , train acc : 0.8463810930576071\n",
      "epoch 250 ,Accuracy: 0.8253 , train acc : 0.8485967503692762\n",
      "epoch 300 ,Accuracy: 0.8266 , train acc : 0.8596750369276218\n",
      "epoch 350 ,Accuracy: 0.8204 , train acc : 0.8559822747415067\n",
      "epoch 400 ,Accuracy: 0.8290 , train acc : 0.8604135893648449\n",
      "epoch 450 ,Accuracy: 0.8782 , train acc : 0.9217134416543574\n",
      "epoch 500 ,Accuracy: 0.8831 , train acc : 0.9187592319054653\n",
      "epoch 550 ,Accuracy: 0.8758 , train acc : 0.9217134416543574\n",
      "epoch 600 ,Accuracy: 0.8782 , train acc : 0.9231905465288035\n",
      "epoch 650 ,Accuracy: 0.8745 , train acc : 0.930576070901034\n",
      "epoch 700 ,Accuracy: 0.8733 , train acc : 0.9254062038404727\n",
      "epoch 750 ,Accuracy: 0.8807 , train acc : 0.9327917282127031\n",
      "epoch 800 ,Accuracy: 0.8733 , train acc : 0.9431314623338257\n",
      "epoch 850 ,Accuracy: 0.8721 , train acc : 0.9364844903988183\n",
      "epoch 900 ,Accuracy: 0.8782 , train acc : 0.9394387001477105\n",
      "epoch 950 ,Accuracy: 0.8819 , train acc : 0.9438700147710487\n",
      "epoch 1000 ,Accuracy: 0.8795 , train acc : 0.9409158050221565\n",
      "epoch 1050 ,Accuracy: 0.8795 , train acc : 0.9401772525849336\n",
      "epoch 1100 ,Accuracy: 0.8831 , train acc : 0.9438700147710487\n",
      "epoch 1150 ,Accuracy: 0.8807 , train acc : 0.9527326440177253\n",
      "epoch 1200 ,Accuracy: 0.8844 , train acc : 0.9534711964549483\n",
      "epoch 1250 ,Accuracy: 0.8844 , train acc : 0.9564254062038404\n",
      "epoch 1300 ,Accuracy: 0.8807 , train acc : 0.9608567208271788\n",
      "epoch 1350 ,Accuracy: 0.8819 , train acc : 0.9579025110782866\n",
      "epoch 1400 ,Accuracy: 0.8758 , train acc : 0.9630723781388478\n",
      "epoch 1450 ,Accuracy: 0.8795 , train acc : 0.9615952732644018\n",
      "epoch 1500 ,Accuracy: 0.8807 , train acc : 0.9608567208271788\n",
      "epoch 1550 ,Accuracy: 0.8758 , train acc : 0.965288035450517\n",
      "epoch 1600 ,Accuracy: 0.8795 , train acc : 0.9675036927621861\n",
      "epoch 1650 ,Accuracy: 0.8770 , train acc : 0.9726735598227474\n",
      "epoch 1700 ,Accuracy: 0.8807 , train acc : 0.9704579025110783\n",
      "epoch 1750 ,Accuracy: 0.8881 , train acc : 0.9778434268833087\n",
      "epoch 1800 ,Accuracy: 0.8733 , train acc : 0.9763663220088626\n",
      "epoch 1850 ,Accuracy: 0.8758 , train acc : 0.9756277695716395\n",
      "epoch 1900 ,Accuracy: 0.8844 , train acc : 0.9800590841949779\n",
      "epoch 1950 ,Accuracy: 0.8733 , train acc : 0.9800590841949779\n",
      "epoch 2000 ,Accuracy: 0.8770 , train acc : 0.9844903988183161\n",
      "epoch 2050 ,Accuracy: 0.8708 , train acc : 0.9881831610044313\n",
      "epoch 2100 ,Accuracy: 0.8844 , train acc : 0.9874446085672083\n",
      "epoch 2150 ,Accuracy: 0.8782 , train acc : 0.982274741506647\n",
      "epoch 2200 ,Accuracy: 0.8819 , train acc : 0.9911373707533235\n",
      "epoch 2250 ,Accuracy: 0.8831 , train acc : 0.9881831610044313\n",
      "epoch 2300 ,Accuracy: 0.8647 , train acc : 0.9859675036927622\n",
      "epoch 2350 ,Accuracy: 0.8758 , train acc : 0.9933530280649926\n",
      "epoch 2400 ,Accuracy: 0.8819 , train acc : 0.9911373707533235\n",
      "epoch 2450 ,Accuracy: 0.8770 , train acc : 0.9926144756277696\n",
      "epoch 2500 ,Accuracy: 0.8856 , train acc : 0.9911373707533235\n",
      "epoch 2550 ,Accuracy: 0.8721 , train acc : 0.9911373707533235\n",
      "epoch 2600 ,Accuracy: 0.8684 , train acc : 0.9896602658788775\n",
      "epoch 2650 ,Accuracy: 0.8807 , train acc : 0.9948301329394387\n",
      "epoch 2700 ,Accuracy: 0.8733 , train acc : 0.9948301329394387\n",
      "epoch 2750 ,Accuracy: 0.8758 , train acc : 0.9963072378138847\n",
      "epoch 2800 ,Accuracy: 0.8831 , train acc : 0.9940915805022157\n",
      "epoch 2850 ,Accuracy: 0.8684 , train acc : 0.9933530280649926\n",
      "epoch 2900 ,Accuracy: 0.8733 , train acc : 0.9918759231905465\n",
      "epoch 2950 ,Accuracy: 0.8733 , train acc : 0.9933530280649926\n",
      "Accuracy: 0.8782\n",
      "base model\n",
      "0.8511685116851169\n",
      "0.8245549233369756\n",
      "OurModel\n",
      "0.8782287822878229\n",
      "0.8617476056738083\n",
      "*****\n",
      "epoch : 9 / 10\n",
      "epoch : 50 = >  Accuracy: 0.3801\n",
      "epoch : 100 = >  Accuracy: 0.3641\n",
      "epoch : 150 = >  Accuracy: 0.3555\n",
      "epoch : 200 = >  Accuracy: 0.3579\n",
      "epoch : 250 = >  Accuracy: 0.3702\n",
      "epoch : 300 = >  Accuracy: 0.4256\n",
      "epoch : 350 = >  Accuracy: 0.5166\n",
      "epoch : 400 = >  Accuracy: 0.5978\n",
      "epoch : 450 = >  Accuracy: 0.6507\n",
      "epoch : 500 = >  Accuracy: 0.7232\n",
      "epoch : 550 = >  Accuracy: 0.7860\n",
      "epoch : 600 = >  Accuracy: 0.8167\n",
      "epoch : 650 = >  Accuracy: 0.8376\n",
      "epoch : 700 = >  Accuracy: 0.8512\n",
      "Final accuracy: 0.8512\n",
      "epoch 0 ,Accuracy: 0.3149 , train acc : 0.3064992614475628\n",
      "epoch 50 ,Accuracy: 0.7454 , train acc : 0.757754800590842\n",
      "epoch 100 ,Accuracy: 0.8216 , train acc : 0.8367799113737076\n",
      "epoch 150 ,Accuracy: 0.8180 , train acc : 0.8493353028064993\n",
      "epoch 200 ,Accuracy: 0.8130 , train acc : 0.8500738552437223\n",
      "epoch 250 ,Accuracy: 0.8204 , train acc : 0.8522895125553914\n",
      "epoch 300 ,Accuracy: 0.8192 , train acc : 0.8530280649926145\n",
      "epoch 350 ,Accuracy: 0.8241 , train acc : 0.8559822747415067\n",
      "epoch 400 ,Accuracy: 0.8782 , train acc : 0.9187592319054653\n",
      "epoch 450 ,Accuracy: 0.8807 , train acc : 0.9261447562776958\n",
      "epoch 500 ,Accuracy: 0.8758 , train acc : 0.9231905465288035\n",
      "epoch 550 ,Accuracy: 0.8807 , train acc : 0.9298375184638109\n",
      "epoch 600 ,Accuracy: 0.8844 , train acc : 0.9290989660265879\n",
      "epoch 650 ,Accuracy: 0.8758 , train acc : 0.9320531757754801\n",
      "epoch 700 ,Accuracy: 0.8893 , train acc : 0.931314623338257\n",
      "epoch 750 ,Accuracy: 0.8770 , train acc : 0.9335302806499262\n",
      "epoch 800 ,Accuracy: 0.8831 , train acc : 0.9394387001477105\n",
      "epoch 850 ,Accuracy: 0.8745 , train acc : 0.9431314623338257\n",
      "epoch 900 ,Accuracy: 0.8844 , train acc : 0.9394387001477105\n",
      "epoch 950 ,Accuracy: 0.8770 , train acc : 0.9416543574593796\n",
      "epoch 1000 ,Accuracy: 0.8807 , train acc : 0.9438700147710487\n",
      "epoch 1050 ,Accuracy: 0.8819 , train acc : 0.9453471196454948\n",
      "epoch 1100 ,Accuracy: 0.8819 , train acc : 0.946824224519941\n",
      "epoch 1150 ,Accuracy: 0.8758 , train acc : 0.9556868537666174\n",
      "epoch 1200 ,Accuracy: 0.8795 , train acc : 0.9549483013293943\n",
      "epoch 1250 ,Accuracy: 0.8770 , train acc : 0.9571639586410635\n",
      "epoch 1300 ,Accuracy: 0.8819 , train acc : 0.9638109305760709\n",
      "epoch 1350 ,Accuracy: 0.8807 , train acc : 0.9645494830132939\n",
      "epoch 1400 ,Accuracy: 0.8782 , train acc : 0.9704579025110783\n",
      "epoch 1450 ,Accuracy: 0.8856 , train acc : 0.96602658788774\n",
      "epoch 1500 ,Accuracy: 0.8819 , train acc : 0.9586410635155096\n",
      "epoch 1550 ,Accuracy: 0.8758 , train acc : 0.9741506646971935\n",
      "epoch 1600 ,Accuracy: 0.8831 , train acc : 0.965288035450517\n",
      "epoch 1650 ,Accuracy: 0.8782 , train acc : 0.9785819793205317\n",
      "epoch 1700 ,Accuracy: 0.8758 , train acc : 0.9748892171344166\n",
      "epoch 1750 ,Accuracy: 0.8708 , train acc : 0.9778434268833087\n",
      "epoch 1800 ,Accuracy: 0.8782 , train acc : 0.9807976366322009\n",
      "epoch 1850 ,Accuracy: 0.8856 , train acc : 0.9719350073855244\n",
      "epoch 1900 ,Accuracy: 0.8782 , train acc : 0.9763663220088626\n",
      "epoch 1950 ,Accuracy: 0.8819 , train acc : 0.9852289512555391\n",
      "epoch 2000 ,Accuracy: 0.8745 , train acc : 0.9844903988183161\n",
      "epoch 2050 ,Accuracy: 0.8745 , train acc : 0.9874446085672083\n",
      "epoch 2100 ,Accuracy: 0.8819 , train acc : 0.98301329394387\n",
      "epoch 2150 ,Accuracy: 0.8819 , train acc : 0.9852289512555391\n",
      "epoch 2200 ,Accuracy: 0.8819 , train acc : 0.9881831610044313\n",
      "epoch 2250 ,Accuracy: 0.8733 , train acc : 0.9867060561299852\n",
      "epoch 2300 ,Accuracy: 0.8831 , train acc : 0.9918759231905465\n",
      "epoch 2350 ,Accuracy: 0.8770 , train acc : 0.9896602658788775\n",
      "epoch 2400 ,Accuracy: 0.8770 , train acc : 0.9940915805022157\n",
      "epoch 2450 ,Accuracy: 0.8733 , train acc : 0.9918759231905465\n",
      "epoch 2500 ,Accuracy: 0.8782 , train acc : 0.9911373707533235\n",
      "epoch 2550 ,Accuracy: 0.8770 , train acc : 0.9926144756277696\n",
      "epoch 2600 ,Accuracy: 0.8733 , train acc : 0.9963072378138847\n",
      "epoch 2650 ,Accuracy: 0.8758 , train acc : 0.9940915805022157\n",
      "epoch 2700 ,Accuracy: 0.8831 , train acc : 0.9940915805022157\n",
      "epoch 2750 ,Accuracy: 0.8721 , train acc : 0.9985228951255539\n",
      "epoch 2800 ,Accuracy: 0.8795 , train acc : 0.9963072378138847\n",
      "epoch 2850 ,Accuracy: 0.8831 , train acc : 0.9977843426883308\n",
      "epoch 2900 ,Accuracy: 0.8770 , train acc : 0.9933530280649926\n",
      "epoch 2950 ,Accuracy: 0.8819 , train acc : 0.9963072378138847\n",
      "Accuracy: 0.8721\n",
      "base model\n",
      "0.8511685116851169\n",
      "0.8245549233369756\n",
      "OurModel\n",
      "0.8720787207872078\n",
      "0.8539798875521868\n",
      "*****\n",
      "epoch : 10 / 10\n",
      "epoch : 50 = >  Accuracy: 0.3862\n",
      "epoch : 100 = >  Accuracy: 0.3678\n",
      "epoch : 150 = >  Accuracy: 0.3567\n",
      "epoch : 200 = >  Accuracy: 0.3579\n",
      "epoch : 250 = >  Accuracy: 0.3715\n",
      "epoch : 300 = >  Accuracy: 0.4280\n",
      "epoch : 350 = >  Accuracy: 0.5154\n",
      "epoch : 400 = >  Accuracy: 0.5978\n",
      "epoch : 450 = >  Accuracy: 0.6519\n",
      "epoch : 500 = >  Accuracy: 0.7208\n",
      "epoch : 550 = >  Accuracy: 0.7860\n",
      "epoch : 600 = >  Accuracy: 0.8180\n",
      "epoch : 650 = >  Accuracy: 0.8389\n",
      "epoch : 700 = >  Accuracy: 0.8512\n",
      "Final accuracy: 0.8512\n",
      "epoch 0 ,Accuracy: 0.2485 , train acc : 0.23412112259970458\n",
      "epoch 50 ,Accuracy: 0.7368 , train acc : 0.7555391432791728\n",
      "epoch 100 ,Accuracy: 0.7675 , train acc : 0.7865583456425406\n",
      "epoch 150 ,Accuracy: 0.7724 , train acc : 0.7895125553914328\n",
      "epoch 200 ,Accuracy: 0.7737 , train acc : 0.793205317577548\n",
      "epoch 250 ,Accuracy: 0.7712 , train acc : 0.7976366322008862\n",
      "epoch 300 ,Accuracy: 0.7724 , train acc : 0.7998522895125554\n",
      "epoch 350 ,Accuracy: 0.7761 , train acc : 0.7976366322008862\n",
      "epoch 400 ,Accuracy: 0.8241 , train acc : 0.8626292466765141\n",
      "epoch 450 ,Accuracy: 0.8253 , train acc : 0.8670605612998523\n",
      "epoch 500 ,Accuracy: 0.8327 , train acc : 0.8663220088626292\n",
      "epoch 550 ,Accuracy: 0.8303 , train acc : 0.8663220088626292\n",
      "epoch 600 ,Accuracy: 0.8303 , train acc : 0.8751846381093058\n",
      "epoch 650 ,Accuracy: 0.8376 , train acc : 0.8729689807976366\n",
      "epoch 700 ,Accuracy: 0.8376 , train acc : 0.8744460856720827\n",
      "epoch 750 ,Accuracy: 0.8303 , train acc : 0.8788774002954209\n",
      "epoch 800 ,Accuracy: 0.8303 , train acc : 0.878138847858198\n",
      "epoch 850 ,Accuracy: 0.8290 , train acc : 0.8810930576070901\n",
      "epoch 900 ,Accuracy: 0.8278 , train acc : 0.8833087149187593\n",
      "epoch 950 ,Accuracy: 0.8253 , train acc : 0.8855243722304283\n",
      "epoch 1000 ,Accuracy: 0.8290 , train acc : 0.8899556868537666\n",
      "epoch 1050 ,Accuracy: 0.8721 , train acc : 0.9387001477104875\n",
      "epoch 1100 ,Accuracy: 0.8807 , train acc : 0.9409158050221565\n",
      "epoch 1150 ,Accuracy: 0.8745 , train acc : 0.9460856720827179\n",
      "epoch 1200 ,Accuracy: 0.8819 , train acc : 0.9497784342688331\n",
      "epoch 1250 ,Accuracy: 0.8831 , train acc : 0.9519940915805022\n",
      "epoch 1300 ,Accuracy: 0.8856 , train acc : 0.9549483013293943\n",
      "epoch 1350 ,Accuracy: 0.8758 , train acc : 0.9564254062038404\n",
      "epoch 1400 ,Accuracy: 0.8844 , train acc : 0.9638109305760709\n",
      "epoch 1450 ,Accuracy: 0.8672 , train acc : 0.9586410635155096\n",
      "epoch 1500 ,Accuracy: 0.8893 , train acc : 0.9608567208271788\n",
      "epoch 1550 ,Accuracy: 0.8795 , train acc : 0.9682422451994092\n",
      "epoch 1600 ,Accuracy: 0.8844 , train acc : 0.9645494830132939\n",
      "epoch 1650 ,Accuracy: 0.8831 , train acc : 0.9667651403249631\n",
      "epoch 1700 ,Accuracy: 0.8733 , train acc : 0.965288035450517\n",
      "epoch 1750 ,Accuracy: 0.8745 , train acc : 0.9734121122599705\n",
      "epoch 1800 ,Accuracy: 0.8795 , train acc : 0.9734121122599705\n",
      "epoch 1850 ,Accuracy: 0.8893 , train acc : 0.9778434268833087\n",
      "epoch 1900 ,Accuracy: 0.8770 , train acc : 0.9793205317577548\n",
      "epoch 1950 ,Accuracy: 0.8758 , train acc : 0.9807976366322009\n",
      "epoch 2000 ,Accuracy: 0.8770 , train acc : 0.9807976366322009\n",
      "epoch 2050 ,Accuracy: 0.8708 , train acc : 0.9807976366322009\n",
      "epoch 2100 ,Accuracy: 0.8782 , train acc : 0.9844903988183161\n",
      "epoch 2150 ,Accuracy: 0.8745 , train acc : 0.9800590841949779\n",
      "epoch 2200 ,Accuracy: 0.8782 , train acc : 0.98301329394387\n",
      "epoch 2250 ,Accuracy: 0.8758 , train acc : 0.9874446085672083\n",
      "epoch 2300 ,Accuracy: 0.8708 , train acc : 0.9867060561299852\n",
      "epoch 2350 ,Accuracy: 0.8807 , train acc : 0.9874446085672083\n",
      "epoch 2400 ,Accuracy: 0.8770 , train acc : 0.9940915805022157\n",
      "epoch 2450 ,Accuracy: 0.8807 , train acc : 0.9867060561299852\n",
      "epoch 2500 ,Accuracy: 0.8721 , train acc : 0.9926144756277696\n",
      "epoch 2550 ,Accuracy: 0.8721 , train acc : 0.9926144756277696\n",
      "epoch 2600 ,Accuracy: 0.8758 , train acc : 0.9903988183161004\n",
      "epoch 2650 ,Accuracy: 0.8807 , train acc : 0.9918759231905465\n",
      "epoch 2700 ,Accuracy: 0.8721 , train acc : 0.9903988183161004\n",
      "epoch 2750 ,Accuracy: 0.8758 , train acc : 0.9933530280649926\n",
      "epoch 2800 ,Accuracy: 0.8782 , train acc : 0.9933530280649926\n",
      "epoch 2850 ,Accuracy: 0.8733 , train acc : 0.9940915805022157\n",
      "epoch 2900 ,Accuracy: 0.8745 , train acc : 0.9933530280649926\n",
      "epoch 2950 ,Accuracy: 0.8721 , train acc : 0.9940915805022157\n",
      "Accuracy: 0.8807\n",
      "base model\n",
      "0.8511685116851169\n",
      "0.825252218193612\n",
      "OurModel\n",
      "0.8806888068880688\n",
      "0.8714611349165376\n",
      "+++***Fianal Result***+++\n",
      "base\n",
      "Accuracy avg = 0.8511685116851169\n",
      "Accuracy deviation = 0.0\n",
      "F1 score(macro) avg = 0.8247641117939665\n",
      "F1 score(macro) deviation = 0.00033682541568998863\n",
      "new model\n",
      "Accuracy avg = 0.8765067650676507\n",
      "Accuracy deviation = 0.004681952414471569\n",
      "F1 score(macro) avg = 0.8622022560204707\n",
      "F1 score(macro) deviation = 0.006570252759321471\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "GNN_model = GCN(64, dataset.num_classes).to(device)\n",
    "run_model(GNN_model , Gnn_epochs=900 ,enc_address=\"/content/encoder_emb64_cora.pt\" ,reply_threshold=0.95 , head_epochs= 1500 , validation_mask=new_valdation_mask , early_stop=0.89 )"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BvFiAIc4A-g7",
    "outputId": "ebc209aa-9e1c-4188-9fa4-5b463a9f7563",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "*****\n",
      "epoch : 1 / 10\n",
      "epoch : 50 = >  Accuracy: 0.5375\n",
      "epoch : 100 = >  Accuracy: 0.6162\n",
      "epoch : 150 = >  Accuracy: 0.7146\n",
      "epoch : 200 = >  Accuracy: 0.7663\n",
      "epoch : 250 = >  Accuracy: 0.7970\n",
      "epoch : 300 = >  Accuracy: 0.8093\n",
      "epoch : 350 = >  Accuracy: 0.8216\n",
      "epoch : 400 = >  Accuracy: 0.8549\n",
      "epoch : 450 = >  Accuracy: 0.8672\n",
      "epoch : 500 = >  Accuracy: 0.8819\n",
      "epoch : 550 = >  Accuracy: 0.8856\n",
      "epoch : 600 = >  Accuracy: 0.8881\n",
      "epoch : 650 = >  Accuracy: 0.8918\n",
      "epoch : 700 = >  Accuracy: 0.8918\n",
      "epoch : 750 = >  Accuracy: 0.8881\n",
      "epoch : 800 = >  Accuracy: 0.8881\n",
      "epoch : 850 = >  Accuracy: 0.8881\n",
      "epoch : 900 = >  Accuracy: 0.8868\n",
      "Final accuracy: 0.8868\n",
      "epoch 0 ,Accuracy: 0.4280 , train acc : 0.41432791728212703\n",
      "epoch 50 ,Accuracy: 0.8303 , train acc : 0.8899556868537666\n",
      "epoch 100 ,Accuracy: 0.8290 , train acc : 0.8943870014771049\n",
      "epoch 150 ,Accuracy: 0.8278 , train acc : 0.9010339734121122\n",
      "epoch 200 ,Accuracy: 0.8315 , train acc : 0.9076809453471196\n",
      "epoch 250 ,Accuracy: 0.8315 , train acc : 0.9084194977843427\n",
      "epoch 300 ,Accuracy: 0.8180 , train acc : 0.9113737075332349\n",
      "epoch 350 ,Accuracy: 0.8192 , train acc : 0.9135893648449039\n",
      "epoch 400 ,Accuracy: 0.8180 , train acc : 0.9224519940915805\n",
      "epoch 450 ,Accuracy: 0.8192 , train acc : 0.9231905465288035\n",
      "epoch 500 ,Accuracy: 0.8241 , train acc : 0.930576070901034\n",
      "epoch 550 ,Accuracy: 0.8303 , train acc : 0.9268833087149188\n",
      "epoch 600 ,Accuracy: 0.8130 , train acc : 0.930576070901034\n",
      "epoch 650 ,Accuracy: 0.8721 , train acc : 0.9903988183161004\n",
      "epoch 700 ,Accuracy: 0.8831 , train acc : 0.9948301329394387\n",
      "epoch 750 ,Accuracy: 0.8807 , train acc : 0.9963072378138847\n",
      "epoch 800 ,Accuracy: 0.8708 , train acc : 0.9970457902511078\n",
      "epoch 850 ,Accuracy: 0.8807 , train acc : 0.9970457902511078\n",
      "epoch 900 ,Accuracy: 0.8672 , train acc : 0.9963072378138847\n",
      "epoch 950 ,Accuracy: 0.8770 , train acc : 0.9977843426883308\n",
      "epoch 1000 ,Accuracy: 0.8782 , train acc : 0.9963072378138847\n",
      "epoch 1050 ,Accuracy: 0.8733 , train acc : 0.9985228951255539\n",
      "epoch 1100 ,Accuracy: 0.8696 , train acc : 0.9985228951255539\n",
      "epoch 1150 ,Accuracy: 0.8696 , train acc : 0.9977843426883308\n",
      "epoch 1200 ,Accuracy: 0.8795 , train acc : 0.9963072378138847\n",
      "epoch 1250 ,Accuracy: 0.8696 , train acc : 0.9985228951255539\n",
      "epoch 1300 ,Accuracy: 0.8721 , train acc : 0.9977843426883308\n",
      "epoch 1350 ,Accuracy: 0.8782 , train acc : 0.9985228951255539\n",
      "epoch 1400 ,Accuracy: 0.8684 , train acc : 0.9977843426883308\n",
      "epoch 1450 ,Accuracy: 0.8684 , train acc : 0.9985228951255539\n",
      "Accuracy: 0.8721\n",
      "base model\n",
      "0.8868388683886839\n",
      "0.8796867502201875\n",
      "OurModel\n",
      "0.8720787207872078\n",
      "0.8615447718653726\n",
      "*****\n",
      "epoch : 2 / 10\n",
      "epoch : 50 = >  Accuracy: 0.5400\n",
      "epoch : 100 = >  Accuracy: 0.6162\n",
      "epoch : 150 = >  Accuracy: 0.7134\n",
      "epoch : 200 = >  Accuracy: 0.7638\n",
      "epoch : 250 = >  Accuracy: 0.7970\n",
      "epoch : 300 = >  Accuracy: 0.8106\n",
      "epoch : 350 = >  Accuracy: 0.8462\n",
      "epoch : 400 = >  Accuracy: 0.8573\n",
      "epoch : 450 = >  Accuracy: 0.8721\n",
      "epoch : 500 = >  Accuracy: 0.8807\n",
      "epoch : 550 = >  Accuracy: 0.8881\n",
      "epoch : 600 = >  Accuracy: 0.8868\n",
      "epoch : 650 = >  Accuracy: 0.8881\n",
      "epoch : 700 = >  Accuracy: 0.8918\n",
      "epoch : 750 = >  Accuracy: 0.8881\n",
      "epoch : 800 = >  Accuracy: 0.8881\n",
      "epoch : 850 = >  Accuracy: 0.8881\n",
      "epoch : 900 = >  Accuracy: 0.8868\n",
      "Final accuracy: 0.8868\n",
      "epoch 0 ,Accuracy: 0.2669 , train acc : 0.29394387001477107\n",
      "epoch 50 ,Accuracy: 0.7675 , train acc : 0.8190546528803545\n",
      "epoch 100 ,Accuracy: 0.8266 , train acc : 0.8722304283604135\n",
      "epoch 150 ,Accuracy: 0.8303 , train acc : 0.8892171344165436\n",
      "epoch 200 ,Accuracy: 0.8315 , train acc : 0.8862629246676514\n",
      "epoch 250 ,Accuracy: 0.8266 , train acc : 0.8906942392909897\n",
      "epoch 300 ,Accuracy: 0.8155 , train acc : 0.8951255539143279\n",
      "epoch 350 ,Accuracy: 0.8204 , train acc : 0.8973412112259971\n",
      "epoch 400 ,Accuracy: 0.8167 , train acc : 0.9032496307237814\n",
      "epoch 450 ,Accuracy: 0.8167 , train acc : 0.9032496307237814\n",
      "epoch 500 ,Accuracy: 0.8192 , train acc : 0.9069423929098966\n",
      "epoch 550 ,Accuracy: 0.8204 , train acc : 0.9098966026587888\n",
      "epoch 600 ,Accuracy: 0.8155 , train acc : 0.9135893648449039\n",
      "epoch 650 ,Accuracy: 0.8155 , train acc : 0.9135893648449039\n",
      "epoch 700 ,Accuracy: 0.8167 , train acc : 0.9135893648449039\n",
      "epoch 750 ,Accuracy: 0.8007 , train acc : 0.914327917282127\n",
      "epoch 800 ,Accuracy: 0.8057 , train acc : 0.9158050221565731\n",
      "epoch 850 ,Accuracy: 0.8118 , train acc : 0.9165435745937962\n",
      "epoch 900 ,Accuracy: 0.8093 , train acc : 0.9158050221565731\n",
      "epoch 950 ,Accuracy: 0.8130 , train acc : 0.9172821270310192\n",
      "epoch 1000 ,Accuracy: 0.8057 , train acc : 0.9172821270310192\n",
      "epoch 1050 ,Accuracy: 0.8143 , train acc : 0.9165435745937962\n",
      "epoch 1100 ,Accuracy: 0.8167 , train acc : 0.9172821270310192\n",
      "epoch 1150 ,Accuracy: 0.8093 , train acc : 0.914327917282127\n",
      "epoch 1200 ,Accuracy: 0.8130 , train acc : 0.9172821270310192\n",
      "epoch 1250 ,Accuracy: 0.8167 , train acc : 0.9158050221565731\n",
      "epoch 1300 ,Accuracy: 0.8093 , train acc : 0.9172821270310192\n",
      "epoch 1350 ,Accuracy: 0.8106 , train acc : 0.9180206794682423\n",
      "epoch 1400 ,Accuracy: 0.8093 , train acc : 0.9172821270310192\n",
      "epoch 1450 ,Accuracy: 0.8167 , train acc : 0.9180206794682423\n",
      "epoch 0 ,Accuracy: 0.2522 , train acc : 0.28064992614475626\n",
      "epoch 50 ,Accuracy: 0.8229 , train acc : 0.8670605612998523\n",
      "epoch 100 ,Accuracy: 0.8266 , train acc : 0.878138847858198\n",
      "epoch 150 ,Accuracy: 0.8167 , train acc : 0.8847858197932054\n",
      "epoch 200 ,Accuracy: 0.8216 , train acc : 0.8855243722304283\n",
      "epoch 250 ,Accuracy: 0.8229 , train acc : 0.8943870014771049\n",
      "epoch 300 ,Accuracy: 0.8180 , train acc : 0.8929098966026587\n",
      "epoch 350 ,Accuracy: 0.8229 , train acc : 0.9010339734121122\n",
      "epoch 400 ,Accuracy: 0.8155 , train acc : 0.9032496307237814\n",
      "epoch 450 ,Accuracy: 0.8204 , train acc : 0.9106351550960118\n",
      "epoch 500 ,Accuracy: 0.8081 , train acc : 0.9091580502215657\n",
      "epoch 550 ,Accuracy: 0.8167 , train acc : 0.9084194977843427\n",
      "epoch 600 ,Accuracy: 0.8167 , train acc : 0.9135893648449039\n",
      "epoch 650 ,Accuracy: 0.8106 , train acc : 0.9135893648449039\n",
      "epoch 700 ,Accuracy: 0.8241 , train acc : 0.9158050221565731\n",
      "epoch 750 ,Accuracy: 0.8106 , train acc : 0.914327917282127\n",
      "epoch 800 ,Accuracy: 0.8118 , train acc : 0.9158050221565731\n",
      "epoch 850 ,Accuracy: 0.8093 , train acc : 0.9150664697193501\n",
      "epoch 900 ,Accuracy: 0.8106 , train acc : 0.9150664697193501\n",
      "epoch 950 ,Accuracy: 0.8106 , train acc : 0.9158050221565731\n",
      "epoch 1000 ,Accuracy: 0.8081 , train acc : 0.9165435745937962\n",
      "epoch 1050 ,Accuracy: 0.8143 , train acc : 0.9172821270310192\n",
      "epoch 1100 ,Accuracy: 0.8032 , train acc : 0.9172821270310192\n",
      "epoch 1150 ,Accuracy: 0.8093 , train acc : 0.9165435745937962\n",
      "epoch 1200 ,Accuracy: 0.8093 , train acc : 0.9180206794682423\n",
      "epoch 1250 ,Accuracy: 0.8167 , train acc : 0.9165435745937962\n",
      "epoch 1300 ,Accuracy: 0.8057 , train acc : 0.9180206794682423\n",
      "epoch 1350 ,Accuracy: 0.8081 , train acc : 0.9172821270310192\n",
      "epoch 1400 ,Accuracy: 0.8081 , train acc : 0.9172821270310192\n",
      "epoch 1450 ,Accuracy: 0.8118 , train acc : 0.9172821270310192\n",
      "epoch 0 ,Accuracy: 0.4010 , train acc : 0.3929098966026588\n",
      "epoch 50 ,Accuracy: 0.7995 , train acc : 0.8441654357459379\n",
      "epoch 100 ,Accuracy: 0.7958 , train acc : 0.8485967503692762\n",
      "epoch 150 ,Accuracy: 0.7970 , train acc : 0.8574593796159528\n",
      "epoch 200 ,Accuracy: 0.8807 , train acc : 0.9534711964549483\n",
      "epoch 250 ,Accuracy: 0.8881 , train acc : 0.9697193500738552\n",
      "epoch 300 ,Accuracy: 0.8856 , train acc : 0.9682422451994092\n",
      "epoch 350 ,Accuracy: 0.8844 , train acc : 0.9763663220088626\n",
      "epoch 400 ,Accuracy: 0.8831 , train acc : 0.9807976366322009\n",
      "epoch 450 ,Accuracy: 0.8782 , train acc : 0.9867060561299852\n",
      "epoch 500 ,Accuracy: 0.8733 , train acc : 0.9896602658788775\n",
      "epoch 550 ,Accuracy: 0.8795 , train acc : 0.9911373707533235\n",
      "epoch 600 ,Accuracy: 0.8745 , train acc : 0.9948301329394387\n",
      "epoch 650 ,Accuracy: 0.8807 , train acc : 0.9955686853766618\n",
      "epoch 700 ,Accuracy: 0.8745 , train acc : 0.9977843426883308\n",
      "epoch 750 ,Accuracy: 0.8758 , train acc : 0.9963072378138847\n",
      "epoch 800 ,Accuracy: 0.8795 , train acc : 0.9955686853766618\n",
      "epoch 850 ,Accuracy: 0.8807 , train acc : 0.9985228951255539\n",
      "epoch 900 ,Accuracy: 0.8721 , train acc : 0.9977843426883308\n",
      "epoch 950 ,Accuracy: 0.8782 , train acc : 0.9970457902511078\n",
      "epoch 1000 ,Accuracy: 0.8770 , train acc : 0.9977843426883308\n",
      "epoch 1050 ,Accuracy: 0.8795 , train acc : 0.9985228951255539\n",
      "epoch 1100 ,Accuracy: 0.8745 , train acc : 0.9970457902511078\n",
      "epoch 1150 ,Accuracy: 0.8708 , train acc : 0.9985228951255539\n",
      "epoch 1200 ,Accuracy: 0.8745 , train acc : 0.9985228951255539\n",
      "epoch 1250 ,Accuracy: 0.8831 , train acc : 0.9977843426883308\n",
      "epoch 1300 ,Accuracy: 0.8733 , train acc : 0.9985228951255539\n",
      "epoch 1350 ,Accuracy: 0.8721 , train acc : 0.9985228951255539\n",
      "epoch 1400 ,Accuracy: 0.8721 , train acc : 0.9992614475627769\n",
      "epoch 1450 ,Accuracy: 0.8844 , train acc : 0.9985228951255539\n",
      "Accuracy: 0.8770\n",
      "base model\n",
      "0.8868388683886839\n",
      "0.8796867502201875\n",
      "OurModel\n",
      "0.8769987699876999\n",
      "0.8667559174978623\n",
      "*****\n",
      "epoch : 3 / 10\n",
      "epoch : 50 = >  Accuracy: 0.5387\n",
      "epoch : 100 = >  Accuracy: 0.6175\n",
      "epoch : 150 = >  Accuracy: 0.7134\n",
      "epoch : 200 = >  Accuracy: 0.7663\n",
      "epoch : 250 = >  Accuracy: 0.7958\n",
      "epoch : 300 = >  Accuracy: 0.8093\n",
      "epoch : 350 = >  Accuracy: 0.8450\n",
      "epoch : 400 = >  Accuracy: 0.8573\n",
      "epoch : 450 = >  Accuracy: 0.8745\n",
      "epoch : 500 = >  Accuracy: 0.8819\n",
      "epoch : 550 = >  Accuracy: 0.8868\n",
      "epoch : 600 = >  Accuracy: 0.8868\n",
      "epoch : 650 = >  Accuracy: 0.8881\n",
      "epoch : 700 = >  Accuracy: 0.8918\n",
      "epoch : 750 = >  Accuracy: 0.8881\n",
      "epoch : 800 = >  Accuracy: 0.8881\n",
      "epoch : 850 = >  Accuracy: 0.8881\n",
      "epoch : 900 = >  Accuracy: 0.8868\n",
      "Final accuracy: 0.8868\n",
      "epoch 0 ,Accuracy: 0.3149 , train acc : 0.3005908419497784\n",
      "epoch 50 ,Accuracy: 0.7589 , train acc : 0.7961595273264401\n",
      "epoch 100 ,Accuracy: 0.7577 , train acc : 0.7983751846381093\n",
      "epoch 150 ,Accuracy: 0.7601 , train acc : 0.8020679468242246\n",
      "epoch 200 ,Accuracy: 0.7540 , train acc : 0.8064992614475628\n",
      "epoch 250 ,Accuracy: 0.7577 , train acc : 0.8064992614475628\n",
      "epoch 300 ,Accuracy: 0.7565 , train acc : 0.8131462333825702\n",
      "epoch 350 ,Accuracy: 0.7601 , train acc : 0.8190546528803545\n",
      "epoch 400 ,Accuracy: 0.8782 , train acc : 0.9771048744460856\n",
      "epoch 450 ,Accuracy: 0.8758 , train acc : 0.9859675036927622\n",
      "epoch 500 ,Accuracy: 0.8782 , train acc : 0.9903988183161004\n",
      "epoch 550 ,Accuracy: 0.8807 , train acc : 0.9903988183161004\n",
      "epoch 600 ,Accuracy: 0.8696 , train acc : 0.9940915805022157\n",
      "epoch 650 ,Accuracy: 0.8745 , train acc : 0.9940915805022157\n",
      "epoch 700 ,Accuracy: 0.8782 , train acc : 0.9948301329394387\n",
      "epoch 750 ,Accuracy: 0.8708 , train acc : 0.9955686853766618\n",
      "epoch 800 ,Accuracy: 0.8758 , train acc : 0.9970457902511078\n",
      "epoch 850 ,Accuracy: 0.8733 , train acc : 0.9963072378138847\n",
      "epoch 900 ,Accuracy: 0.8733 , train acc : 0.9955686853766618\n",
      "epoch 950 ,Accuracy: 0.8721 , train acc : 0.9963072378138847\n",
      "epoch 1000 ,Accuracy: 0.8672 , train acc : 0.9977843426883308\n",
      "epoch 1050 ,Accuracy: 0.8758 , train acc : 0.9970457902511078\n",
      "epoch 1100 ,Accuracy: 0.8758 , train acc : 0.9948301329394387\n",
      "epoch 1150 ,Accuracy: 0.8696 , train acc : 0.9970457902511078\n",
      "epoch 1200 ,Accuracy: 0.8721 , train acc : 0.9977843426883308\n",
      "epoch 1250 ,Accuracy: 0.8770 , train acc : 0.9977843426883308\n",
      "epoch 1300 ,Accuracy: 0.8795 , train acc : 0.9970457902511078\n",
      "epoch 1350 ,Accuracy: 0.8733 , train acc : 0.9970457902511078\n",
      "epoch 1400 ,Accuracy: 0.8795 , train acc : 0.9977843426883308\n",
      "epoch 1450 ,Accuracy: 0.8770 , train acc : 0.9977843426883308\n",
      "Accuracy: 0.8795\n",
      "base model\n",
      "0.8868388683886839\n",
      "0.8796867502201875\n",
      "OurModel\n",
      "0.8794587945879458\n",
      "0.8697559014897944\n",
      "*****\n",
      "epoch : 4 / 10\n",
      "epoch : 50 = >  Accuracy: 0.5424\n",
      "epoch : 100 = >  Accuracy: 0.6175\n",
      "epoch : 150 = >  Accuracy: 0.7134\n",
      "epoch : 200 = >  Accuracy: 0.7663\n",
      "epoch : 250 = >  Accuracy: 0.7958\n",
      "epoch : 300 = >  Accuracy: 0.8118\n",
      "epoch : 350 = >  Accuracy: 0.8438\n",
      "epoch : 400 = >  Accuracy: 0.8573\n",
      "epoch : 450 = >  Accuracy: 0.8721\n",
      "epoch : 500 = >  Accuracy: 0.8807\n",
      "epoch : 550 = >  Accuracy: 0.8856\n",
      "epoch : 600 = >  Accuracy: 0.8868\n",
      "epoch : 650 = >  Accuracy: 0.8881\n",
      "epoch : 700 = >  Accuracy: 0.8918\n",
      "epoch : 750 = >  Accuracy: 0.8881\n",
      "epoch : 800 = >  Accuracy: 0.8881\n",
      "epoch : 850 = >  Accuracy: 0.8881\n",
      "epoch : 900 = >  Accuracy: 0.8868\n",
      "Final accuracy: 0.8868\n",
      "epoch 0 ,Accuracy: 0.5228 , train acc : 0.5450516986706057\n",
      "epoch 50 ,Accuracy: 0.8253 , train acc : 0.896602658788774\n",
      "epoch 100 ,Accuracy: 0.8229 , train acc : 0.8921713441654358\n",
      "epoch 150 ,Accuracy: 0.8241 , train acc : 0.9017725258493353\n",
      "epoch 200 ,Accuracy: 0.8868 , train acc : 0.9593796159527327\n",
      "epoch 250 ,Accuracy: 0.8868 , train acc : 0.9704579025110783\n",
      "epoch 300 ,Accuracy: 0.8807 , train acc : 0.9711964549483013\n",
      "epoch 350 ,Accuracy: 0.8856 , train acc : 0.9771048744460856\n",
      "epoch 400 ,Accuracy: 0.8795 , train acc : 0.9859675036927622\n",
      "epoch 450 ,Accuracy: 0.8770 , train acc : 0.9852289512555391\n",
      "epoch 500 ,Accuracy: 0.8856 , train acc : 0.9867060561299852\n",
      "epoch 550 ,Accuracy: 0.8770 , train acc : 0.9933530280649926\n",
      "epoch 600 ,Accuracy: 0.8770 , train acc : 0.9933530280649926\n",
      "epoch 650 ,Accuracy: 0.8733 , train acc : 0.9970457902511078\n",
      "epoch 700 ,Accuracy: 0.8721 , train acc : 0.9948301329394387\n",
      "epoch 750 ,Accuracy: 0.8831 , train acc : 0.9970457902511078\n",
      "epoch 800 ,Accuracy: 0.8807 , train acc : 0.9955686853766618\n",
      "epoch 850 ,Accuracy: 0.8807 , train acc : 0.9970457902511078\n",
      "epoch 900 ,Accuracy: 0.8795 , train acc : 0.9955686853766618\n",
      "epoch 950 ,Accuracy: 0.8733 , train acc : 0.9963072378138847\n",
      "epoch 1000 ,Accuracy: 0.8807 , train acc : 0.9985228951255539\n",
      "epoch 1050 ,Accuracy: 0.8770 , train acc : 0.9963072378138847\n",
      "epoch 1100 ,Accuracy: 0.8807 , train acc : 0.9992614475627769\n",
      "epoch 1150 ,Accuracy: 0.8758 , train acc : 0.9977843426883308\n",
      "epoch 1200 ,Accuracy: 0.8758 , train acc : 0.9985228951255539\n",
      "epoch 1250 ,Accuracy: 0.8758 , train acc : 0.9977843426883308\n",
      "epoch 1300 ,Accuracy: 0.8721 , train acc : 0.9992614475627769\n",
      "epoch 1350 ,Accuracy: 0.8782 , train acc : 0.9992614475627769\n",
      "epoch 1400 ,Accuracy: 0.8782 , train acc : 1.0\n",
      "epoch 1450 ,Accuracy: 0.8856 , train acc : 1.0\n",
      "Accuracy: 0.8733\n",
      "base model\n",
      "0.8868388683886839\n",
      "0.8796867502201875\n",
      "OurModel\n",
      "0.8733087330873309\n",
      "0.8629828300024391\n",
      "*****\n",
      "epoch : 5 / 10\n",
      "epoch : 50 = >  Accuracy: 0.5375\n",
      "epoch : 100 = >  Accuracy: 0.6175\n",
      "epoch : 150 = >  Accuracy: 0.7134\n",
      "epoch : 200 = >  Accuracy: 0.7663\n",
      "epoch : 250 = >  Accuracy: 0.7983\n",
      "epoch : 300 = >  Accuracy: 0.8081\n",
      "epoch : 350 = >  Accuracy: 0.8241\n",
      "epoch : 400 = >  Accuracy: 0.8561\n",
      "epoch : 450 = >  Accuracy: 0.8672\n",
      "epoch : 500 = >  Accuracy: 0.8819\n",
      "epoch : 550 = >  Accuracy: 0.8856\n",
      "epoch : 600 = >  Accuracy: 0.8868\n",
      "epoch : 650 = >  Accuracy: 0.8881\n",
      "epoch : 700 = >  Accuracy: 0.8918\n",
      "epoch : 750 = >  Accuracy: 0.8881\n",
      "epoch : 800 = >  Accuracy: 0.8881\n",
      "epoch : 850 = >  Accuracy: 0.8881\n",
      "epoch : 900 = >  Accuracy: 0.8868\n",
      "Final accuracy: 0.8868\n",
      "epoch 0 ,Accuracy: 0.2460 , train acc : 0.2466765140324963\n",
      "epoch 50 ,Accuracy: 0.8696 , train acc : 0.9357459379615952\n",
      "epoch 100 ,Accuracy: 0.8868 , train acc : 0.9519940915805022\n",
      "epoch 150 ,Accuracy: 0.8819 , train acc : 0.9542097488921714\n",
      "epoch 200 ,Accuracy: 0.8856 , train acc : 0.9615952732644018\n",
      "epoch 250 ,Accuracy: 0.8868 , train acc : 0.965288035450517\n",
      "epoch 300 ,Accuracy: 0.8844 , train acc : 0.9682422451994092\n",
      "epoch 350 ,Accuracy: 0.8856 , train acc : 0.9726735598227474\n",
      "epoch 400 ,Accuracy: 0.8831 , train acc : 0.9852289512555391\n",
      "epoch 450 ,Accuracy: 0.8819 , train acc : 0.9844903988183161\n",
      "epoch 500 ,Accuracy: 0.8782 , train acc : 0.98301329394387\n",
      "epoch 550 ,Accuracy: 0.8807 , train acc : 0.9918759231905465\n",
      "epoch 600 ,Accuracy: 0.8831 , train acc : 0.9911373707533235\n",
      "epoch 650 ,Accuracy: 0.8782 , train acc : 0.9911373707533235\n",
      "epoch 700 ,Accuracy: 0.8770 , train acc : 0.9948301329394387\n",
      "epoch 750 ,Accuracy: 0.8770 , train acc : 0.9955686853766618\n",
      "epoch 800 ,Accuracy: 0.8770 , train acc : 0.9940915805022157\n",
      "epoch 850 ,Accuracy: 0.8807 , train acc : 0.9970457902511078\n",
      "epoch 900 ,Accuracy: 0.8770 , train acc : 0.9970457902511078\n",
      "epoch 950 ,Accuracy: 0.8758 , train acc : 0.9985228951255539\n",
      "epoch 1000 ,Accuracy: 0.8868 , train acc : 0.9992614475627769\n",
      "epoch 1050 ,Accuracy: 0.8844 , train acc : 0.9963072378138847\n",
      "epoch 1100 ,Accuracy: 0.8856 , train acc : 0.9977843426883308\n",
      "epoch 1150 ,Accuracy: 0.8819 , train acc : 0.9992614475627769\n",
      "epoch 1200 ,Accuracy: 0.8795 , train acc : 0.9970457902511078\n",
      "epoch 1250 ,Accuracy: 0.8795 , train acc : 0.9992614475627769\n",
      "epoch 1300 ,Accuracy: 0.8745 , train acc : 0.9985228951255539\n",
      "epoch 1350 ,Accuracy: 0.8831 , train acc : 1.0\n",
      "epoch 1400 ,Accuracy: 0.8745 , train acc : 0.9992614475627769\n",
      "epoch 1450 ,Accuracy: 0.8758 , train acc : 0.9992614475627769\n",
      "Accuracy: 0.8770\n",
      "base model\n",
      "0.8868388683886839\n",
      "0.8796867502201875\n",
      "OurModel\n",
      "0.8769987699876999\n",
      "0.8687308792192958\n",
      "*****\n",
      "epoch : 6 / 10\n",
      "epoch : 50 = >  Accuracy: 0.5424\n",
      "epoch : 100 = >  Accuracy: 0.6150\n",
      "epoch : 150 = >  Accuracy: 0.7134\n",
      "epoch : 200 = >  Accuracy: 0.7663\n",
      "epoch : 250 = >  Accuracy: 0.7958\n",
      "epoch : 300 = >  Accuracy: 0.8118\n",
      "epoch : 350 = >  Accuracy: 0.8450\n",
      "epoch : 400 = >  Accuracy: 0.8573\n",
      "epoch : 450 = >  Accuracy: 0.8733\n",
      "epoch : 500 = >  Accuracy: 0.8819\n",
      "epoch : 550 = >  Accuracy: 0.8856\n",
      "epoch : 600 = >  Accuracy: 0.8868\n",
      "epoch : 650 = >  Accuracy: 0.8881\n",
      "epoch : 700 = >  Accuracy: 0.8918\n",
      "epoch : 750 = >  Accuracy: 0.8881\n",
      "epoch : 800 = >  Accuracy: 0.8881\n",
      "epoch : 850 = >  Accuracy: 0.8881\n",
      "epoch : 900 = >  Accuracy: 0.8868\n",
      "Final accuracy: 0.8868\n",
      "epoch 0 ,Accuracy: 0.4994 , train acc : 0.4586410635155096\n",
      "epoch 50 ,Accuracy: 0.8745 , train acc : 0.946824224519941\n",
      "epoch 100 ,Accuracy: 0.8868 , train acc : 0.9512555391432792\n",
      "epoch 150 ,Accuracy: 0.8831 , train acc : 0.9593796159527327\n",
      "epoch 200 ,Accuracy: 0.8795 , train acc : 0.9638109305760709\n",
      "epoch 250 ,Accuracy: 0.8795 , train acc : 0.9675036927621861\n",
      "Accuracy: 0.8795\n",
      "base model\n",
      "0.8868388683886839\n",
      "0.8796867502201875\n",
      "OurModel\n",
      "0.8794587945879458\n",
      "0.8715592947227695\n",
      "*****\n",
      "epoch : 7 / 10\n",
      "epoch : 50 = >  Accuracy: 0.5400\n",
      "epoch : 100 = >  Accuracy: 0.6187\n",
      "epoch : 150 = >  Accuracy: 0.7134\n",
      "epoch : 200 = >  Accuracy: 0.7638\n",
      "epoch : 250 = >  Accuracy: 0.7970\n",
      "epoch : 300 = >  Accuracy: 0.8315\n",
      "epoch : 350 = >  Accuracy: 0.8499\n",
      "epoch : 400 = >  Accuracy: 0.8622\n",
      "epoch : 450 = >  Accuracy: 0.8758\n",
      "epoch : 500 = >  Accuracy: 0.8807\n",
      "epoch : 550 = >  Accuracy: 0.8881\n",
      "epoch : 600 = >  Accuracy: 0.8868\n",
      "epoch : 650 = >  Accuracy: 0.8905\n",
      "epoch : 700 = >  Accuracy: 0.8918\n",
      "epoch : 750 = >  Accuracy: 0.8881\n",
      "epoch : 800 = >  Accuracy: 0.8881\n",
      "epoch : 850 = >  Accuracy: 0.8881\n",
      "epoch : 900 = >  Accuracy: 0.8868\n",
      "Final accuracy: 0.8868\n",
      "epoch 0 ,Accuracy: 0.4084 , train acc : 0.3906942392909897\n",
      "epoch 50 ,Accuracy: 0.7048 , train acc : 0.7178729689807977\n",
      "epoch 100 ,Accuracy: 0.7023 , train acc : 0.7245199409158051\n",
      "epoch 150 ,Accuracy: 0.6937 , train acc : 0.7296898079763663\n",
      "epoch 200 ,Accuracy: 0.6913 , train acc : 0.7304283604135894\n",
      "epoch 250 ,Accuracy: 0.6950 , train acc : 0.7392909896602659\n",
      "epoch 300 ,Accuracy: 0.6863 , train acc : 0.7378138847858198\n",
      "epoch 350 ,Accuracy: 0.6925 , train acc : 0.7407680945347119\n",
      "epoch 400 ,Accuracy: 0.6863 , train acc : 0.7444608567208272\n",
      "epoch 450 ,Accuracy: 0.6888 , train acc : 0.7474150664697193\n",
      "epoch 500 ,Accuracy: 0.6876 , train acc : 0.7451994091580503\n",
      "epoch 550 ,Accuracy: 0.6876 , train acc : 0.7466765140324964\n",
      "epoch 600 ,Accuracy: 0.6851 , train acc : 0.7488921713441654\n",
      "epoch 650 ,Accuracy: 0.6888 , train acc : 0.7511078286558346\n",
      "epoch 700 ,Accuracy: 0.6962 , train acc : 0.7496307237813885\n",
      "epoch 750 ,Accuracy: 0.6937 , train acc : 0.7488921713441654\n",
      "epoch 800 ,Accuracy: 0.6913 , train acc : 0.7511078286558346\n",
      "epoch 850 ,Accuracy: 0.6876 , train acc : 0.7503692762186115\n",
      "epoch 900 ,Accuracy: 0.6888 , train acc : 0.7511078286558346\n",
      "epoch 950 ,Accuracy: 0.6863 , train acc : 0.7511078286558346\n",
      "epoch 1000 ,Accuracy: 0.6888 , train acc : 0.7496307237813885\n",
      "epoch 1050 ,Accuracy: 0.6863 , train acc : 0.7503692762186115\n",
      "epoch 1100 ,Accuracy: 0.6900 , train acc : 0.7496307237813885\n",
      "epoch 1150 ,Accuracy: 0.6839 , train acc : 0.7511078286558346\n",
      "epoch 1200 ,Accuracy: 0.6863 , train acc : 0.7511078286558346\n",
      "epoch 1250 ,Accuracy: 0.6900 , train acc : 0.7511078286558346\n",
      "epoch 1300 ,Accuracy: 0.6888 , train acc : 0.7511078286558346\n",
      "epoch 1350 ,Accuracy: 0.6986 , train acc : 0.7511078286558346\n",
      "epoch 1400 ,Accuracy: 0.6913 , train acc : 0.7511078286558346\n",
      "epoch 1450 ,Accuracy: 0.6950 , train acc : 0.7511078286558346\n",
      "epoch 0 ,Accuracy: 0.3407 , train acc : 0.38404726735598227\n",
      "epoch 50 ,Accuracy: 0.8044 , train acc : 0.8522895125553914\n",
      "epoch 100 ,Accuracy: 0.8167 , train acc : 0.8737075332348597\n",
      "epoch 150 ,Accuracy: 0.8733 , train acc : 0.9387001477104875\n",
      "epoch 200 ,Accuracy: 0.8819 , train acc : 0.9542097488921714\n",
      "epoch 250 ,Accuracy: 0.8758 , train acc : 0.9601181683899557\n",
      "epoch 300 ,Accuracy: 0.8795 , train acc : 0.9638109305760709\n",
      "epoch 350 ,Accuracy: 0.8721 , train acc : 0.9689807976366323\n",
      "epoch 400 ,Accuracy: 0.8721 , train acc : 0.9719350073855244\n",
      "epoch 450 ,Accuracy: 0.8807 , train acc : 0.9734121122599705\n",
      "epoch 500 ,Accuracy: 0.8745 , train acc : 0.9756277695716395\n",
      "epoch 550 ,Accuracy: 0.8733 , train acc : 0.9807976366322009\n",
      "epoch 600 ,Accuracy: 0.8696 , train acc : 0.9807976366322009\n",
      "epoch 650 ,Accuracy: 0.8684 , train acc : 0.982274741506647\n",
      "epoch 700 ,Accuracy: 0.8684 , train acc : 0.9867060561299852\n",
      "epoch 750 ,Accuracy: 0.8721 , train acc : 0.9844903988183161\n",
      "epoch 800 ,Accuracy: 0.8708 , train acc : 0.9859675036927622\n",
      "epoch 850 ,Accuracy: 0.8672 , train acc : 0.9896602658788775\n",
      "epoch 900 ,Accuracy: 0.8659 , train acc : 0.9918759231905465\n",
      "epoch 950 ,Accuracy: 0.8684 , train acc : 0.9859675036927622\n",
      "epoch 1000 ,Accuracy: 0.8770 , train acc : 0.9918759231905465\n",
      "epoch 1050 ,Accuracy: 0.8795 , train acc : 0.9874446085672083\n",
      "epoch 1100 ,Accuracy: 0.8659 , train acc : 0.9903988183161004\n",
      "epoch 1150 ,Accuracy: 0.8758 , train acc : 0.9918759231905465\n",
      "epoch 1200 ,Accuracy: 0.8733 , train acc : 0.9903988183161004\n",
      "epoch 1250 ,Accuracy: 0.8684 , train acc : 0.9926144756277696\n",
      "epoch 1300 ,Accuracy: 0.8721 , train acc : 0.9911373707533235\n",
      "epoch 1350 ,Accuracy: 0.8721 , train acc : 0.9918759231905465\n",
      "epoch 1400 ,Accuracy: 0.8708 , train acc : 0.9933530280649926\n",
      "epoch 1450 ,Accuracy: 0.8758 , train acc : 0.9926144756277696\n",
      "Accuracy: 0.8647\n",
      "base model\n",
      "0.8868388683886839\n",
      "0.8796867502201875\n",
      "OurModel\n",
      "0.8646986469864698\n",
      "0.8549573024973276\n",
      "*****\n",
      "epoch : 8 / 10\n",
      "epoch : 50 = >  Accuracy: 0.5338\n",
      "epoch : 100 = >  Accuracy: 0.6162\n",
      "epoch : 150 = >  Accuracy: 0.7134\n",
      "epoch : 200 = >  Accuracy: 0.7651\n",
      "epoch : 250 = >  Accuracy: 0.7970\n",
      "epoch : 300 = >  Accuracy: 0.8118\n",
      "epoch : 350 = >  Accuracy: 0.8475\n",
      "epoch : 400 = >  Accuracy: 0.8585\n",
      "epoch : 450 = >  Accuracy: 0.8745\n",
      "epoch : 500 = >  Accuracy: 0.8807\n",
      "epoch : 550 = >  Accuracy: 0.8868\n",
      "epoch : 600 = >  Accuracy: 0.8868\n",
      "epoch : 650 = >  Accuracy: 0.8868\n",
      "epoch : 700 = >  Accuracy: 0.8918\n",
      "epoch : 750 = >  Accuracy: 0.8881\n",
      "epoch : 800 = >  Accuracy: 0.8881\n",
      "epoch : 850 = >  Accuracy: 0.8881\n",
      "epoch : 900 = >  Accuracy: 0.8868\n",
      "Final accuracy: 0.8868\n",
      "epoch 0 ,Accuracy: 0.3862 , train acc : 0.38847858197932056\n",
      "epoch 50 ,Accuracy: 0.8831 , train acc : 0.946824224519941\n",
      "epoch 100 ,Accuracy: 0.8831 , train acc : 0.9527326440177253\n",
      "epoch 150 ,Accuracy: 0.8942 , train acc : 0.9564254062038404\n",
      "epoch 200 ,Accuracy: 0.8795 , train acc : 0.9615952732644018\n",
      "epoch 250 ,Accuracy: 0.8881 , train acc : 0.965288035450517\n",
      "epoch 300 ,Accuracy: 0.8868 , train acc : 0.9734121122599705\n",
      "epoch 350 ,Accuracy: 0.8868 , train acc : 0.9734121122599705\n",
      "epoch 400 ,Accuracy: 0.8795 , train acc : 0.9793205317577548\n",
      "epoch 450 ,Accuracy: 0.8819 , train acc : 0.9867060561299852\n",
      "epoch 500 ,Accuracy: 0.8758 , train acc : 0.9889217134416544\n",
      "epoch 550 ,Accuracy: 0.8733 , train acc : 0.9933530280649926\n",
      "epoch 600 ,Accuracy: 0.8745 , train acc : 0.9903988183161004\n",
      "epoch 650 ,Accuracy: 0.8758 , train acc : 0.9918759231905465\n",
      "epoch 700 ,Accuracy: 0.8721 , train acc : 0.9955686853766618\n",
      "epoch 750 ,Accuracy: 0.8782 , train acc : 0.9940915805022157\n",
      "epoch 800 ,Accuracy: 0.8758 , train acc : 0.9977843426883308\n",
      "epoch 850 ,Accuracy: 0.8745 , train acc : 0.9970457902511078\n",
      "epoch 900 ,Accuracy: 0.8696 , train acc : 0.9985228951255539\n",
      "epoch 950 ,Accuracy: 0.8770 , train acc : 0.9985228951255539\n",
      "epoch 1000 ,Accuracy: 0.8721 , train acc : 0.9963072378138847\n",
      "epoch 1050 ,Accuracy: 0.8770 , train acc : 0.9977843426883308\n",
      "epoch 1100 ,Accuracy: 0.8758 , train acc : 0.9992614475627769\n",
      "epoch 1150 ,Accuracy: 0.8868 , train acc : 0.9970457902511078\n",
      "epoch 1200 ,Accuracy: 0.8770 , train acc : 0.9977843426883308\n",
      "epoch 1250 ,Accuracy: 0.8733 , train acc : 0.9977843426883308\n",
      "epoch 1300 ,Accuracy: 0.8745 , train acc : 0.9985228951255539\n",
      "epoch 1350 ,Accuracy: 0.8819 , train acc : 0.9985228951255539\n",
      "epoch 1400 ,Accuracy: 0.8598 , train acc : 0.9985228951255539\n",
      "epoch 1450 ,Accuracy: 0.8770 , train acc : 0.9977843426883308\n",
      "Accuracy: 0.8696\n",
      "base model\n",
      "0.8868388683886839\n",
      "0.8796867502201875\n",
      "OurModel\n",
      "0.8696186961869619\n",
      "0.8600931183804084\n",
      "*****\n",
      "epoch : 9 / 10\n",
      "epoch : 50 = >  Accuracy: 0.5449\n",
      "epoch : 100 = >  Accuracy: 0.6150\n",
      "epoch : 150 = >  Accuracy: 0.7134\n",
      "epoch : 200 = >  Accuracy: 0.7663\n",
      "epoch : 250 = >  Accuracy: 0.7970\n",
      "epoch : 300 = >  Accuracy: 0.8106\n",
      "epoch : 350 = >  Accuracy: 0.8450\n",
      "epoch : 400 = >  Accuracy: 0.8573\n",
      "epoch : 450 = >  Accuracy: 0.8721\n",
      "epoch : 500 = >  Accuracy: 0.8807\n",
      "epoch : 550 = >  Accuracy: 0.8881\n",
      "epoch : 600 = >  Accuracy: 0.8868\n",
      "epoch : 650 = >  Accuracy: 0.8905\n",
      "epoch : 700 = >  Accuracy: 0.8918\n",
      "epoch : 750 = >  Accuracy: 0.8881\n",
      "epoch : 800 = >  Accuracy: 0.8881\n",
      "epoch : 850 = >  Accuracy: 0.8881\n",
      "epoch : 900 = >  Accuracy: 0.8868\n",
      "Final accuracy: 0.8868\n",
      "epoch 0 ,Accuracy: 0.3739 , train acc : 0.38404726735598227\n",
      "epoch 50 ,Accuracy: 0.7306 , train acc : 0.7872968980797637\n",
      "epoch 100 ,Accuracy: 0.7380 , train acc : 0.7961595273264401\n",
      "epoch 150 ,Accuracy: 0.7442 , train acc : 0.8035450516986706\n",
      "epoch 200 ,Accuracy: 0.7442 , train acc : 0.8131462333825702\n",
      "epoch 250 ,Accuracy: 0.7368 , train acc : 0.8161004431314623\n",
      "epoch 300 ,Accuracy: 0.7380 , train acc : 0.8175775480059084\n",
      "epoch 350 ,Accuracy: 0.7319 , train acc : 0.8197932053175776\n",
      "epoch 400 ,Accuracy: 0.7306 , train acc : 0.8249630723781388\n",
      "epoch 450 ,Accuracy: 0.7294 , train acc : 0.8264401772525849\n",
      "epoch 500 ,Accuracy: 0.7319 , train acc : 0.829394387001477\n",
      "epoch 550 ,Accuracy: 0.7343 , train acc : 0.8316100443131462\n",
      "epoch 600 ,Accuracy: 0.7306 , train acc : 0.8330871491875923\n",
      "epoch 650 ,Accuracy: 0.7269 , train acc : 0.8330871491875923\n",
      "epoch 700 ,Accuracy: 0.7269 , train acc : 0.8345642540620384\n",
      "epoch 750 ,Accuracy: 0.7306 , train acc : 0.8338257016248154\n",
      "epoch 800 ,Accuracy: 0.7282 , train acc : 0.8353028064992615\n",
      "epoch 850 ,Accuracy: 0.7282 , train acc : 0.8338257016248154\n",
      "epoch 900 ,Accuracy: 0.7282 , train acc : 0.8345642540620384\n",
      "epoch 950 ,Accuracy: 0.7319 , train acc : 0.8360413589364845\n",
      "epoch 1000 ,Accuracy: 0.7306 , train acc : 0.8353028064992615\n",
      "epoch 1050 ,Accuracy: 0.7282 , train acc : 0.8360413589364845\n",
      "epoch 1100 ,Accuracy: 0.7306 , train acc : 0.8367799113737076\n",
      "epoch 1150 ,Accuracy: 0.7282 , train acc : 0.8367799113737076\n",
      "epoch 1200 ,Accuracy: 0.7294 , train acc : 0.8375184638109305\n",
      "epoch 1250 ,Accuracy: 0.7331 , train acc : 0.8367799113737076\n",
      "epoch 1300 ,Accuracy: 0.7269 , train acc : 0.8367799113737076\n",
      "epoch 1350 ,Accuracy: 0.7269 , train acc : 0.8367799113737076\n",
      "epoch 1400 ,Accuracy: 0.7319 , train acc : 0.8375184638109305\n",
      "epoch 1450 ,Accuracy: 0.7269 , train acc : 0.8360413589364845\n",
      "epoch 0 ,Accuracy: 0.4932 , train acc : 0.49926144756277696\n",
      "epoch 50 ,Accuracy: 0.8167 , train acc : 0.8862629246676514\n",
      "epoch 100 ,Accuracy: 0.8266 , train acc : 0.8943870014771049\n",
      "epoch 150 ,Accuracy: 0.8339 , train acc : 0.8995568685376661\n",
      "epoch 200 ,Accuracy: 0.8167 , train acc : 0.895864106351551\n",
      "epoch 250 ,Accuracy: 0.8241 , train acc : 0.9054652880354506\n",
      "epoch 300 ,Accuracy: 0.8241 , train acc : 0.9084194977843427\n",
      "epoch 350 ,Accuracy: 0.8253 , train acc : 0.912850812407681\n",
      "epoch 400 ,Accuracy: 0.8290 , train acc : 0.9194977843426884\n",
      "epoch 450 ,Accuracy: 0.8204 , train acc : 0.9231905465288035\n",
      "epoch 500 ,Accuracy: 0.8229 , train acc : 0.9231905465288035\n",
      "epoch 550 ,Accuracy: 0.8204 , train acc : 0.9231905465288035\n",
      "epoch 600 ,Accuracy: 0.8229 , train acc : 0.9276218611521418\n",
      "epoch 650 ,Accuracy: 0.8167 , train acc : 0.9268833087149188\n",
      "epoch 700 ,Accuracy: 0.8180 , train acc : 0.9283604135893648\n",
      "epoch 750 ,Accuracy: 0.8229 , train acc : 0.930576070901034\n",
      "epoch 800 ,Accuracy: 0.8192 , train acc : 0.931314623338257\n",
      "epoch 850 ,Accuracy: 0.8204 , train acc : 0.9320531757754801\n",
      "epoch 900 ,Accuracy: 0.8253 , train acc : 0.9335302806499262\n",
      "epoch 950 ,Accuracy: 0.8204 , train acc : 0.9320531757754801\n",
      "epoch 1000 ,Accuracy: 0.8180 , train acc : 0.9327917282127031\n",
      "epoch 1050 ,Accuracy: 0.8143 , train acc : 0.9320531757754801\n",
      "epoch 1100 ,Accuracy: 0.8241 , train acc : 0.931314623338257\n",
      "epoch 1150 ,Accuracy: 0.8167 , train acc : 0.9335302806499262\n",
      "epoch 1200 ,Accuracy: 0.8204 , train acc : 0.9327917282127031\n",
      "epoch 1250 ,Accuracy: 0.8229 , train acc : 0.9335302806499262\n",
      "epoch 1300 ,Accuracy: 0.8216 , train acc : 0.9342688330871491\n",
      "epoch 1350 ,Accuracy: 0.8216 , train acc : 0.9320531757754801\n",
      "epoch 1400 ,Accuracy: 0.8216 , train acc : 0.9320531757754801\n",
      "epoch 1450 ,Accuracy: 0.8266 , train acc : 0.9335302806499262\n",
      "epoch 0 ,Accuracy: 0.4182 , train acc : 0.41063515509601184\n",
      "epoch 50 ,Accuracy: 0.7417 , train acc : 0.774741506646972\n",
      "epoch 100 ,Accuracy: 0.7392 , train acc : 0.7791728212703102\n",
      "epoch 150 ,Accuracy: 0.7343 , train acc : 0.7813884785819794\n",
      "epoch 200 ,Accuracy: 0.7429 , train acc : 0.7843426883308715\n",
      "epoch 250 ,Accuracy: 0.7306 , train acc : 0.7843426883308715\n",
      "epoch 300 ,Accuracy: 0.7392 , train acc : 0.7895125553914328\n",
      "epoch 350 ,Accuracy: 0.7282 , train acc : 0.7909896602658789\n",
      "epoch 400 ,Accuracy: 0.7331 , train acc : 0.7939438700147711\n",
      "epoch 450 ,Accuracy: 0.7282 , train acc : 0.7917282127031019\n",
      "epoch 500 ,Accuracy: 0.7294 , train acc : 0.7983751846381093\n",
      "epoch 550 ,Accuracy: 0.7269 , train acc : 0.7983751846381093\n",
      "epoch 600 ,Accuracy: 0.7257 , train acc : 0.7998522895125554\n",
      "epoch 650 ,Accuracy: 0.7220 , train acc : 0.7983751846381093\n",
      "epoch 700 ,Accuracy: 0.7269 , train acc : 0.8005908419497785\n",
      "epoch 750 ,Accuracy: 0.7245 , train acc : 0.8013293943870015\n",
      "epoch 800 ,Accuracy: 0.7269 , train acc : 0.8005908419497785\n",
      "epoch 850 ,Accuracy: 0.7232 , train acc : 0.7998522895125554\n",
      "epoch 900 ,Accuracy: 0.7232 , train acc : 0.7998522895125554\n",
      "epoch 950 ,Accuracy: 0.7282 , train acc : 0.7998522895125554\n",
      "epoch 1000 ,Accuracy: 0.7294 , train acc : 0.8005908419497785\n",
      "epoch 1050 ,Accuracy: 0.7269 , train acc : 0.8013293943870015\n",
      "epoch 1100 ,Accuracy: 0.7245 , train acc : 0.8013293943870015\n",
      "epoch 1150 ,Accuracy: 0.7220 , train acc : 0.8005908419497785\n",
      "epoch 1200 ,Accuracy: 0.7306 , train acc : 0.8013293943870015\n",
      "epoch 1250 ,Accuracy: 0.7269 , train acc : 0.8020679468242246\n",
      "epoch 1300 ,Accuracy: 0.7257 , train acc : 0.8020679468242246\n",
      "epoch 1350 ,Accuracy: 0.7220 , train acc : 0.8013293943870015\n",
      "epoch 1400 ,Accuracy: 0.7269 , train acc : 0.8013293943870015\n",
      "epoch 1450 ,Accuracy: 0.7245 , train acc : 0.7998522895125554\n",
      "epoch 0 ,Accuracy: 0.3985 , train acc : 0.4076809453471196\n",
      "epoch 50 ,Accuracy: 0.7897 , train acc : 0.8419497784342689\n",
      "epoch 100 ,Accuracy: 0.7860 , train acc : 0.845642540620384\n",
      "epoch 150 ,Accuracy: 0.7983 , train acc : 0.8515509601181684\n",
      "epoch 200 ,Accuracy: 0.7934 , train acc : 0.8537666174298375\n",
      "epoch 250 ,Accuracy: 0.8007 , train acc : 0.8611521418020679\n",
      "epoch 300 ,Accuracy: 0.7958 , train acc : 0.8648449039881831\n",
      "epoch 350 ,Accuracy: 0.7884 , train acc : 0.8685376661742984\n",
      "epoch 400 ,Accuracy: 0.7921 , train acc : 0.8663220088626292\n",
      "epoch 450 ,Accuracy: 0.7934 , train acc : 0.8722304283604135\n",
      "epoch 500 ,Accuracy: 0.7909 , train acc : 0.8759231905465288\n",
      "epoch 550 ,Accuracy: 0.7921 , train acc : 0.8722304283604135\n",
      "epoch 600 ,Accuracy: 0.7921 , train acc : 0.8766617429837519\n",
      "epoch 650 ,Accuracy: 0.7847 , train acc : 0.8818316100443131\n",
      "epoch 700 ,Accuracy: 0.7946 , train acc : 0.8810930576070901\n",
      "epoch 750 ,Accuracy: 0.7835 , train acc : 0.8825701624815362\n",
      "epoch 800 ,Accuracy: 0.7909 , train acc : 0.8810930576070901\n",
      "epoch 850 ,Accuracy: 0.7884 , train acc : 0.879615952732644\n",
      "epoch 900 ,Accuracy: 0.7860 , train acc : 0.880354505169867\n",
      "epoch 950 ,Accuracy: 0.7897 , train acc : 0.8825701624815362\n",
      "epoch 1000 ,Accuracy: 0.7847 , train acc : 0.8818316100443131\n",
      "epoch 1050 ,Accuracy: 0.7909 , train acc : 0.8818316100443131\n",
      "epoch 1100 ,Accuracy: 0.7811 , train acc : 0.8833087149187593\n",
      "epoch 1150 ,Accuracy: 0.7897 , train acc : 0.8818316100443131\n",
      "epoch 1200 ,Accuracy: 0.7860 , train acc : 0.8825701624815362\n",
      "epoch 1250 ,Accuracy: 0.7872 , train acc : 0.8825701624815362\n",
      "epoch 1300 ,Accuracy: 0.7897 , train acc : 0.8833087149187593\n",
      "epoch 1350 ,Accuracy: 0.7909 , train acc : 0.8825701624815362\n",
      "epoch 1400 ,Accuracy: 0.7897 , train acc : 0.8818316100443131\n",
      "epoch 1450 ,Accuracy: 0.7872 , train acc : 0.8818316100443131\n",
      "epoch 0 ,Accuracy: 0.3493 , train acc : 0.3759231905465288\n",
      "epoch 50 ,Accuracy: 0.8770 , train acc : 0.9446085672082718\n",
      "epoch 100 ,Accuracy: 0.8856 , train acc : 0.9512555391432792\n",
      "epoch 150 ,Accuracy: 0.8782 , train acc : 0.9542097488921714\n",
      "epoch 200 ,Accuracy: 0.8844 , train acc : 0.965288035450517\n",
      "epoch 250 ,Accuracy: 0.8831 , train acc : 0.9682422451994092\n",
      "epoch 300 ,Accuracy: 0.8831 , train acc : 0.9711964549483013\n",
      "epoch 350 ,Accuracy: 0.8831 , train acc : 0.9734121122599705\n",
      "epoch 400 ,Accuracy: 0.8881 , train acc : 0.9807976366322009\n",
      "epoch 450 ,Accuracy: 0.8819 , train acc : 0.9852289512555391\n",
      "epoch 500 ,Accuracy: 0.8795 , train acc : 0.9867060561299852\n",
      "epoch 550 ,Accuracy: 0.8770 , train acc : 0.9903988183161004\n",
      "epoch 600 ,Accuracy: 0.8733 , train acc : 0.9911373707533235\n",
      "epoch 650 ,Accuracy: 0.8782 , train acc : 0.9918759231905465\n",
      "epoch 700 ,Accuracy: 0.8807 , train acc : 0.9940915805022157\n",
      "epoch 750 ,Accuracy: 0.8782 , train acc : 0.9918759231905465\n",
      "epoch 800 ,Accuracy: 0.8708 , train acc : 0.9963072378138847\n",
      "epoch 850 ,Accuracy: 0.8782 , train acc : 0.9963072378138847\n",
      "epoch 900 ,Accuracy: 0.8807 , train acc : 0.9963072378138847\n",
      "epoch 950 ,Accuracy: 0.8819 , train acc : 0.9992614475627769\n",
      "epoch 1000 ,Accuracy: 0.8770 , train acc : 0.9977843426883308\n",
      "epoch 1050 ,Accuracy: 0.8795 , train acc : 0.9977843426883308\n",
      "epoch 1100 ,Accuracy: 0.8696 , train acc : 0.9970457902511078\n",
      "epoch 1150 ,Accuracy: 0.8758 , train acc : 0.9977843426883308\n",
      "epoch 1200 ,Accuracy: 0.8782 , train acc : 0.9985228951255539\n",
      "epoch 1250 ,Accuracy: 0.8708 , train acc : 0.9992614475627769\n",
      "epoch 1300 ,Accuracy: 0.8745 , train acc : 0.9992614475627769\n",
      "epoch 1350 ,Accuracy: 0.8721 , train acc : 0.9992614475627769\n",
      "epoch 1400 ,Accuracy: 0.8795 , train acc : 0.9992614475627769\n",
      "epoch 1450 ,Accuracy: 0.8795 , train acc : 0.9992614475627769\n",
      "Accuracy: 0.8733\n",
      "base model\n",
      "0.8868388683886839\n",
      "0.8796867502201875\n",
      "OurModel\n",
      "0.8733087330873309\n",
      "0.8645016497097368\n",
      "*****\n",
      "epoch : 10 / 10\n",
      "epoch : 50 = >  Accuracy: 0.5400\n",
      "epoch : 100 = >  Accuracy: 0.6162\n",
      "epoch : 150 = >  Accuracy: 0.7134\n",
      "epoch : 200 = >  Accuracy: 0.7651\n",
      "epoch : 250 = >  Accuracy: 0.7970\n",
      "epoch : 300 = >  Accuracy: 0.8204\n",
      "epoch : 350 = >  Accuracy: 0.8487\n",
      "epoch : 400 = >  Accuracy: 0.8598\n",
      "epoch : 450 = >  Accuracy: 0.8745\n",
      "epoch : 500 = >  Accuracy: 0.8807\n",
      "epoch : 550 = >  Accuracy: 0.8881\n",
      "epoch : 600 = >  Accuracy: 0.8868\n",
      "epoch : 650 = >  Accuracy: 0.8893\n",
      "epoch : 700 = >  Accuracy: 0.8918\n",
      "epoch : 750 = >  Accuracy: 0.8881\n",
      "epoch : 800 = >  Accuracy: 0.8881\n",
      "epoch : 850 = >  Accuracy: 0.8881\n",
      "epoch : 900 = >  Accuracy: 0.8868\n",
      "Final accuracy: 0.8868\n",
      "epoch 0 ,Accuracy: 0.4748 , train acc : 0.4859675036927622\n",
      "epoch 50 ,Accuracy: 0.8659 , train acc : 0.914327917282127\n",
      "epoch 100 ,Accuracy: 0.8770 , train acc : 0.9387001477104875\n",
      "epoch 150 ,Accuracy: 0.8819 , train acc : 0.94903988183161\n",
      "epoch 200 ,Accuracy: 0.8745 , train acc : 0.9549483013293943\n",
      "epoch 250 ,Accuracy: 0.8807 , train acc : 0.9593796159527327\n",
      "epoch 300 ,Accuracy: 0.8733 , train acc : 0.9645494830132939\n",
      "epoch 350 ,Accuracy: 0.8684 , train acc : 0.9719350073855244\n",
      "epoch 400 ,Accuracy: 0.8831 , train acc : 0.9741506646971935\n",
      "epoch 450 ,Accuracy: 0.8721 , train acc : 0.9719350073855244\n",
      "epoch 500 ,Accuracy: 0.8708 , train acc : 0.9756277695716395\n",
      "epoch 550 ,Accuracy: 0.8721 , train acc : 0.98301329394387\n",
      "epoch 600 ,Accuracy: 0.8721 , train acc : 0.982274741506647\n",
      "epoch 650 ,Accuracy: 0.8721 , train acc : 0.9867060561299852\n",
      "epoch 700 ,Accuracy: 0.8721 , train acc : 0.982274741506647\n",
      "epoch 750 ,Accuracy: 0.8721 , train acc : 0.9859675036927622\n",
      "epoch 800 ,Accuracy: 0.8758 , train acc : 0.9859675036927622\n",
      "epoch 850 ,Accuracy: 0.8770 , train acc : 0.9881831610044313\n",
      "epoch 900 ,Accuracy: 0.8807 , train acc : 0.9903988183161004\n",
      "epoch 950 ,Accuracy: 0.8672 , train acc : 0.9859675036927622\n",
      "epoch 1000 ,Accuracy: 0.8758 , train acc : 0.9896602658788775\n",
      "epoch 1050 ,Accuracy: 0.8696 , train acc : 0.9896602658788775\n",
      "epoch 1100 ,Accuracy: 0.8733 , train acc : 0.9881831610044313\n",
      "epoch 1150 ,Accuracy: 0.8708 , train acc : 0.9933530280649926\n",
      "epoch 1200 ,Accuracy: 0.8696 , train acc : 0.9926144756277696\n",
      "epoch 1250 ,Accuracy: 0.8745 , train acc : 0.9911373707533235\n",
      "epoch 1300 ,Accuracy: 0.8782 , train acc : 0.9933530280649926\n",
      "epoch 1350 ,Accuracy: 0.8733 , train acc : 0.9926144756277696\n",
      "epoch 1400 ,Accuracy: 0.8831 , train acc : 0.9918759231905465\n",
      "epoch 1450 ,Accuracy: 0.8758 , train acc : 0.9933530280649926\n",
      "Accuracy: 0.8758\n",
      "base model\n",
      "0.8868388683886839\n",
      "0.8796867502201875\n",
      "OurModel\n",
      "0.8757687576875769\n",
      "0.8681103574983312\n",
      "+++***Fianal Result***+++\n",
      "base\n",
      "Accuracy avg = 0.8868388683886839\n",
      "Accuracy deviation = 0.0\n",
      "F1 score(macro) avg = 0.8796867502201875\n",
      "F1 score(macro) deviation = 0.0\n",
      "new model\n",
      "Accuracy avg = 0.874169741697417\n",
      "Accuracy deviation = 0.004604110549776076\n",
      "F1 score(macro) avg = 0.8648992022883337\n",
      "F1 score(macro) deviation = 0.0050925606292333845\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "BcMP1Pcl2jyn"
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}